{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc48761a-1be2-449b-939d-99c3ec2aa9ea"}}},{"cell_type":"markdown","source":["# Experiment Tracking\n\nThe machine learning life cycle involves training multiple algorithms using different hyperparameters and libraries, all with different performance results and trained models.  This lesson explores tracking those experiments to organize the machine learning life cycle.\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n - Introduce tracking ML experiments in MLflow\n - Log an experiment and explore the results in the UI\n - Record parameters, metrics, and a model\n - Query past runs programatically\n \n## Prerequisites\n- Web browser: Chrome\n- A cluster configured with **8 cores** and **DBR 7.0 ML**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4eea2b8a-6bb0-46b7-97d2-09f1f4cb184c"}}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the<br/>\nstart of each lesson (see the next cell) and the **`Classroom-Cleanup`** cell at the end of each lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7f9d570-2b1f-41bd-b4d0-2b963847b21c"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e348110-4514-498f-9c64-d550c5bfc012"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Over the course of the machine learning life cycle...<br><br>\n\n* Data scientists test many different models \n* Using various libraries \n* Each with different hyperparameters\n\nTracking these various results poses an organizational challenge, including... <br><br>\n\n* Storing experiments\n* Results\n* Models\n* Supplementary artifacts\n* Code\n* Data snapshots"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"920d450e-71c1-41c3-80d0-f6bc42c27fe8"}}},{"cell_type":"markdown","source":["### Tracking Experiments with MLflow\n\nMLflow Tracking is...<br><br>\n\n* a logging API specific for machine learning \n* agnostic to libraries and environments that do the training\n* organized around the concept of **runs**, which are executions of data science code\n* runs are aggregated into **experiments** where many runs can be a part of a given experiment \n* An MLflow server can host many experiments.\n\nEach run can record the following information:<br><br>\n\n- **Parameters:** Key-value pairs of input parameters such as the number of trees in a random forest model\n- **Metrics:** Evaluation metrics such as RMSE or Area Under the ROC Curve\n- **Artifacts:** Arbitrary output files in any format.  This can include images, pickled models, and data files\n- **Source:** The code that originally ran the experiment\n\nExperiments can be tracked using libraries in Python, R, and Java as well as by using the CLI and REST calls"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a203159d-b2aa-459d-acb9-99207aca766f"}}},{"cell_type":"markdown","source":["-sandbox\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ML-Part-4/mlflow-tracking.png\" style=\"height: 400px; margin: 20px\"/></div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eebae557-8f17-4c5f-ab5b-cd599fcf535a"}}},{"cell_type":"markdown","source":["-sandbox\n### Experiment Logging and UI\n\nMLflow is an open source software project developed by Databricks available to developers regardless of which platform they are using.  Databricks hosts MLflow for you, which reduces deployment configuration and adds security benefits.  It is accessible on all Databricks workspaces in Azure and AWS.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> See <a href=\"https://mlflow.org/docs/latest/quickstart.html#\" target=\"_blank\">the MLflow quickstart guide</a> for details on setting up MLflow locally or on your own server."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79361839-96a6-4829-af00-a0dd7cfeac69"}}},{"cell_type":"markdown","source":["Use the **Databricks runtime version 7.0 or install `mlflow==1.7.0` using `PyPi` manually.**  <a href=\"https://files.training.databricks.com/static/step-by-step/installing-libraries-from-pypi.html\" target=\"_blank\">See the instructions on how to install a library from PyPi</a> if you're unfamiliar with the process"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f02daa2f-4374-4880-bece-bdba1a57091b"}}},{"cell_type":"markdown","source":["Import a dataset of Airbnb listings and featurize the data.  We'll use this to train a model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"097d2a8b-b708-4e50-a9e9-42f4da3432e2"}}},{"cell_type":"markdown","source":["-sandbox\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> If you are running the below code on Community Edition with DBR 7.0+, please replace it with `df = spark.read.csv(\"/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\", header = True).toPandas()`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e359be6-a5b7-4350-9984-606068bad10a"}}},{"cell_type":"code","source":["import pandas as pd\n\ndf = pd.read_csv(\"/dbfs/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"29620156-99aa-4296-908d-f7f7aed672eb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Perform a train/test split."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"025c1500-08d6-411f-b3bf-09665859c20e"}}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df.drop([\"price\"], axis=1), df[[\"price\"]].values.ravel(), random_state=42)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46653b27-7f14-426a-b44b-22aba356c8f5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n**Navigate to the MLflow UI by clicking on the `Runs` button on the top of the screen.**\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Every Python notebook in a Azure Databricks Workspace has its own experiment. When you use MLflow in a notebook, it records runs in the notebook experiment. A notebook experiment shares the same name and ID as its corresponding notebook."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4712e860-85d6-4869-af00-f4e44edffbd5"}}},{"cell_type":"markdown","source":["Log a basic experiment by doing the following:<br><br>\n\n1. Start an experiment using `mlflow.start_run()` and passing it a name for the run\n2. Train your model\n3. Log the model using `mlflow.sklearn.log_model()`\n4. Log the model error using `mlflow.log_metric()`\n5. Print out the run id using `run.info.run_uuid`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20dd101d-843e-4e9f-91c5-89f212f00e22"}}},{"cell_type":"code","source":["import mlflow.sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nwith mlflow.start_run(run_name=\"Basic RF Experiment\") as run:\n  # Create model, train it, and create predictions\n  rf = RandomForestRegressor()\n  rf.fit(X_train, y_train)\n  predictions = rf.predict(X_test)\n  \n  # Log model\n  mlflow.sklearn.log_model(rf, \"random-forest-model\")\n  \n  # Create metrics\n  mse = mean_squared_error(y_test, predictions)\n  print(f\"mse: {mse}\")\n  \n  # Log metrics\n  mlflow.log_metric(\"mse\", mse)\n  \n  runID = run.info.run_uuid\n  experimentID = run.info.experiment_id\n  \n  print(f\"Inside MLflow Run with run_id `{runID}` and experiment_id `{experimentID}`\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7260bcaa-05a0-47d7-8314-cf898db6f8bb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\nExamine the results in the UI.  Look for the following:<br><br>\n\n1. The `Experiment ID`\n2. The artifact location.  This is where the artifacts are stored in DBFS, which is backed by cloud object storage.\n3. The time the run was executed.  **Click this timestamp to see more information on the run.**\n4. The code that executed the run.\n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ML-Part-4/mlflow-ui-lesson2.png\" style=\"height: 400px; margin: 20px\"/></div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b1180b3-fb50-421b-bd84-bb4d9aa4dd2a"}}},{"cell_type":"markdown","source":["-sandbox\nAfter clicking on the time of the run, take a look at the following:<br><br>\n\n1. The Run ID will match what we printed above\n2. The model that we saved, included a picked version of the model as well as the Conda environment and the `MLmodel` file, which will be discussed in the next lesson.\n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ML-Part-4/mlflow-ui-lesson2b.png\" style=\"height: 400px; margin: 20px\"/></div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83849a23-6a36-4876-942e-f0cb4a5f78a7"}}},{"cell_type":"markdown","source":["Now take a look at the directory structure backing this experiment.  This allows you to retrieve artifacts."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47cb43eb-b955-4833-a6c0-7fbf5f2aec2c"}}},{"cell_type":"code","source":["from mlflow.tracking import MlflowClient\n\nartifactURL = MlflowClient().get_experiment(experimentID).artifact_location\ndbutils.fs.ls(artifactURL)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c56f160f-d57b-449c-9721-49bba205e6d9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Take a look at the contents of `random-forest-model`, which match what we see in the UI."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1832f305-c9ca-49b8-a972-64cea3d074fe"}}},{"cell_type":"code","source":["modelURL = f\"{artifactURL}/{runID}/artifacts/random-forest-model\"\ndbutils.fs.ls(modelURL)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7b74da8-4d14-4a3d-943b-d4d157b6d761"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n### Parameters, Metrics, and Artifacts\n\nBut wait, there's more!  In the last example, you logged the run name, an evaluation metric, and your model itself as an artifact.  Now let's log parameters, multiple metrics, and other artifacts including the feature importances.\n\nFirst, create a function to perform this.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> To log artifacts, we have to save them somewhere before MLflow can log them.  This code accomplishes that by using a temporary file that it then deletes."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58005885-f105-426f-9547-ab9bd191f6eb"}}},{"cell_type":"code","source":["def log_rf(experimentID, run_name, params, X_train, X_test, y_train, y_test):\n  import os\n  import matplotlib.pyplot as plt\n  import mlflow.sklearn\n  import seaborn as sns\n  from sklearn.ensemble import RandomForestRegressor\n  from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n  import tempfile\n\n  with mlflow.start_run(experiment_id=experimentID, run_name=run_name) as run:\n    # Create model, train it, and create predictions\n    rf = RandomForestRegressor(**params)\n    rf.fit(X_train, y_train)\n    predictions = rf.predict(X_test)\n\n    # Log model\n    mlflow.sklearn.log_model(rf, \"random-forest-model\")\n\n    # Log params\n    mlflow.log_params(params)\n\n    # Create metrics\n    mse = mean_squared_error(y_test, predictions)\n    mae = mean_absolute_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)\n\n    # Log metrics\n    mlflow.log_metrics({\"mse\": mse, \"mae\": mae, \"r2\": r2})\n    \n    # Create feature importance\n    importance = pd.DataFrame(list(zip(df.columns, rf.feature_importances_)), \n                                columns=[\"Feature\", \"Importance\"]\n                              ).sort_values(\"Importance\", ascending=False)\n    \n    # Log importances using a temporary file\n    temp = tempfile.NamedTemporaryFile(prefix=\"feature-importance-\", suffix=\".csv\")\n    temp_name = temp.name\n    try:\n      importance.to_csv(temp_name, index=False)\n      mlflow.log_artifact(temp_name, \"feature-importance.csv\")\n    finally:\n      temp.close() # Delete the temp file\n    \n    # Create plot\n    fig, ax = plt.subplots()\n\n    sns.residplot(predictions, y_test, lowess=True)\n    plt.xlabel(\"Predicted values for Price ($)\")\n    plt.ylabel(\"Residual\")\n    plt.title(\"Residual Plot\")\n\n    # Log residuals using a temporary file\n    temp = tempfile.NamedTemporaryFile(prefix=\"residuals-\", suffix=\".png\")\n    temp_name = temp.name\n    try:\n      fig.savefig(temp_name)\n      mlflow.log_artifact(temp_name, \"residuals.png\")\n    finally:\n      temp.close() # Delete the temp file\n      \n    display(fig)\n    return run.info.run_uuid"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37160b87-193c-4c95-be50-c20b8cbe534e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Run with new parameters."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1cfd5d96-f962-493d-84df-2413e29cbb7d"}}},{"cell_type":"code","source":["params = {\n  \"n_estimators\": 100,\n  \"max_depth\": 5,\n  \"random_state\": 42\n}\n\nlog_rf(experimentID, \"Second Run\", params, X_train, X_test, y_train, y_test)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9d700144-be55-45e0-91ab-b2523c87d193"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Check the UI to see how this appears.  Take a look at the artifact to see where the plot was saved.\n\nNow, run a third run."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37dca8fe-44d6-4941-884c-8f2d1d85636b"}}},{"cell_type":"code","source":["params_1000_trees = {\n  \"n_estimators\": 1000,\n  \"max_depth\": 10,\n  \"random_state\": 42\n}\n\nlog_rf(experimentID, \"Third Run\", params_1000_trees, X_train, X_test, y_train, y_test)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ef01fd8-e867-4238-b1bd-aa5c4d6c99e7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n### Querying Past Runs\n\nYou can query past runs programatically in order to use this data back in Python.  The pathway to doing this is an `MlflowClient` object. \n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> You can also set tags for runs using `client.set_tag(run.info.run_uuid, \"tag_key\", \"tag_value\")`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"978467ab-a1e0-4b8f-a21e-721b09779288"}}},{"cell_type":"code","source":["from  mlflow.tracking import MlflowClient\n\nclient = MlflowClient()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5477a02b-3e56-4453-ab42-ff72c560a2cd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["List all experiements."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"40b022b4-4c97-4e9c-bdf0-32fc1c0b73e5"}}},{"cell_type":"code","source":["client.list_experiments()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cff2444c-35ec-450f-8a7c-390a0ec6fc02"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now list all the runs for your experiment using `.search_runs()`, which takes your `experiment_id` as a parameter."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3c1c2249-336e-4c16-85ff-91d18028e50c"}}},{"cell_type":"code","source":["client.search_runs(experimentID)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"257def92-897f-4004-a42a-d30bb1672c7d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Take a look at the metrics from the last run."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f826185b-973b-4511-8bc6-333e48701eed"}}},{"cell_type":"code","source":["runs = client.search_runs(experimentID, order_by=[\"attributes.start_time desc\"], max_results=1)\nruns[0].data.metrics"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b877d6d-b42b-489e-a1d2-d5ce30c49772"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Pull the last run and take a look at the associated artifacts."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84c4cc7e-5abf-4b6d-b9f2-bd3d2c82a3c3"}}},{"cell_type":"code","source":["artifactURI = f\"{runs[0].info.artifact_uri}/random-forest-model/\"\n\ndbutils.fs.ls(artifactURI)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"40f8cb61-8bee-4f55-afed-d2217acca04e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Reload the model and take a look at the feature importance."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4236c73-2e8c-403c-9ab3-28614f5393d4"}}},{"cell_type":"code","source":["import mlflow.sklearn\n\nmodel = mlflow.sklearn.load_model(artifactURI)\nmodel.feature_importances_"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"989c8a29-4688-49e4-a9fd-13c1b45b6218"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###Hyperopt Integration\nDatabricks Runtime for Machine Learning includes Hyperopt, augmented with an implementation powered by Apache Spark. By using the [SparkTrials](https://docs.microsoft.com/en-us/azure/databricks/applications/machine-learning/automl/hyperopt/hyperopt-spark-mlflow-integration) extension of hyperopt.Trials, you can easily distribute a Hyperopt run without making other changes to your Hyperopt usage. When applying the hyperopt.fmin() function, you pass in the SparkTrials class. SparkTrials can accelerate single-machine tuning by distributing trials to Spark workers.\n\nAnother exciting feature of Hyperopt integration through Spark is the ability to track nested MLflow models."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"edde549b-48de-418a-b22c-8b00440d5749"}}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\nfrom hyperopt import STATUS_OK\n  \ndef objective_function(params):\n\n  # set the hyperparameters that we want to tune:\n  max_depth = params[\"max_depth\"]\n  max_features = params[\"max_features\"]\n\n  regressor = RandomForestRegressor(max_depth=max_depth, max_features=max_features, random_state=42)\n  regressor.fit(X_train, y_train)\n\n  # Evaluate predictions\n  r2 = regressor.score(X_test, y_test)\n\n  # Note: since we aim to maximize r2, we need to return it as a negative value (\"loss\": -metric)\n  return {\"loss\": -r2, \"status\": STATUS_OK}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"859e4026-ef94-4638-b25f-aec55a8bdd30"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from hyperopt import hp\n\nsearch_space = {\n  \"max_depth\": hp.randint(\"max_depth\", 2, 10),\n  \"max_features\": hp.choice(\"max_features\", [\"auto\", \"sqrt\", \"log2\"])\n}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7905e4d-eec7-4479-b4d9-f7294238e437"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Instead of using the default `Trials` class, you can leverage the `SparkTrials` class to trigger the distribution of tuning tasks across Spark executors. On Databricks, SparkTrials are automatically logged with MLflow.\n\n`SparkTrials` takes 3 optional arguments, namely `parallelism`, `timeout`, and `spark_session`. You can refer to this [page](http://hyperopt.github.io/hyperopt/scaleout/spark/) to read more."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb91fb8b-7a75-443e-8584-81af667bdd90"}}},{"cell_type":"code","source":["from hyperopt import fmin, tpe, STATUS_OK, SparkTrials\n\n# the number of models we want to evaluate\nnum_evals = 8\n# set the number of models to be trained concurrently\nspark_trials = SparkTrials(parallelism=2)\nbest_hyperparam = fmin(fn = objective_function, \n                       space = search_space,\n                       algo = tpe.suggest, \n                       trials = spark_trials,\n                       max_evals = num_evals)\n\nbest_hyperparam"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"abc9cf5a-dbb7-4f15-86ae-9e5f93c3dcb0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\nNow you can compare all of the models using the MLflow UI. \n\nTo understand the effect of tuning a hyperparameter:\n\n1. Select the resulting runs and click Compare.\n2. In the Scatter Plot, select a hyperparameter for the X-axis and loss for the Y-axis.\n\n <div><img src=\"https://files.training.databricks.com/images/eLearning/ML-Practice/hyperopt1.png\" style=\"height: 400px; margin: 20px\"/></div>\n\n3. You can also visualize all hyperparameters using a Parallel Coordinates Plot.\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ML-Practice/hyperopt3.png\" style=\"height: 400px; margin: 20px\"/></div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0a840ab-7bc1-4169-b858-6d143a65dfb0"}}},{"cell_type":"markdown","source":["## Review\n**Question:** What can MLflow Tracking log?  \n**Answer:** MLflow can log the following:\n- **Parameters:** inputs to a model\n- **Metrics:** the performance of the model\n- **Artifacts:** any object including data, models, and images\n- **Source:** the original code, including the commit hash if linked to git\n\n**Question:** How do you log experiments?  \n**Answer:** Experiments are logged by first creating a run and using the logging methods on that run object (e.g. `run.log_param(\"MSE\", .2)`).\n\n**Question:** Where do logged artifacts get saved?  \n**Answer:** Logged artifacts are saved in a directory of your choosing.  On Databricks, this would be DBFS, or the Databricks File System, which backed by a blob store.\n\n**Question:** How can I query past runs?  \n**Answer:** This can be done using an `MlflowClient` object.  This allows you do everything you can within the UI programatically so you never have to step outside of your programming environment."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd075bd8-181e-41c6-b3c5-f002bab8b140"}}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2653e3fd-14df-47d4-9bba-1f83c6ce8bb3"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Cleanup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"819b2524-f24c-4da2-b02b-bab5e3b1ff06"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Next Steps\n\nStart the labs for this lesson, [Experiment Tracking Lab]($./Labs/02-Lab)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e5513d4-9f90-47ef-8630-c07e04209b02"}}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n**Q:** What is MLflow at a high level?  \n**A:** <a href=\"https://databricks.com/session/accelerating-the-machine-learning-lifecycle-with-mlflow-1-0\" target=\"_blank\">Listen to Spark and MLflow creator Matei Zaharia's talk at Spark Summit in 2019.</a>\n\n**Q:** What is a good source for the larger context of machine learning tools?  \n**A:** <a href=\"https://roaringelephant.org/2019/06/18/episode-145-alex-zeltov-on-mlops-with-mlflow-kubeflow-and-other-tools-part-1/#more-1958\" target=\"_blank\">Check out this episode of the podcast Roaring Elephant.</a>\n\n**Q:** Where can I find the MLflow docs?\n**A:** <a href=\"https://www.mlflow.org/docs/latest/index.html\" target=\"_blank\">You can find the docs here.</a>\n\n**Q:** What is a good general resource for machine learning?  \n**A:** <a href=\"https://www-bcf.usc.edu/~gareth/ISL/\" target=\"_blank\">_An Introduction to Statistical Learning_</a> is a good starting point for the themes and basic approaches to machine learning.\n\n**Q:** Where can I find out more information on machine learning with Spark?\n**A:** Check out the Databricks blog <a href=\"https://databricks.com/blog/category/engineering/machine-learning\" target=\"_blank\">dedicated to machine learning</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"060d3acd-8932-46c2-be3d-70dd93fb7758"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6af1814-47a6-40d8-b2a5-1bfcea5eb2ee"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"02-Experiment-Tracking","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2854191802663074}},"nbformat":4,"nbformat_minor":0}
