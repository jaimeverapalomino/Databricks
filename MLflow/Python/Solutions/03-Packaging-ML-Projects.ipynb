{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ead0f9b-6ea9-4da3-b96f-569df87bcb99"}}},{"cell_type":"markdown","source":["# Packaging ML Projects\n\nMachine learning projects need to produce both reusable code and reproducible results.  This lesson examines creating, organizing, and packaging machine learning projects with a focus on reproducibility and collaborating with a team.\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n - Introduce organizing code into projects\n - Package a basic project with parameters and an environment\n - Run a basic project locally and remotely using Github\n\n## Prerequisites\n- Web browser: Chrome\n- A cluster configured with **8 cores** and **DBR 7.0 ML**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7777740-577e-4687-9dd3-1bb2cb2fbac6"}}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the<br/>\nstart of each lesson (see the next cell) and the **`Classroom-Cleanup`** cell at the end of each lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"007275d8-77fa-4ede-85f6-4b1d8cbba77b"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab0912d2-1b82-4426-b6bc-1da3a22c920c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n### The Case for Packaging\n\nThere are a number of different reasons why teams need to package their machine learning projects:<br><br>\n\n1. Projects have various library dependencies \n  - Shipping a machine learning solution involves the environment in which it was built\n  - MLflow allows for this environment to be a conda environment or docker container\n  - This means that teams can easily share and publish their code for others to use\n2. Machine learning projects become increasingly complex as time goes on\n  - This includes ETL and featurization steps, machine learning models used for pre-processing, and finally the model training itself\n3. Each component of a machine learning pipeline needs to allow for tracing its lineage\n  - If there's a failure at some point, tracing the full end-to-end lineage of a model allows for easier debugging."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45bd0936-6c8b-4b09-aca7-6b2beb3c69a2"}}},{"cell_type":"markdown","source":["-sandbox\n**ML Projects is a specification for how to organize code in a project.**<br><br>\n\n- The heart of this is an **MLproject file,** a YAML specification for the components of the ML project\n- This allows for more complex workflows since a project can execute another project\n   - This allows for encapsulation of each stage of a more complex machine learning architecture\n- This means that teams can collaborate more easily using this architecture\n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ML-Part-4/mlflow-project.png\" style=\"height: 400px; margin: 20px\"/></div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9adfbc28-b501-4290-91cc-1ab4d3873e07"}}},{"cell_type":"markdown","source":["### Packaging a Simple Project\n\nFirst we're going to create a simple MLflow project consisting of the following elements:<br><br>\n\n1. MLProject file\n2. Conda environment\n3. Basic machine learning script\n\nWe're going to want to be able to pass parameters into this code so that we can try different hyperparameter options."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4fd7bc34-5180-4bb8-a71f-3b53d2d122ad"}}},{"cell_type":"markdown","source":["Create a new experiment for this exercise.  Navigate to the UI in another tab."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4ac5ae7-ed85-4e63-8c86-5bd8b2183944"}}},{"cell_type":"code","source":["experimentPath = \"/Users/\" + username + \"/experiment-ILL3\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd4b6208-abed-4544-b53c-204fa2668840"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import mlflow\n\nmlflow.set_experiment(experimentPath)\nmlflow_client = mlflow.tracking.MlflowClient()\nexperimentID = mlflow_client.get_experiment_by_name(name=experimentPath).experiment_id\n\nprint(f\"The experiment can be found at the path `{experimentPath}` and has an experiment_id of `{experimentID}`\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3901192-21c4-44d2-b4fc-a4655fd27b75"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\nFirst, examine the code we're going to run.  This looks similar to what we ran in the last lesson with the addition of decorators from the `click` library.  This allows us to parameterize our code.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> We'll uncomment out the `__main__` block when we save this code as a Python file.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Check out the <a href=\"https://click.palletsprojects.com/en/7.x/\" target=\"_blank\">`click` docs here.</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2316cf0f-2151-4183-8353-e7e5bfa2328e"}}},{"cell_type":"code","source":["import click\nimport mlflow.sklearn\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\n@click.command()\n@click.option(\"--data_path\", default=\"/dbfs/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\", type=str)\n@click.option(\"--n_estimators\", default=10, type=int)\n@click.option(\"--max_depth\", default=20, type=int)\n@click.option(\"--max_features\", default=\"auto\", type=str)\ndef mlflow_rf(data_path, n_estimators, max_depth, max_features):\n\n  with mlflow.start_run() as run:\n    # Import the data\n    df = pd.read_csv(data_path)\n    X_train, X_test, y_train, y_test = train_test_split(df.drop([\"price\"], axis=1), df[[\"price\"]].values.ravel(), random_state=42)\n    \n    # Create model, train it, and create predictions\n    rf = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features)\n    rf.fit(X_train, y_train)\n    predictions = rf.predict(X_test)\n\n    # Log model\n    mlflow.sklearn.log_model(rf, \"random-forest-model\")\n    \n    # Log params\n    mlflow.log_param(\"n_estimators\", n_estimators)\n    mlflow.log_param(\"max_depth\", max_depth)\n    mlflow.log_param(\"max_features\", max_features)\n\n    # Log metrics\n    mlflow.log_metric(\"mse\", mean_squared_error(y_test, predictions))\n    mlflow.log_metric(\"mae\", mean_absolute_error(y_test, predictions))  \n    mlflow.log_metric(\"r2\", r2_score(y_test, predictions))  \n\n# if __name__ == \"__main__\":\n#   mlflow_rf() # Note that this does not need arguments thanks to click"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"796f1ffc-1744-4ad9-af1e-b94478b138a7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Test that it works using the `click` `CliRunner`, which will execute the code in the same way we expect to."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2599dea9-22b4-442d-b8a9-97edc5ffb8f4"}}},{"cell_type":"code","source":["from click.testing import CliRunner\n\nrunner = CliRunner()\nresult = runner.invoke(mlflow_rf, ['--n_estimators', 10, '--max_depth', 20], catch_exceptions=True)\n\nassert result.exit_code == 0, \"Code failed\" # Check to see that it worked\n\nprint(\"Success!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55566bee-00c3-4482-8305-e52a5c4d43e8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now create a directory to hold our project files.  This will be a unique directory for this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"140883e9-c37f-4076-aefd-dc583fc39846"}}},{"cell_type":"code","source":["# Adust our working directory from what DBFS sees to what python actually sees\nworking_path = workingDir.replace(\"dbfs:\", \"/dbfs\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef0295cf-1f02-4b70-aec1-55faaa633641"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\nCreate the `MLproject` file.  This is the heart of an MLflow project.  It includes pointers to the conda environment and a `main` entry point, which is backed by the file `train.py`.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Any `.py` or `.sh` file can be an entry point."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6d0d2db-500c-4003-b968-7498b5eab604"}}},{"cell_type":"code","source":["dbutils.fs.put(f\"{workingDir}/MLproject\", \n'''\nname: Lesson-3-Model-Training\n\nconda_env: conda.yaml\n\nentry_points:\n  main:\n    parameters:\n      data_path: {type: str, default: \"/dbfs/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\"}\n      n_estimators: {type: int, default: 10}\n      max_depth: {type: int, default: 20}\n      max_features: {type: str, default: \"auto\"}\n    command: \"python train.py --data_path {data_path} --n_estimators {n_estimators} --max_depth {max_depth} --max_features {max_features}\"\n'''.strip())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45f02ff4-3c1f-4d1a-b267-3d8c78b92d85"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\nCreate the conda environment.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> You can also dynamically view and use a package version by calling `.__version__` on the package."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"642b2d3f-f481-4485-8902-4c0472f8e16a"}}},{"cell_type":"code","source":["import cloudpickle, numpy, pandas, sklearn\n\nfile_contents = f\"\"\"\nname: Lesson-03\nchannels:\n  - defaults\ndependencies:\n  - cloudpickle={cloudpickle.__version__}\n  - numpy={numpy.__version__}\n  - pandas={pandas.__version__}\n  - scikit-learn={sklearn.__version__}\n  - pip:\n    - mlflow=={mlflow.__version__}\n\"\"\".strip()\n\ndbutils.fs.put(f\"{workingDir}/conda.yaml\", file_contents, overwrite=True)\n\nprint(file_contents)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da2d11d3-f34c-4d9f-9c0e-96555adbf0b8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now create the code itself.  This is the same as above except for with the `__main__` is included.  Note how there are no arguments passed into `mlflow_rf()` on the final line.  `click` is handling the arguments for us."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"315c7fda-115c-4956-bec0-dd02188ccf52"}}},{"cell_type":"code","source":["dbutils.fs.put(f\"{workingDir}/train.py\", \n'''\nimport click\nimport mlflow.sklearn\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\n@click.command()\n@click.option(\"--data_path\", default=\"/dbfs/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\", type=str)\n@click.option(\"--n_estimators\", default=10, type=int)\n@click.option(\"--max_depth\", default=20, type=int)\n@click.option(\"--max_features\", default=\"auto\", type=str)\ndef mlflow_rf(data_path, n_estimators, max_depth, max_features):\n\n  with mlflow.start_run() as run:\n    # Import the data\n    df = pd.read_csv(data_path)\n    X_train, X_test, y_train, y_test = train_test_split(df.drop([\"price\"], axis=1), df[[\"price\"]].values.ravel(), random_state=42)\n    \n    # Create model, train it, and create predictions\n    rf = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features)\n    rf.fit(X_train, y_train)\n    predictions = rf.predict(X_test)\n\n    # Log model\n    mlflow.sklearn.log_model(rf, \"random-forest-model\")\n    \n    # Log params\n    mlflow.log_param(\"n_estimators\", n_estimators)\n    mlflow.log_param(\"max_depth\", max_depth)\n    mlflow.log_param(\"max_features\", max_features)\n\n    # Log metrics\n    mlflow.log_metric(\"mse\", mean_squared_error(y_test, predictions))\n    mlflow.log_metric(\"mae\", mean_absolute_error(y_test, predictions))  \n    mlflow.log_metric(\"r2\", r2_score(y_test, predictions))  \n\nif __name__ == \"__main__\":\n  mlflow_rf() # Note that this does not need arguments thanks to click\n'''.strip(), True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2187be1b-5c72-4932-a58e-6fe895cc25fd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["To summarize, you now have three files: `MLproject`, `conda.yaml`, and `train.py`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8aa21b88-0dfa-4610-8a30-9dfb0eab0ecd"}}},{"cell_type":"code","source":["display( dbutils.fs.ls(workingDir) )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b565f9b-8385-49fe-9444-09851ab918c3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n### Running Projects\n\nNow you have the three files we need to run the project, we can trigger the run.  We'll do this in a few different ways:<br><br>\n\n1. On the driver node of our Spark cluster\n2. On a new Spark cluster submitted as a job\n3. Using files backed by GitHub\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> This currently relies on environment variables.  [See the setup script for details.]($./Includes/MLflow)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a9bb9c3-618b-47e8-884d-ca2dea3cfd54"}}},{"cell_type":"markdown","source":["-sandbox\nNow run the experiment.  This command will execute against the driver node of a Spark cluster, though it could be running locally or on a different remote VM.\n\nFirst set the experiment using the `experimentPath` defined earlier.  Prepend `/dbfs` to the file path, which allows the cluster's file system to access DBFS.  Then, pass your parameters.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> This will take a few minutes to build the environment for the first time.  Subsequent runs are faster since `mlflow` can reuse the same environment after it has been built."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19eb2a1b-a05f-4091-b25d-c08ac2ea7081"}}},{"cell_type":"code","source":["import mlflow\n\nmlflow.projects.run(uri=working_path,\n  parameters={\n    \"data_path\": \"/dbfs/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\",\n    \"n_estimators\": 10,\n    \"max_depth\": 20,\n    \"max_features\": \"auto\"\n})"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c256d9f6-7276-4717-b402-c890d1dc06ae"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Check the run in the UI.  Notice that you can see the run command.  **This is very helpful in debugging.**\n\nNow that it's working, experiment with other parameters."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"740b1999-882c-4598-baba-e9eb97a9cb56"}}},{"cell_type":"code","source":["mlflow.projects.run(working_path,\n  parameters={\n    \"data_path\": \"/dbfs/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\",\n    \"n_estimators\": 1000,\n    \"max_depth\": 10,\n    \"max_features\": \"log2\"\n})"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"962e0216-bc13-4b9a-9348-300878808d36"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["How did the new model do?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a0ccdb13-997f-4436-a014-d2d04123a480"}}},{"cell_type":"markdown","source":["-sandbox\nNow try executing this code against a new Databricks cluster.  This needs cluster specifications in order for Databricks to know what kind of cluster to use.  Uncomment the following to run it.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/>  <a href=\"https://docs.databricks.com/api/latest/clusters.html\" target=\"_blank\">See the clusters API docs</a> to see how to define cluster specifications."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f2822b4-be87-402d-9e06-8b068808675c"}}},{"cell_type":"markdown","source":["Finally, run this example, which is <a href=\"https://github.com/mlflow/mlflow-example\" target=\"_blank\">a project backed by GitHub.</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da78aa01-73f2-4280-a9a8-4cb82f232036"}}},{"cell_type":"code","source":["mlflow.run(\n  uri=\"https://github.com/mlflow/mlflow-example\",\n  parameters={'alpha':0.4}\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db18eb5d-0914-415c-be0a-4ff7f16da0a0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Review\n\n**Question:** Why is packaging important?  \n**Answer:** Packaging not only manages your code but the environment in which it was run.  This environment can be a Conda or Docker environment.  This ensures that you have reproducible code and models that can be used in a number of downstream environments.\n\n**Question:** What are the core components of MLflow projects?  \n**Answer:** An MLmodel specifies the project components using YAML.  The environment file contains specifics about the environment.  The code itself contains the steps to create a model or process data.\n\n**Question:** What code can I run and where can I run it?  \n**Answer:** Arbitrary code can be run in any number of different languages.  It can be run locally or remotely, whether on a remote VM, Spark cluster, or submitted as a Databricks job."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e4d5ae93-c25e-433d-baa0-4b2431c2aad2"}}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"068a3900-8de6-4d55-bc24-bd30d58d1842"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Cleanup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b632af4c-7abf-4b0a-82e3-17f32c0e9985"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Next Steps\n\nStart the labs for this lesson, [Packaging ML Projects Lab]($./Labs/03-Lab)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac0c51d5-645b-43c8-bd13-9b150f7bb1b9"}}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n**Q:** Where can I find out more information on MLflow Projects?  \n**A:** Check out the <a href=\"https://www.mlflow.org/docs/latest/projects.html\" target=\"_blank\">MLflow docs</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"442470f4-3e58-4503-9741-c93bd8042028"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3eefbd2c-6cfc-4e36-b070-2d7cada0ad49"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"03-Packaging-ML-Projects","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2854191802662337}},"nbformat":4,"nbformat_minor":0}
