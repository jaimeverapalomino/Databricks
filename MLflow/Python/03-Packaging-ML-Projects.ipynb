{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3988ed6f-471b-4a23-874f-29c3a4d74ccc"}}},{"cell_type":"markdown","source":["# Packaging ML Projects\n\nMachine learning projects need to produce both reusable code and reproducible results.  This lesson examines creating, organizing, and packaging machine learning projects with a focus on reproducibility and collaborating with a team.\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n - Introduce organizing code into projects\n - Package a basic project with parameters and an environment\n - Run a basic project locally and remotely using Github\n\n## Prerequisites\n- Web browser: Chrome\n- A cluster configured with **8 cores** and **DBR 7.0 ML**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4fdf2a5-e9ac-4c9b-b992-47a5e7733c52"}}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the<br/>\nstart of each lesson (see the next cell) and the **`Classroom-Cleanup`** cell at the end of each lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12023626-220b-469c-af9a-e1107c2cf9c9"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"363feb54-67d1-41a9-9bd6-48bda804ba2a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n### The Case for Packaging\n\nThere are a number of different reasons why teams need to package their machine learning projects:<br><br>\n\n1. Projects have various library dependencies \n  - Shipping a machine learning solution involves the environment in which it was built\n  - MLflow allows for this environment to be a conda environment or docker container\n  - This means that teams can easily share and publish their code for others to use\n2. Machine learning projects become increasingly complex as time goes on\n  - This includes ETL and featurization steps, machine learning models used for pre-processing, and finally the model training itself\n3. Each component of a machine learning pipeline needs to allow for tracing its lineage\n  - If there's a failure at some point, tracing the full end-to-end lineage of a model allows for easier debugging."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3b5b183-6099-4c55-8894-b68f3c6edba5"}}},{"cell_type":"markdown","source":["-sandbox\n**ML Projects is a specification for how to organize code in a project.**<br><br>\n\n- The heart of this is an **MLproject file,** a YAML specification for the components of the ML project\n- This allows for more complex workflows since a project can execute another project\n   - This allows for encapsulation of each stage of a more complex machine learning architecture\n- This means that teams can collaborate more easily using this architecture\n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ML-Part-4/mlflow-project.png\" style=\"height: 400px; margin: 20px\"/></div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4505c9f-5818-4f55-a905-94ed06c39927"}}},{"cell_type":"markdown","source":["### Packaging a Simple Project\n\nFirst we're going to create a simple MLflow project consisting of the following elements:<br><br>\n\n1. MLProject file\n2. Conda environment\n3. Basic machine learning script\n\nWe're going to want to be able to pass parameters into this code so that we can try different hyperparameter options."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f37531ad-fdf2-4322-ba56-cd8b2b7a07aa"}}},{"cell_type":"markdown","source":["Create a new experiment for this exercise.  Navigate to the UI in another tab."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97d30492-8ab7-43eb-9aa1-cc3455c9e6f4"}}},{"cell_type":"code","source":["experimentPath = \"/Users/\" + username + \"/experiment-ILL3\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"69446430-5c7c-443e-b190-0d18ce0af9e9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import mlflow\n\nmlflow.set_experiment(experimentPath)\nmlflow_client = mlflow.tracking.MlflowClient()\nexperimentID = mlflow_client.get_experiment_by_name(name=experimentPath).experiment_id\n\nprint(f\"The experiment can be found at the path `{experimentPath}` and has an experiment_id of `{experimentID}`\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"703e6601-78dd-45e0-b9aa-62070669fa0c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\nFirst, examine the code we're going to run.  This looks similar to what we ran in the last lesson with the addition of decorators from the `click` library.  This allows us to parameterize our code.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> We'll uncomment out the `__main__` block when we save this code as a Python file.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Check out the <a href=\"https://click.palletsprojects.com/en/7.x/\" target=\"_blank\">`click` docs here.</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80a65945-cc62-40cb-99a2-d656d1d5cce8"}}},{"cell_type":"code","source":["import click\nimport mlflow.sklearn\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\n@click.command()\n@click.option(\"--data_path\", default=\"/dbfs/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\", type=str)\n@click.option(\"--n_estimators\", default=10, type=int)\n@click.option(\"--max_depth\", default=20, type=int)\n@click.option(\"--max_features\", default=\"auto\", type=str)\ndef mlflow_rf(data_path, n_estimators, max_depth, max_features):\n\n  with mlflow.start_run() as run:\n    # Import the data\n    df = pd.read_csv(data_path)\n    X_train, X_test, y_train, y_test = train_test_split(df.drop([\"price\"], axis=1), df[[\"price\"]].values.ravel(), random_state=42)\n    \n    # Create model, train it, and create predictions\n    rf = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features)\n    rf.fit(X_train, y_train)\n    predictions = rf.predict(X_test)\n\n    # Log model\n    mlflow.sklearn.log_model(rf, \"random-forest-model\")\n    \n    # Log params\n    mlflow.log_param(\"n_estimators\", n_estimators)\n    mlflow.log_param(\"max_depth\", max_depth)\n    mlflow.log_param(\"max_features\", max_features)\n\n    # Log metrics\n    mlflow.log_metric(\"mse\", mean_squared_error(y_test, predictions))\n    mlflow.log_metric(\"mae\", mean_absolute_error(y_test, predictions))  \n    mlflow.log_metric(\"r2\", r2_score(y_test, predictions))  \n\n# if __name__ == \"__main__\":\n#   mlflow_rf() # Note that this does not need arguments thanks to click"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f63a1ac-a335-4d21-90f1-9173312c9f22"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Test that it works using the `click` `CliRunner`, which will execute the code in the same way we expect to."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1aad4d24-ed37-41a9-9f9c-41b2366db849"}}},{"cell_type":"code","source":["from click.testing import CliRunner\n\nrunner = CliRunner()\nresult = runner.invoke(mlflow_rf, ['--n_estimators', 10, '--max_depth', 20], catch_exceptions=True)\n\nassert result.exit_code == 0, \"Code failed\" # Check to see that it worked\n\nprint(\"Success!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ced2016d-d080-4758-b211-a755826a0532"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now create a directory to hold our project files.  This will be a unique directory for this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d11a30d7-70c8-4373-b401-acc3995add2f"}}},{"cell_type":"code","source":["# Adust our working directory from what DBFS sees to what python actually sees\nworking_path = workingDir.replace(\"dbfs:\", \"/dbfs\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b0ab157-04ea-4b88-8db6-79c3d456f559"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\nCreate the `MLproject` file.  This is the heart of an MLflow project.  It includes pointers to the conda environment and a `main` entry point, which is backed by the file `train.py`.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Any `.py` or `.sh` file can be an entry point."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b195ef6a-acaa-49d7-b41c-0d0688170314"}}},{"cell_type":"code","source":["dbutils.fs.put(f\"{workingDir}/MLproject\", \n'''\nname: Lesson-3-Model-Training\n\nconda_env: conda.yaml\n\nentry_points:\n  main:\n    parameters:\n      data_path: {type: str, default: \"/dbfs/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\"}\n      n_estimators: {type: int, default: 10}\n      max_depth: {type: int, default: 20}\n      max_features: {type: str, default: \"auto\"}\n    command: \"python train.py --data_path {data_path} --n_estimators {n_estimators} --max_depth {max_depth} --max_features {max_features}\"\n'''.strip())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cfdee3ea-19ba-4746-9ea5-63192cd29a32"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\nCreate the conda environment.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> You can also dynamically view and use a package version by calling `.__version__` on the package."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"673a91c9-6464-4512-86f2-c6846ecf59d0"}}},{"cell_type":"code","source":["import cloudpickle, numpy, pandas, sklearn\n\nfile_contents = f\"\"\"\nname: Lesson-03\nchannels:\n  - defaults\ndependencies:\n  - cloudpickle={cloudpickle.__version__}\n  - numpy={numpy.__version__}\n  - pandas={pandas.__version__}\n  - scikit-learn={sklearn.__version__}\n  - pip:\n    - mlflow=={mlflow.__version__}\n\"\"\".strip()\n\ndbutils.fs.put(f\"{workingDir}/conda.yaml\", file_contents, overwrite=True)\n\nprint(file_contents)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6fe0a9d3-d60e-4731-8abb-ae47d36091e2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now create the code itself.  This is the same as above except for with the `__main__` is included.  Note how there are no arguments passed into `mlflow_rf()` on the final line.  `click` is handling the arguments for us."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e27fabe3-7c52-40cf-8610-812fed93ac72"}}},{"cell_type":"code","source":["dbutils.fs.put(f\"{workingDir}/train.py\", \n'''\nimport click\nimport mlflow.sklearn\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\n@click.command()\n@click.option(\"--data_path\", default=\"/dbfs/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\", type=str)\n@click.option(\"--n_estimators\", default=10, type=int)\n@click.option(\"--max_depth\", default=20, type=int)\n@click.option(\"--max_features\", default=\"auto\", type=str)\ndef mlflow_rf(data_path, n_estimators, max_depth, max_features):\n\n  with mlflow.start_run() as run:\n    # Import the data\n    df = pd.read_csv(data_path)\n    X_train, X_test, y_train, y_test = train_test_split(df.drop([\"price\"], axis=1), df[[\"price\"]].values.ravel(), random_state=42)\n    \n    # Create model, train it, and create predictions\n    rf = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features)\n    rf.fit(X_train, y_train)\n    predictions = rf.predict(X_test)\n\n    # Log model\n    mlflow.sklearn.log_model(rf, \"random-forest-model\")\n    \n    # Log params\n    mlflow.log_param(\"n_estimators\", n_estimators)\n    mlflow.log_param(\"max_depth\", max_depth)\n    mlflow.log_param(\"max_features\", max_features)\n\n    # Log metrics\n    mlflow.log_metric(\"mse\", mean_squared_error(y_test, predictions))\n    mlflow.log_metric(\"mae\", mean_absolute_error(y_test, predictions))  \n    mlflow.log_metric(\"r2\", r2_score(y_test, predictions))  \n\nif __name__ == \"__main__\":\n  mlflow_rf() # Note that this does not need arguments thanks to click\n'''.strip(), True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"71fb3a0e-df52-47a7-804a-7a5425da31fd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["To summarize, you now have three files: `MLproject`, `conda.yaml`, and `train.py`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ce4c5cb-88f2-41a0-b91c-0dd15a7587e5"}}},{"cell_type":"code","source":["display( dbutils.fs.ls(workingDir) )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"693ba915-4644-4835-ab81-643cbea6efa9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n### Running Projects\n\nNow you have the three files we need to run the project, we can trigger the run.  We'll do this in a few different ways:<br><br>\n\n1. On the driver node of our Spark cluster\n2. On a new Spark cluster submitted as a job\n3. Using files backed by GitHub\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> This currently relies on environment variables.  [See the setup script for details.]($./Includes/MLflow)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"29cc8b91-c6ff-4088-b9a5-29df28079dec"}}},{"cell_type":"markdown","source":["-sandbox\nNow run the experiment.  This command will execute against the driver node of a Spark cluster, though it could be running locally or on a different remote VM.\n\nFirst set the experiment using the `experimentPath` defined earlier.  Prepend `/dbfs` to the file path, which allows the cluster's file system to access DBFS.  Then, pass your parameters.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> This will take a few minutes to build the environment for the first time.  Subsequent runs are faster since `mlflow` can reuse the same environment after it has been built."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"021d5389-fce6-4594-b057-8e67b8af28ed"}}},{"cell_type":"code","source":["import mlflow\n\nmlflow.projects.run(uri=working_path,\n  parameters={\n    \"data_path\": \"/dbfs/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\",\n    \"n_estimators\": 10,\n    \"max_depth\": 20,\n    \"max_features\": \"auto\"\n})"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f45e69a-6fec-4e3c-962b-672dced2d3fa"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Check the run in the UI.  Notice that you can see the run command.  **This is very helpful in debugging.**\n\nNow that it's working, experiment with other parameters."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6965131d-e4c7-4cc3-99ef-d193c675d6d3"}}},{"cell_type":"code","source":["mlflow.projects.run(working_path,\n  parameters={\n    \"data_path\": \"/dbfs/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\",\n    \"n_estimators\": 1000,\n    \"max_depth\": 10,\n    \"max_features\": \"log2\"\n})"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5713567f-b17b-43ef-8b52-91702882200c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["How did the new model do?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4569b9ec-2551-4085-bfe9-7beafff35610"}}},{"cell_type":"markdown","source":["-sandbox\nNow try executing this code against a new Databricks cluster.  This needs cluster specifications in order for Databricks to know what kind of cluster to use.  Uncomment the following to run it.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/>  <a href=\"https://docs.databricks.com/api/latest/clusters.html\" target=\"_blank\">See the clusters API docs</a> to see how to define cluster specifications."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"149d21b0-800c-41e6-b99e-ea640161881e"}}},{"cell_type":"markdown","source":["Finally, run this example, which is <a href=\"https://github.com/mlflow/mlflow-example\" target=\"_blank\">a project backed by GitHub.</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"680a0882-04df-4e10-8f59-ec8a3e6ef3a3"}}},{"cell_type":"code","source":["mlflow.run(\n  uri=\"https://github.com/mlflow/mlflow-example\",\n  parameters={'alpha':0.4}\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30894588-cb3d-4ebf-b238-728648fbdcac"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Review\n\n**Question:** Why is packaging important?  \n**Answer:** Packaging not only manages your code but the environment in which it was run.  This environment can be a Conda or Docker environment.  This ensures that you have reproducible code and models that can be used in a number of downstream environments.\n\n**Question:** What are the core components of MLflow projects?  \n**Answer:** An MLmodel specifies the project components using YAML.  The environment file contains specifics about the environment.  The code itself contains the steps to create a model or process data.\n\n**Question:** What code can I run and where can I run it?  \n**Answer:** Arbitrary code can be run in any number of different languages.  It can be run locally or remotely, whether on a remote VM, Spark cluster, or submitted as a Databricks job."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"280e93ea-1855-4fae-8987-a9ea30a59837"}}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b43c0bd4-6e5b-4f4f-b17d-06f7ce479b76"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Cleanup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"491eb364-bb82-483e-a9a7-972cedf08ce9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Next Steps\n\nStart the labs for this lesson, [Packaging ML Projects Lab]($./Labs/03-Lab)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f16fd90-9064-48a2-afa7-338ce3b723ec"}}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n**Q:** Where can I find out more information on MLflow Projects?  \n**A:** Check out the <a href=\"https://www.mlflow.org/docs/latest/projects.html\" target=\"_blank\">MLflow docs</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be4f8389-b3f6-4f8d-ab24-2ea7ae039f68"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c75b8eae-e8cb-44b7-b3f4-7a1df527b41d"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"03-Packaging-ML-Projects","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2854191802662914}},"nbformat":4,"nbformat_minor":0}
