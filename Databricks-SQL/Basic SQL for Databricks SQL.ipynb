{"cells":[{"cell_type":"markdown","source":["## <span style='color:Blue'> CREATE DATABASE & TABLE </span>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8e726d7-742b-4d05-8e48-f05a35844d91"}}},{"cell_type":"code","source":["%sql\n\nCREATE DATABASE IF NOT EXISTS dbacademy;\n-- The following grants SELECT, CREATE, MODIFY, READ_METADATA, and CREATE_NAMED_FUNCTION privileges to the learner for this database\n-- **You MUST change the username to the learner's username**\n--GRANT ALL PRIVILEGES ON DATABASE dbacademy TO `jaime.vera.palomino@gmail.com`;\nUSE dbacademy;\n--The following ensures the tables use the latest data set\nDROP TABLE IF EXISTS basic_sql_for_databricks_sql_customers_csv;\nDROP TABLE IF EXISTS basic_sql_for_databricks_sql_customers;\nDROP TABLE IF EXISTS basic_sql_for_databricks_sql_loyalty_segments;\nDROP TABLE IF EXISTS basic_sql_for_databricks_sql_loyalty_segments_csv;\nDROP TABLE IF EXISTS basic_sql_for_databricks_sql_sales_gold;\nDROP TABLE IF EXISTS basic_sql_for_databricks_sql_silver_promo_prices;\nDROP TABLE IF EXISTS basic_sql_for_databricks_sql_silver_purchase_orders;\nDROP TABLE IF EXISTS basic_sql_for_databricks_sql_silver_sales_orders;\nDROP TABLE IF EXISTS basic_sql_for_databricks_sql_source_silver_suppliers;\nDROP TABLE IF EXISTS intro_to_databricks_sql_gym_logs;\n-- The following creates a table for use in the current course\n-- Data for the table comes from a read-only object store\n\n-- Gym Logs table\nCREATE TABLE intro_to_databricks_sql_gym_logs \n    USING JSON\n    LOCATION 'wasbs://courseware@dbacademy.blob.core.windows.net/introduction-to-databricks-sql/v01/gym-logs';\n-- The following ensures everything worked correctly\nSELECT * FROM intro_to_databricks_sql_gym_logs;\n\n-- Customers table\nCREATE TABLE basic_sql_for_databricks_sql_customers_csv\n  USING csv \n  OPTIONS (\n    path \"wasbs://courseware@dbacademy.blob.core.windows.net/basic-sql-for-databricks-sql/v01/retail-org/customers\",\n    header \"true\",\n    inferSchema \"true\"\n);\n\nCREATE TABLE basic_sql_for_databricks_sql_customers AS\n  SELECT * FROM basic_sql_for_databricks_sql_customers_csv;\nDROP TABLE basic_sql_for_databricks_sql_customers_csv;\n\n-- Loyalty Segments table\nCREATE TABLE basic_sql_for_databricks_sql_loyalty_segments_csv\n  USING csv \n  OPTIONS (\n    path \"wasbs://courseware@dbacademy.blob.core.windows.net/basic-sql-for-databricks-sql/v01/retail-org/loyalty_segments\",\n    header \"true\",\n    inferSchema \"true\"\n);\n\nCREATE TABLE basic_sql_for_databricks_sql_loyalty_segments AS\n  SELECT * FROM basic_sql_for_databricks_sql_loyalty_segments_csv;\nDROP TABLE basic_sql_for_databricks_sql_loyalty_segments_csv;\n\nCREATE TABLE basic_sql_for_databricks_sql_sales_gold AS\n  SELECT * FROM delta.`wasbs://courseware@dbacademy.blob.core.windows.net/basic-sql-for-databricks-sql/v01/retail-org/solutions/gold/sales`;\nCREATE TABLE basic_sql_for_databricks_sql_silver_promo_prices AS\n  SELECT * FROM delta.`wasbs://courseware@dbacademy.blob.core.windows.net/basic-sql-for-databricks-sql/v01/retail-org/solutions/silver/promo_prices`;\nCREATE TABLE basic_sql_for_databricks_sql_silver_purchase_orders AS\n  SELECT * FROM delta.`wasbs://courseware@dbacademy.blob.core.windows.net/basic-sql-for-databricks-sql/v01/retail-org/solutions/silver/purchase_orders.delta`;\nCREATE TABLE basic_sql_for_databricks_sql_silver_sales_orders AS\n  SELECT * FROM delta.`wasbs://courseware@dbacademy.blob.core.windows.net/basic-sql-for-databricks-sql/v01/retail-org/solutions/silver/sales_orders`;\nCREATE TABLE basic_sql_for_databricks_sql_silver_suppliers AS\n  SELECT * FROM delta.`wasbs://courseware@dbacademy.blob.core.windows.net/basic-sql-for-databricks-sql/v01/retail-org/solutions/silver/suppliers`;\nCREATE TABLE basic_sql_for_databricks_sql_source_silver_suppliers AS\n  SELECT * FROM delta.`wasbs://courseware@dbacademy.blob.core.windows.net/basic-sql-for-databricks-sql/v01/retail-org/solutions/silver/suppliers`;\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de8f6ab4-5863-478d-8626-41db23a8ea74"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: Cannot create table ('`dbacademy`.`basic_sql_for_databricks_sql_customers`'). The associated location ('dbfs:/user/hive/warehouse/dbacademy.db/basic_sql_for_databricks_sql_customers') is not empty but it's not a Delta table\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.assertPathEmpty(CreateDeltaTableCommand.scala:258)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:133)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:122)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:395)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:484)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:504)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:479)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:404)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:395)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:367)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:57)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:137)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:71)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:58)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:98)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:429)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:408)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:53)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:120)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:104)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:53)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:122)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:203)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:602)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:515)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1654)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:500)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:495)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicCreateTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:104)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicCreateTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:126)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:41)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:41)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:101)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:689)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:684)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$11(DriverLocal.scala:526)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:50)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:50)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:503)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:689)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:681)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:522)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:634)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:427)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:370)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:221)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:130)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$11(DriverLocal.scala:526)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:50)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:50)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:503)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:689)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:681)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:522)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:634)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:427)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:370)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:221)\n\tat java.lang.Thread.run(Thread.java:748)\n","errorSummary":"Error in SQL statement: AnalysisException: Cannot create table ('`dbacademy`.`basic_sql_for_databricks_sql_customers`'). The associated location ('dbfs:/user/hive/warehouse/dbacademy.db/basic_sql_for_databricks_sql_customers') is not empty but it's not a Delta table","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\ncom.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: Cannot create table ('`dbacademy`.`basic_sql_for_databricks_sql_customers`'). The associated location ('dbfs:/user/hive/warehouse/dbacademy.db/basic_sql_for_databricks_sql_customers') is not empty but it's not a Delta table\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.assertPathEmpty(CreateDeltaTableCommand.scala:258)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:133)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:122)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:395)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:484)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:504)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:479)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:404)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:395)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:367)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:57)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:137)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:71)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:58)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:98)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:429)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:408)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:53)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:120)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:104)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:53)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:122)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:203)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:602)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:515)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1654)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:500)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:495)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicCreateTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:104)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicCreateTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:126)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:41)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:41)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:101)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:689)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:684)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$11(DriverLocal.scala:526)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:50)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:50)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:503)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:689)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:681)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:522)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:634)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:427)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:370)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:221)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:130)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$11(DriverLocal.scala:526)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:50)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:50)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:503)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:689)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:681)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:522)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:634)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:427)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:370)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:221)\n\tat java.lang.Thread.run(Thread.java:748)"]}}],"execution_count":0},{"cell_type":"markdown","source":["## <span style='color:Blue'> Retrieving Data </span>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa4dc181-301f-468c-8b88-8ce051d2cf3a"}}},{"cell_type":"code","source":["%sql\n-- SELECT ... FROM\n-- SELECT * FROM dbacademy.basic_sql_for_databricks_sql_customers;\n\n-- SELECT specific columns and AS\n-- SELECT customer_name AS Customer FROM dbacademy.basic_sql_for_databricks_sql_customers;\n\n-- DISTINCT\n-- SELECT DISTINCT state FROM dbacademy.basic_sql_for_databricks_sql_customers;\n\n-- WHERE\n-- SELECT * FROM dbacademy.basic_sql_for_databricks_sql_customers WHERE loyalty_segment = 3;\n\n-- GROUP BY\n-- This query will fail:\n-- SELECT * FROM dbacademy.basic_sql_for_databricks_sql_customers GROUP BY loyalty_segment;\n\n-- GROUP BY (cont.)\n-- SELECT loyalty_segment, count(loyalty_segment) from dbacademy.basic_sql_for_databricks_sql_customers GROUP BY loyalty_segment;\n\n-- ORDER BY\n-- SELECT loyalty_segment, count(loyalty_segment) from dbacademy.basic_sql_for_databricks_sql_customers GROUP BY loyalty_segment ORDER BY loyalty_segment;\n\n-- HAVING\n-- SELECT loyalty_segment, count(loyalty_segment) AS loyalty_count from dbacademy.basic_sql_for_databricks_sql_customers GROUP BY loyalty_segment HAVING loyalty_count > 4000 ORDER BY loyalty_segment;\n\n-- SELECT on Delta tables\n-- DESCRIBE HISTORY dbacademy.basic_sql_for_databricks_sql_customers;\n\n-- UPDATE dbacademy.basic_sql_for_databricks_sql_customers SET loyalty_segment = 10 WHERE loyalty_segment = 0;\n-- DESCRIBE HISTORY dbacademy.basic_sql_for_databricks_sql_customers;\n\n-- UPDATE dbacademy.basic_sql_for_databricks_sql_customers SET loyalty_segment = 0 WHERE loyalty_segment = 10;\n-- DESCRIBE HISTORY dbacademy.basic_sql_for_databricks_sql_customers;\n\n-- TIMESTAMP AS OF\n-- You will need to change the timestamp to a time that works with your tables' history\n-- SELECT loyalty_segment FROM dbacademy.basic_sql_for_databricks_sql_customers TIMESTAMP AS OF '2021-11-18T01:39:30.013Z';\n\n-- VERSION AS OF\n-- You may need to change the version number to a number that works with your table's history\n-- SELECT loyalty_segment FROM dbacademy.basic_sql_for_databricks_sql_customers VERSION AS OF 1;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dff65a9e-13b7-49c1-aa4c-bdddd3a180da"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## <span style='color:Blue'> Column Expressions </span>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c0016785-125c-447b-a2a6-a402c2a3bc1d"}}},{"cell_type":"code","source":["%sql\n-- Mathematical expressions of two columns\n-- SELECT * FROM dbacademy.basic_sql_for_databricks_sql_silver_promo_prices;\n-- SELECT sales_price - sales_price * promo_disc AS Calculated_Discount, discounted_price AS Discounted_Price FROM dbacademy.basic_sql_for_databricks_sql_silver_promo_prices;\n\n-- Built-in Functions\n-- https://docs.databricks.com/sql/language-manual/sql-ref-functions-builtin.html\n\n-- String column manipulation\n-- SELECT * FROM dbacademy.basic_sql_for_databricks_sql_customers;\n-- SELECT lower(city) AS City FROM dbacademy.basic_sql_for_databricks_sql_customers;\n-- SELECT initcap(lower(city)) AS City FROM dbacademy.basic_sql_for_databricks_sql_customers;\n\n-- Date Functions\n-- SELECT * FROM dbacademy.basic_sql_for_databricks_sql_silver_promo_prices;\n-- SELECT from_unixtime(promo_began) FROM dbacademy.basic_sql_for_databricks_sql_silver_promo_prices;\n-- Datetime Patterns: https://docs.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-datetime-pattern.html\n-- SELECT from_unixtime(promo_began, \"d MMM, y\") AS Beginning_Date FROM dbacademy.basic_sql_for_databricks_sql_silver_promo_prices;\n\n-- Date Calculations\n-- SELECT * FROM dbacademy.basic_sql_for_databricks_sql_silver_promo_prices;\n-- SELECT current_date() - to_date(from_unixtime(promo_began)) FROM dbacademy.basic_sql_for_databricks_sql_silver_promo_prices;\n\n-- CASE WHEN\n-- SELECT customer_name, loyalty_segment,\n--  CASE \n--     WHEN loyalty_segment = 0 THEN 'Rare'\n--     WHEN loyalty_segment = 1 THEN 'Occasional'\n--     WHEN loyalty_segment = 2 THEN 'Frequent'\n--     WHEN loyalty_segment = 3 THEN 'Daily'\n--  END AS Loyalty \n--  FROM dbacademy.basic_sql_for_databricks_sql_customers;\n\n-- CASE WHEN in ORDER BY\n-- SELECT * FROM dbacademy.basic_sql_for_databricks_sql_customers WHERE state = 'UT'\n-- ORDER BY\n-- (CASE \n--     WHEN city IS NULL THEN state\n--     ELSE city\n-- END);\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"082fa71e-3e29-4eaf-932f-9ed364c8bc2b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## <span style='color:Blue'> Manipulating Stored Data </span>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f2b0bb4d-b991-4ae2-afc2-5c9c164a4602"}}},{"cell_type":"code","source":["%sql\n-- UPDATE\n-- SELECT city FROM dbacademy.basic_sql_for_databricks_sql_customers;\n-- SELECT initcap(lower(city)) AS City FROM dbacademy.basic_sql_for_databricks_sql_customers;\n-- UPDATE dbacademy.basic_sql_for_databricks_sql_customers SET city = initcap(lower(city));\n-- SELECT city FROM dbacademy.basic_sql_for_databricks_sql_customers;\n\n-- INSERT INTO\n-- SELECT * FROM dbacademy.basic_sql_for_databricks_sql_loyalty_segments;\n-- INSERT INTO dbacademy.basic_sql_for_databricks_sql_loyalty_segments \n-- (loyalty_segment_id, loyalty_segment_description, unit_threshold, valid_from, valid_to)\n-- VALUES\n-- (4, 'level_4', 100, current_date(), Null);\n\n-- INSERT TABLE\n-- SELECT * FROM dbacademy.basic_sql_for_databricks_sql_silver_suppliers where password_hash = 'f6899b07c3868a5975438ee0caea6623';\n-- INSERT INTO dbacademy.basic_sql_for_databricks_sql_silver_suppliers TABLE dbacademy.basic_sql_for_databricks_sql_source_silver_suppliers; \n-- SELECT * FROM dbacademy.basic_sql_for_databricks_sql_silver_suppliers where password_hash = 'f6899b07c3868a5975438ee0caea6623';\n\n-- INSERT OVERWRITE\n-- select count(*) from  dbacademy.basic_sql_for_databricks_sql_silver_suppliers;\n-- INSERT OVERWRITE dbacademy.basic_sql_for_databricks_sql_silver_suppliers TABLE dbacademy.basic_sql_for_databricks_sql_source_silver_suppliers; \n-- select count(*) from  dbacademy.basic_sql_for_databricks_sql_silver_suppliers;\n\n-- MERGE INTO\n-- MERGE INTO dbacademy.basic_sql_for_databricks_sql_silver_suppliers\n--   USING dbacademy.basic_sql_for_databricks_sql_source_silver_suppliers\n--   ON dbacademy.basic_sql_for_databricks_sql_silver_suppliers.EAN13 = dbacademy.basic_sql_for_databricks_sql_source_silver_suppliers.EAN13\n--   WHEN NOT MATCHED THEN INSERT *\n-- UPDATE dbacademy.basic_sql_for_databricks_sql_source_silver_suppliers SET EAN13 = EAN13 + 1 WHERE EAN13 = 2198122549911;\n-- MERGE INTO dbacademy.basic_sql_for_databricks_sql_silver_suppliers\n--   USING dbacademy.basic_sql_for_databricks_sql_source_silver_suppliers\n--   ON dbacademy.basic_sql_for_databricks_sql_silver_suppliers.EAN13 = dbacademy.basic_sql_for_databricks_sql_source_silver_suppliers.EAN13\n--   WHEN NOT MATCHED THEN INSERT *"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2a8134a9-6d42-4bfe-b1b6-91863288856a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## <span style='color:Blue'> Subsetting Data Using Subqueries</span>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34a4b6ba-32f0-439f-aec2-42dd82bdf32f"}}},{"cell_type":"code","source":["%sql\n-- Subqueries\n\n-- Creating a table using a subquery\n-- CREATE TABLE dbacademy.high_loyalty_customers AS\n--     SELECT * FROM dbacademy.basic_sql_for_databricks_sql_customers WHERE loyalty_segment = 3;\n\n-- DROP TABLE dbacademy.high_loyalty_customers;\n\n-- Creating a view using a subquery\n-- CREATE VIEW dbacademy.high_loyalty_customers AS\n--     SELECT * FROM dbacademy.basic_sql_for_databricks_sql_customers WHERE loyalty_segment = 3;\n    \n-- DROP VIEW dbacademy.high_loyalty_customers;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c149431f-14b4-48ce-bf9c-35a9fbb1d238"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## <span style='color:Blue'>Working With Joins</span>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6573d50e-9187-45fb-a771-8a6edf9a6191"}}},{"cell_type":"code","source":["%sql\n-- INNER JOIN\n-- SELECT\n--   customer_name,\n--   loyalty_segment_description,\n--   unit_threshold\n-- FROM\n--   dbacademy.basic_sql_for_databricks_sql_customers\n--   INNER JOIN dbacademy.basic_sql_for_databricks_sql_loyalty_segments\n--     ON basic_sql_for_databricks_sql_customers.loyalty_segment = basic_sql_for_databricks_sql_loyalty_segments.loyalty_segment_id;\n\n-- LEFT [OUTER] JOIN\n-- SELECT\n--   basic_sql_for_databricks_sql_customers.customer_name,\n--   product_category,\n--   total_price\n-- FROM\n--   dbacademy.basic_sql_for_databricks_sql_customers\n--   LEFT JOIN dbacademy.basic_sql_for_databricks_sql_sales_gold \n--     ON basic_sql_for_databricks_sql_customers.customer_id = basic_sql_for_databricks_sql_sales_gold.customer_id\n-- WHERE\n--   state = 'NC'\n-- ORDER BY product_category DESC;\n\n-- RIGHT [OUTER] JOIN\n-- SELECT\n--   region,\n--   product_category,\n--   total_price\n-- FROM\n--   dbacademy.basic_sql_for_databricks_sql_customers\n--   RIGHT JOIN dbacademy.basic_sql_for_databricks_sql_sales_gold \n--     ON basic_sql_for_databricks_sql_customers.customer_id = basic_sql_for_databricks_sql_sales_gold.customer_id\n-- WHERE\n--     product_category = 'Sioneer'\n-- ORDER BY product_category DESC;\n\n-- FULL [OUTER] JOIN\n-- SELECT\n--   region,\n--   product_category,\n--   total_price\n-- FROM\n--   dbacademy.basic_sql_for_databricks_sql_customers\n--   FULL JOIN dbacademy.basic_sql_for_databricks_sql_sales_gold \n--     ON basic_sql_for_databricks_sql_customers.customer_id = basic_sql_for_databricks_sql_sales_gold.customer_id\n-- ORDER BY product_category DESC;\n\n-- LEFT [SEMI] JOIN\n-- SELECT\n--   *\n-- FROM\n--   dbacademy.basic_sql_for_databricks_sql_customers\n--   LEFT SEMI JOIN dbacademy.basic_sql_for_databricks_sql_sales_gold \n--     ON basic_sql_for_databricks_sql_customers.customer_id = basic_sql_for_databricks_sql_sales_gold.customer_id\n-- WHERE\n--   state = 'NC';\n  \n-- LEFT [ANTI] JOIN\n-- SELECT\n--   *\n-- FROM\n--   dbacademy.basic_sql_for_databricks_sql_customers\n--   LEFT ANTI JOIN dbacademy.basic_sql_for_databricks_sql_sales_gold \n--     ON basic_sql_for_databricks_sql_customers.customer_id = basic_sql_for_databricks_sql_sales_gold.customer_id\n-- WHERE\n--   state = 'NC';\n\n-- CROSS JOIN\n-- SELECT\n--   count(*)\n-- FROM\n--   dbacademy.basic_sql_for_databricks_sql_sales_gold;\n  \n-- SELECT\n--   count(*)\n-- FROM\n--   dbacademy.basic_sql_for_databricks_sql_customers;\n\n-- SELECT\n--   count(*)\n-- FROM\n--   dbacademy.basic_sql_for_databricks_sql_customers\n--   CROSS JOIN dbacademy.basic_sql_for_databricks_sql_sales_gold;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b264db32-db6c-483d-9e46-c77f1c8b72c7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## <span style='color:Blue'>Using Aggregations</span>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4eaac8c-2261-4a65-be68-a42d4a5e423a"}}},{"cell_type":"code","source":["%sql\n-- count()\n-- SELECT count(*) AS Number_of_Customers FROM dbacademy.basic_sql_for_databricks_sql_customers;\n\n-- sum()\n-- SELECT sum(units_purchased) AS Total_California_Units FROM dbacademy.basic_sql_for_databricks_sql_customers WHERE state = 'CA';\n\n-- min(), max()\n-- SELECT min(discounted_price) AS Lowest_Discounted_Price, max(discounted_price) AS Highest_Discounted_Price FROM dbacademy.basic_sql_for_databricks_sql_silver_promo_prices;\n\n-- avg(), mean()\n-- SELECT avg(total_price) AS Mean_Total_Price from dbacademy.basic_sql_for_databricks_sql_sales_gold;\n\n-- std(), stddev()\n-- SELECT std(total_price) AS SD_Total_Price from dbacademy.basic_sql_for_databricks_sql_sales_gold;\n\n-- var_samp(), variance()\n-- SELECT variance(total_price) AS Variance_Total_Price from dbacademy.basic_sql_for_databricks_sql_sales_gold;\n\n-- Combining built in functions\n-- SELECT price FROM dbacademy.basic_sql_for_databricks_sql_silver_purchase_orders;\n-- SELECT\n--   price AS Price,\n--   int(regexp_replace(price, '(\\\\$\\\\s)|(\\\\$)|(USD\\\\s)|(USD)', '')) AS Cleaned_USD_Price\n-- FROM\n--   dbacademy.basic_sql_for_databricks_sql_silver_purchase_orders\n-- WHERE\n--   price like '\\$%'\n--   OR price like 'USD%';\n\n-- SELECT\n--   corr(\n--     int(\n--       regexp_replace(price, '(\\\\$\\\\s)|(\\\\$)|(USD\\\\s)|(USD)', '')\n--     ),\n--     quantity\n--   ) AS Correlation_USD_Price_Quantity\n-- FROM\n--   dbacademy.basic_sql_for_databricks_sql_silver_purchase_orders\n-- WHERE\n--   price like '\\$%'\n--   OR price like 'USD%';"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c1d82c0-3927-4063-9928-7cb656b633a1"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Basic SQL for Databricks SQL","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3714102522684624}},"nbformat":4,"nbformat_minor":0}
