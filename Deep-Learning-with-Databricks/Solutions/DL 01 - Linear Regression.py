# Databricks notebook source
# MAGIC %md-sandbox
# MAGIC 
# MAGIC <div style="text-align: center; line-height: 0; padding-top: 9px;">
# MAGIC   <img src="https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png" alt="Databricks Learning" style="width: 600px">
# MAGIC </div>

# COMMAND ----------

# MAGIC %md # Linear Regression
# MAGIC 
# MAGIC Before you attempt to throw a neural network at a problem, you want to establish a __baseline model__. Often, this will be a simple model, such as linear regression. Once we establish a baseline, then we can get started with Deep Learning.
# MAGIC 
# MAGIC The slides for the course can be found <a href="https://brookewenig.github.io/DeepLearning.html" target="_blank">here</a>.
# MAGIC 
# MAGIC ## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>
# MAGIC  - Build a linear regression model using scikit-learn and reimplement it in Keras 
# MAGIC  - Modify # of epochs
# MAGIC  - Visualize loss

# COMMAND ----------

# MAGIC %run ./Includes/Classroom-Setup

# COMMAND ----------

# MAGIC %md
# MAGIC Let's start by making a simple array of features, and the label we are trying to predict is y = 2*X + 1.

# COMMAND ----------

import numpy as np
np.set_printoptions(suppress=True)

X = np.arange(-10, 11).reshape((21,1))
y = 2*X + 1

list(zip(X, y))

# COMMAND ----------

import matplotlib.pyplot as plt

plt.plot(X, y, "ro", label="True y")

plt.title("X vs Y")
plt.xlabel("X")
plt.ylabel("y")
plt.legend()

plt.show()

# COMMAND ----------

# MAGIC %md
# MAGIC Use sklearn to establish our baseline (for simplicity, we are using the same dataset for training and testing in this toy example, but we will change that later).

# COMMAND ----------

from sklearn.linear_model import LinearRegression

lr = LinearRegression()

lr.fit(X, y)

# COMMAND ----------

from sklearn.metrics import mean_squared_error

y_pred = lr.predict(X)
mse = mean_squared_error(y, y_pred)
print(mse)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Visualize Predictions

# COMMAND ----------

plt.plot(X, y, "ro", label="True y")
plt.plot(X, y_pred, label="Pred y")

plt.title("X vs True Y and Pred Y (Linear Regression)")
plt.xlabel("X")
plt.ylabel("y")
plt.legend()

plt.show()

# COMMAND ----------

# MAGIC %md-sandbox
# MAGIC ## Keras
# MAGIC 
# MAGIC Now that we have established a baseline model, let's see if we can build a fully-connected neural network that can meet or exceed our linear regression model. A fully-connected neural network is simply a set of matrix multiplications followed by some non-linear function (to be discussed later). 
# MAGIC 
# MAGIC <a href="https://www.tensorflow.org/guide/keras" target="_blank">Keras</a> is a high-level API to build neural networks and was released by Fran√ßois Chollet in 2015. It is now the official high-level API of TensorFlow. 
# MAGIC 
# MAGIC ##### Steps to build a Keras model
# MAGIC <img style="width:20%" src="https://files.training.databricks.com/images/5_cycle.jpg" >

# COMMAND ----------

# MAGIC %md # 1. Define a N-Layer Network
# MAGIC 
# MAGIC Here, we need to specify the dimensions of our input and output layers. When we say something is an N-layer neural network, we count all of the layers except the input layer. 
# MAGIC 
# MAGIC A special case of neural network with no hidden layers and no non-linearities is actually just linear regression :).
# MAGIC 
# MAGIC For the next few labs, we will use the <a href="https://www.tensorflow.org/api_docs/python/tf/keras" target="_blank">Sequential model</a> from Keras.

# COMMAND ----------

import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
tf.random.set_seed(42)

# The Sequential model is a linear stack of layers.
model = Sequential()

model.add(Dense(units=1, input_dim=1, activation="linear"))

# COMMAND ----------

# MAGIC %md
# MAGIC We can check the model definition by calling **`.summary()`**. Note the two parameters - any thoughts on why there are TWO?

# COMMAND ----------

model.summary()

# COMMAND ----------

# MAGIC %md # 2. Compile a Network
# MAGIC 
# MAGIC To <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#compile" target="_blank">compile</a> the network, we need to specify the loss function and which optimizer to use. We'll talk more about optimizers and loss metrics in the next lab.
# MAGIC 
# MAGIC For right now, we will use **`mse`** (mean squared error) for our loss function, and the **`adam`** optimizer.

# COMMAND ----------

model.compile(loss="mse", optimizer="adam")

# COMMAND ----------

# MAGIC %md # 3. Fit a Network
# MAGIC 
# MAGIC Let's <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#fit" target="_blank">fit</a> our model on X and y.

# COMMAND ----------

model.fit(X, y)

# COMMAND ----------

# MAGIC %md
# MAGIC And take a look at the predictions.

# COMMAND ----------

keras_pred = model.predict(X)
keras_pred

# COMMAND ----------

def keras_pred_plot(keras_pred):
    plt.clf()
    plt.plot(X, y, "ro", label="True y")
    plt.plot(X, keras_pred, label="Pred y")

    plt.title("X vs True Y and Pred Y (Keras)")
    plt.xlabel("X")
    plt.ylabel("y")
    plt.legend(numpoints=1)
    plt.show()

keras_pred_plot(keras_pred)

# COMMAND ----------

# MAGIC %md
# MAGIC What went wrong?? Turns out there a few more hyperparameters we need to set. Let's take a look at <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#fit" target="_blank">Keras documentation</a>.
# MAGIC 
# MAGIC **`epochs`** specifies how many passes you want over your entire dataset. Let's increase the number of epochs, and look at how the MSE decreases.
# MAGIC 
# MAGIC Here we are capturing the output of model.fit() as it returns a History object, which keeps a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).

# COMMAND ----------

history = model.fit(X, y, epochs=20) 

# COMMAND ----------

def view_model_loss():
    plt.clf()
    plt.plot(history.history["loss"])
    plt.title("Model Loss")
    plt.ylabel("Loss")
    plt.xlabel("Epoch")
    plt.show()

view_model_loss()

# COMMAND ----------

# MAGIC %md
# MAGIC Let's try increasing the epochs even more.

# COMMAND ----------

history = model.fit(X, y, epochs=4000)

# COMMAND ----------

# MAGIC %md
# MAGIC Let's inspect how our model decreased the loss (MSE) as the number of epochs increases.

# COMMAND ----------

view_model_loss()

# COMMAND ----------

# MAGIC %md
# MAGIC Extract model weights. Wahoo! We were able to approximate y=2*X + 1 quite well! If we trained for some more epochs, we should be able to approximate this function exactly (at risk of overfitting of course).

# COMMAND ----------

print(model.get_weights())
predicted_w = model.get_weights()[0][0][0]
predicted_b = model.get_weights()[1][0]

print(f"predicted_w: {predicted_w}")
print(f"predicted_b: {predicted_b}")

# COMMAND ----------

# MAGIC %md # 4. Evaluate Network
# MAGIC 
# MAGIC As mentioned previously, we want to make sure our neural network can beat our benchmark. 

# COMMAND ----------

model.evaluate(X, y) # Prints loss value & metrics values for the model in test mode (both are MSE here)

# COMMAND ----------

# MAGIC %md # 5. Make Predictions

# COMMAND ----------

keras_pred = model.predict(X)
keras_pred

# COMMAND ----------

keras_pred_plot(keras_pred)

# COMMAND ----------

# MAGIC %md
# MAGIC Alright, this was a very simple, contrived example. Let's go ahead and make this a bit more interesting in the next lab!

# COMMAND ----------

# MAGIC %md-sandbox
# MAGIC &copy; 2022 Databricks, Inc. All rights reserved.<br/>
# MAGIC Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href="https://www.apache.org/">Apache Software Foundation</a>.<br/>
# MAGIC <br/>
# MAGIC <a href="https://databricks.com/privacy-policy">Privacy Policy</a> | <a href="https://databricks.com/terms-of-use">Terms of Use</a> | <a href="https://help.databricks.com/">Support</a>
