{"cells":[{"cell_type":"markdown","source":["#Evaluating Risk for Loan Approvals\n\n## Business Value\n\nBeing able to accurately assess the risk of a loan application can save a lender the cost of holding too many risky assets. Rather than a credit score or credit history which tracks how reliable borrowers are, we will generate a score of how profitable a loan will be compared to other loans in the past. The combination of credit scores, credit history, and profitability score will help increase the bottom line for financial institution.\n\nHaving a interporable model that a loan officer can use before performing a full underwriting can provide immediate estimate and response for the borrower and a informative view for the lender.\n\n<a href=\"https://ibb.co/cuQYr6\"><img src=\"https://preview.ibb.co/jNxPym/Image.png\" alt=\"Image\" border=\"0\"></a>\n\nThis notebook has been tested with *DBR 5.4 ML Beta, Python 3*"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a418b3e-bad0-459c-a0f6-44e1f262291a"}}},{"cell_type":"markdown","source":["## The Data\n\nThe data used is public data from Lending Club. It includes all funded loans from 2012 to 2017. Each loan includes applicant information provided by the applicant as well as the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. For a full view of the data please view the data dictionary available [here](https://resources.lendingclub.com/LCDataDictionary.xlsx).\n\n\n![Loan_Data](https://preview.ibb.co/d3tQ4R/Screen_Shot_2018_02_02_at_11_21_51_PM.png)\n\nhttps://www.kaggle.com/wendykan/lending-club-loan-data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e19c55d4-53e4-48ec-a768-025aadfe0d97"}}},{"cell_type":"markdown","source":["### Databricks MLflow Integration\nUncomment the next cell to showcase Databricks MLflow Integration.  Note, this currently does not work in Databricks Community Edition."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c051152e-bd32-4656-a22b-c8cc85b5392f"}}},{"cell_type":"code","source":["# import mlflow\n# print(mlflow.__version__)\n\n# spark.conf.set(\"spark.databricks.mlflow.trackMLlib.enabled\", \"true\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d43b4b2-cc05-467a-9e2c-b230cb52d186"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Configure location of loanstats_2012_2017.parquet\nlspq_path = \"/databricks-datasets/samples/lending_club/parquet/\"\n\n# Read loanstats_2012_2017.parquet\ndata = spark.read.parquet(lspq_path)\n\n# Reduce the amount of data (to run on DBCE)\n(loan_stats_ce, loan_stats_rest) = data.randomSplit([0.025, 0.975], seed=123)\n\n# Select only the columns needed\nloan_stats_ce = loan_stats_ce.select(\"loan_status\", \"int_rate\", \"revol_util\", \"issue_d\", \"earliest_cr_line\", \"emp_length\", \"verification_status\", \"total_pymnt\", \"loan_amnt\", \"grade\", \"annual_inc\", \"dti\", \"addr_state\", \"term\", \"home_ownership\", \"purpose\", \"application_type\", \"delinq_2yrs\", \"total_acc\")\n\n# Print out number of loans\nprint(str(loan_stats_ce.count()) + \" loans opened by Lending Club...\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Import Data","showTitle":true,"inputWidgets":{},"nuid":"d12db680-d8f4-4543-b812-552f325a1da6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\nprint(\"------------------------------------------------------------------------------------------------\")\nprint(\"Create bad loan label, this will include charged off, defaulted, and late repayments on loans...\")\nloan_stats_ce = loan_stats_ce.filter(loan_stats_ce.loan_status.isin([\"Default\", \"Charged Off\", \"Fully Paid\"]))\\\n                       .withColumn(\"bad_loan\", (~(loan_stats_ce.loan_status == \"Fully Paid\")).cast(\"string\"))\n\n\nprint(\"------------------------------------------------------------------------------------------------\")\nprint(\"Turning string interest rate and revoling util columns into numeric columns...\")\nloan_stats_ce = loan_stats_ce.withColumn('int_rate', regexp_replace('int_rate', '%', '').cast('float')) \\\n                       .withColumn('revol_util', regexp_replace('revol_util', '%', '').cast('float')) \\\n                       .withColumn('issue_year',  substring(loan_stats_ce.issue_d, 5, 4).cast('double') ) \\\n                       .withColumn('earliest_year', substring(loan_stats_ce.earliest_cr_line, 5, 4).cast('double'))\nloan_stats_ce = loan_stats_ce.withColumn('credit_length_in_years', (loan_stats_ce.issue_year - loan_stats_ce.earliest_year))\n\n\nprint(\"------------------------------------------------------------------------------------------------\")\nprint(\"Converting emp_length column into numeric...\")\nloan_stats_ce = loan_stats_ce.withColumn('emp_length', trim(regexp_replace(loan_stats_ce.emp_length, \"([ ]*+[a-zA-Z].*)|(n/a)\", \"\") ))\nloan_stats_ce = loan_stats_ce.withColumn('emp_length', trim(regexp_replace(loan_stats_ce.emp_length, \"< 1\", \"0\") ))\nloan_stats_ce = loan_stats_ce.withColumn('emp_length', trim(regexp_replace(loan_stats_ce.emp_length, \"10\\\\+\", \"10\") ).cast('float'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Filter Data and Fix Schema","showTitle":true,"inputWidgets":{},"nuid":"9c998bfd-c1f3-4080-8414-5de131bd8c80"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Easily Convert Parquet to Delta Lake format\nWith Delta Lake, you can easily transform your Parquet data into Delta Lake format."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd5f772e-9a02-4733-ac64-1d7919b426cb"}}},{"cell_type":"code","source":["# Configure Path\nDELTALAKE_GOLD_PATH = \"/ml/loan_stats.delta\"\n\n# Remove table if it exists\ndbutils.fs.rm(DELTALAKE_GOLD_PATH, recurse=True)\n\n# Save table as Delta Lake\nloan_stats_ce.write.format(\"delta\").mode(\"overwrite\").save(DELTALAKE_GOLD_PATH)\n\n# Re-read as Delta Lake\nloan_stats = spark.read.format(\"delta\").load(DELTALAKE_GOLD_PATH)\n\n# Review data\ndisplay(loan_stats)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"44b89cad-34da-4e29-8129-cfccc707d342"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(loan_stats)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Asset Allocation","showTitle":true,"inputWidgets":{},"nuid":"9a764069-4bc6-4d9d-b0e5-7e81695ca6e9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(\"------------------------------------------------------------------------------------------------\")\nprint(\"Map multiple levels into one factor level for verification_status...\")\nloan_stats = loan_stats.withColumn('verification_status', trim(regexp_replace(loan_stats.verification_status, 'Source Verified', 'Verified')))\n\nprint(\"------------------------------------------------------------------------------------------------\")\nprint(\"Calculate the total amount of money earned or lost per loan...\")\nloan_stats = loan_stats.withColumn('net', round( loan_stats.total_pymnt - loan_stats.loan_amnt, 2))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Munge Data","showTitle":true,"inputWidgets":{},"nuid":"bfa184b0-32f6-4e17-960c-6127dc27d614"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Delta Lake Logo Tiny](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Schema Evolution\nWith the `mergeSchema` option, you can evolve your Delta Lake table schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77eb98b1-62b8-4a33-83f8-3aeaf06f1f05"}}},{"cell_type":"code","source":["# Add the mergeSchema option\nloan_stats.write.option(\"mergeSchema\",\"true\").format(\"delta\").mode(\"overwrite\").save(DELTALAKE_GOLD_PATH)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3be650e9-1749-457d-b9d9-e83f500f0af7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Original Schema\nloan_stats_ce.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8cdcfd29-0045-4e5c-b882-e5491ba4cb0c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# New Schema\nloan_stats.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c1c9e2d-5deb-4705-a2f8-d1e4be6c7135"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Review Delta Lake Table History\nAll the transactions for this table are stored within this table including the initial set of insertions, update, delete, merge, and inserts with schema modification"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ea57e26-ac9f-46d1-a345-7e746ca68b59"}}},{"cell_type":"code","source":["spark.sql(\"DROP TABLE IF EXISTS loan_stats\")\nspark.sql(\"CREATE TABLE loan_stats USING DELTA LOCATION '\" + DELTALAKE_GOLD_PATH + \"'\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"983f9115-8d31-4bc9-9e0a-ff270bb17a6a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nDESCRIBE HISTORY loan_stats"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14b1bd61-644b-48af-be34-5e64ccdb2f9c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(loan_stats)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Feature Distribution and Correlation","showTitle":true,"inputWidgets":{},"nuid":"0794391d-e55e-4110-9dbe-831b701e78f5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(loan_stats.groupBy(\"addr_state\").agg((count(col(\"annual_inc\"))).alias(\"ratio\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Loans Per State","showTitle":true,"inputWidgets":{},"nuid":"71c4be8a-c090-47f3-a896-727165a74fb0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(loan_stats)\n# display(loan_stats.groupBy(\"bad_loan\", \"grade\").agg((sum(col(\"net\"))).alias(\"sum_net\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Asset Allocation","showTitle":true,"inputWidgets":{},"nuid":"0131639b-5ef0-4050-a7d6-9388c23951a1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(loan_stats.select(\"net\",\"verification_status\",\"int_rate\", \"revol_util\", \"issue_year\", \"earliest_year\", \"bad_loan\", \"credit_length_in_years\", \"emp_length\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Display munged columns","showTitle":true,"inputWidgets":{},"nuid":"0a5773e2-7e68-45d5-b19f-547da2b13882"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\nprint(\"------------------------------------------------------------------------------------------------\")\nprint(\"Setting variables to predict bad loans\")\nmyY = \"bad_loan\"\ncategoricals = [\"term\", \"home_ownership\", \"purpose\", \"addr_state\",\n                \"verification_status\",\"application_type\"]\nnumerics = [\"loan_amnt\",\"emp_length\", \"annual_inc\",\"dti\",\n            \"delinq_2yrs\",\"revol_util\",\"total_acc\",\n            \"credit_length_in_years\"]\nmyX = categoricals + numerics\n\nloan_stats2 = loan_stats.select(myX + [myY, \"int_rate\", \"net\", \"issue_year\"])\ntrain = loan_stats2.filter(loan_stats2.issue_year <= 2015).cache()\nvalid = loan_stats2.filter(loan_stats2.issue_year > 2015).cache()\n\n# train.count()\n# valid.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Set Response and Predictor Variables","showTitle":true,"inputWidgets":{},"nuid":"e2029ede-134c-4c1c-8a23-9b88122089a8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nUSE default;\nDROP TABLE IF EXISTS loanstats_train;\nDROP TABLE IF EXISTS loanstats_valid;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f251cbd-dbc2-424c-a9bf-3dba4930ac0b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Logistic Regression Notes\n* We will be using the Apache Spark pre-installed GLM and GBTClassifier models in this noteboook\n* **GLM** is in reference to *generalized linear models*; the Apache Spark *logistic regression* model is a special case of a [generalized linear model](https://spark.apache.org/docs/2.2.0/ml-classification-regression.html#logistic-regression)\n* We will also use BinaryClassificationEvaluator, CrossValidator, and ParamGridBuilder to tune our models.\n* References to max F1 threshold (i.e. F_1 score or F-score or F-measure) is the measure of our logistic regression model's accuracy; more information can be found at [F1 score](https://en.wikipedia.org/wiki/F1_score).\n* **GBTClassifier** is in reference to *gradient boosted tree classifier* which is a popular classification and regression method using ensembles of decision trees; more information can be found at [Gradiant Boosted Tree Classifier](https://spark.apache.org/docs/2.2.0/ml-classification-regression.html#gradient-boosted-tree-classifier)\n* In a subsequent notebook, we will be using the XGBoost, an optimized distributed gradient boosting library.  \n  * Underneath the covers, we will be using *XGBoost4J-Spark* - a project aiming to seamlessly integrate XGBoost and Apache Spark by fitting XGBoost to Apache Sparkâ€™s MLLIB framework.  More inforamtion can be found at [XGBoost4J-Spark Tutorial](https://xgboost.readthedocs.io/en/latest/jvm/xgboost4j_spark_tutorial.html)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fbf95a63-935d-45ac-935e-381ae211344b"}}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\nfrom pyspark.ml.feature import StandardScaler, Imputer\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\n## Current possible ways to handle categoricals in string indexer is 'error', 'keep', and 'skip'\nindexers = map(lambda c: StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid = 'keep'), categoricals)\nohes = map(lambda c: OneHotEncoder(inputCol=c + \"_idx\", outputCol=c+\"_class\"),categoricals)\nimputers = Imputer(inputCols = numerics, outputCols = numerics)\n\n# Establish features columns\nfeatureCols = list(map(lambda c: c+\"_class\", categoricals)) + numerics\n\n# Build the stage for the ML pipeline\n# Build the stage for the ML pipeline\nmodel_matrix_stages = list(indexers) + list(ohes) + [imputers] + \\\n                     [VectorAssembler(inputCols=featureCols, outputCol=\"features\"), StringIndexer(inputCol=\"bad_loan\", outputCol=\"label\")]\n\n# Apply StandardScaler to create scaledFeatures\nscaler = StandardScaler(inputCol=\"features\",\n                        outputCol=\"scaledFeatures\",\n                        withStd=True,\n                        withMean=True)\n\n# Use logistic regression \nlr = LogisticRegression(maxIter=10, elasticNetParam=0.5, featuresCol = \"scaledFeatures\")\n\n# Build our ML pipeline\npipeline = Pipeline(stages=model_matrix_stages+[scaler]+[lr])\n\n# Build the parameter grid for model tuning\nparamGrid = ParamGridBuilder() \\\n              .addGrid(lr.regParam, [0.1, 0.01]) \\\n              .build()\n\n# Execute CrossValidator for model tuning\ncrossval = CrossValidator(estimator=pipeline,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=BinaryClassificationEvaluator(),\n                          numFolds=5)\n\n# Train the tuned model and establish our best model\ncvModel = crossval.fit(train)\nglm_model = cvModel.bestModel\n\n# Return ROC\nlr_summary = glm_model.stages[len(glm_model.stages)-1].summary\ndisplay(lr_summary.roc)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Build Grid of GLM Models w/ Standardization+CrossValidation","showTitle":true,"inputWidgets":{},"nuid":"4494a1f2-9a36-4e51-ba40-21a5172bf5ac"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["fMeasure = lr_summary.fMeasureByThreshold\nmaxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\nmaxFMeasure = maxFMeasure['max(F-Measure)']\nfMeasure = fMeasure.toPandas()\nbestThreshold = float ( fMeasure[ fMeasure['F-Measure'] == maxFMeasure] [\"threshold\"])\nlr.setThreshold(bestThreshold)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Set Max F1 Threshold","showTitle":true,"inputWidgets":{},"nuid":"cb3d1059-9cb3-4304-ae5a-fa3c2cd0c04f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.classification import GBTClassifier\n\n# Establish stages for our GBT model\nindexers = map(lambda c: StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid = 'keep'), categoricals)\nimputers = Imputer(inputCols = numerics, outputCols = numerics)\nfeatureCols = list(map(lambda c: c+\"_idx\", categoricals)) + numerics\n\n# Define vector assemblers\nmodel_matrix_stages = list(indexers) + [imputers] + \\\n                     [VectorAssembler(inputCols=featureCols, outputCol=\"features\"), StringIndexer(inputCol=\"bad_loan\", outputCol=\"label\")]\n\n# Define a GBT model.\ngbt = GBTClassifier(featuresCol=\"features\",\n                    labelCol=\"label\",\n                    lossType = \"logistic\",\n                    maxBins = 52,\n                    maxIter=20,\n                    maxDepth=5)\n\n# Chain indexer and GBT in a Pipeline\npipeline = Pipeline(stages=model_matrix_stages+[gbt])\n\n# Train model.  This also runs the indexer.\ngbt_model = pipeline.fit(train)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Build GBT Model","showTitle":true,"inputWidgets":{},"nuid":"cd046205-8cc4-4b9a-b714-463e87d46c35"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.mllib.evaluation import BinaryClassificationMetrics\nfrom pyspark.ml.linalg import Vectors\n\ndef extract(row):\n  return (row.net,) + tuple(row.probability.toArray().tolist()) +  (row.label,) + (row.prediction,)\n\ndef score(model,data):\n  pred = model.transform(data).select(\"net\", \"probability\", \"label\", \"prediction\")\n  pred = pred.rdd.map(extract).toDF([\"net\", \"p0\", \"p1\", \"label\", \"prediction\"])\n  return pred \n\ndef auc(pred):\n  metric = BinaryClassificationMetrics(pred.select(\"p1\", \"label\").rdd)\n  return metric.areaUnderROC\n\nglm_train = score(glm_model, train)\nglm_valid = score(glm_model, valid)\ngbt_train = score(gbt_model, train)\ngbt_valid = score(gbt_model, valid)\n\nglm_train.createOrReplaceTempView(\"glm_train\")\nglm_valid.createOrReplaceTempView(\"glm_valid\")\ngbt_train.createOrReplaceTempView(\"gbt_train\")\ngbt_valid.createOrReplaceTempView(\"gbt_valid\")\n\n\nprint (\"GLM Training AUC:\" + str(auc(glm_train)))\nprint (\"GLM Validation AUC :\" + str(auc(glm_valid)))\nprint (\"GBT Training AUC :\" + str(auc(gbt_train)))\nprint (\"GBT Validation AUC :\" + str(auc(gbt_valid)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Grab Model Metrics","showTitle":true,"inputWidgets":{},"nuid":"2971713b-b81f-4f14-9a37-2001dac7a85b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n// import org.apache.spark.sql.functions.typedLit\nimport org.apache.spark.sql.functions.{array, lit, map, struct}\n\ndef roc(pred:org.apache.spark.sql.DataFrame, model_id:String): org.apache.spark.sql.DataFrame = {\n  var testScoreAndLabel = pred.select(\"p1\", \"label\").map{ case Row(p:Double,l:Double) => (p,l)}\n  val metrics = new BinaryClassificationMetrics(testScoreAndLabel.rdd, 100)\n  val roc = metrics.roc().toDF().withColumn(\"model\", lit(model_id))\n  return roc\n}\n\nval glm_train = roc( spark.table(\"glm_train\"), \"glm_train\")\nval glm_valid = roc( spark.table(\"glm_valid\"), \"glm_valid\")\nval gbt_train = roc( spark.table(\"gbt_train\"), \"gbt_train\")\nval gbt_valid = roc( spark.table(\"gbt_valid\"), \"gbt_valid\")\n\nval roc_curves = glm_train.union(glm_valid).union(gbt_train).union(gbt_valid)\n\ndisplay(roc_curves)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Stacked ROC Curves","showTitle":true,"inputWidgets":{},"nuid":"6c29cef8-d50c-4857-a6f5-04148c2867a7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["gbt_valid_table = spark.table(\"gbt_valid\")\ngbt_valid_table.createOrReplaceTempView(\"gbt_valid_table\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6657459a-557c-48f4-a5a7-2c0b506d922d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nselect * from gbt_valid_table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e70ebe9-37c7-40c7-8da2-eeb933540614"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Quantify the Business Value\n\nA great way to quickly understand the business value of this model is to create a confusion matrix.  The definition of our matrix is as follows:\n\n* Prediction=1, Label=1 (Blue) : Correctly found bad loans. sum_net = loss avoided.\n* Prediction=1, Label=0 (Orange) : Incorrectly labeled bad loans. sum_net = profit forfeited.\n* Prediction=0, Label=1 (Green) : Incorrectly labeled good loans. sum_net = loss still incurred.\n* Prediction=0, Label=0 (Red) : Correctly found good loans. sum_net = profit retained.\n\nThe following code snippet calculates the following confusion matrix."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"852e2d5a-147c-469e-9b55-3f596682c9b9"}}},{"cell_type":"code","source":["display(glm_valid.groupBy(\"label\", \"prediction\").agg((sum(col(\"net\"))).alias(\"sum_net\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Business Value","showTitle":true,"inputWidgets":{},"nuid":"bb6dfb9e-19ce-4b3f-a8c2-fac339928e1b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Using the MLflow Runs Sidebar\n\nBecause of the code snippet added in cell 5, you can view your MLflow runs using the [MLflow Runs Sidebar](https://databricks.com/blog/2019/04/30/introducing-mlflow-run-sidebar-in-databricks-notebooks.html).  *Note, this feature is currently not available in Databricks Community Edition.*\n\n![](https://pages.databricks.com/rs/094-YMS-629/images/db-mlflow-integration.gif)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8692a242-1236-404b-995a-b0b0655236e2"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Delta Lake Workshop - Including ML","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1886517325837256}},"nbformat":4,"nbformat_minor":0}
