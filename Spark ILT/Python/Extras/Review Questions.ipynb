{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f4e0ace-c5d4-411d-8dec-495fc6b0e5c9"}}},{"cell_type":"markdown","source":["# Review Questions\nThe following questions are intended to be reviewed the following day and are simply suggestions based on the material covered in the corresponding notebooks."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0c7f4f3-fb69-4f25-8760-377f7bc96545"}}},{"cell_type":"markdown","source":["## Spark Programming\n* What is Apache Spark?\n  * A database\n  * A processing engine\n  * A design pattern for processing big data\n  * A no-sql database with a computation engine\n  * A multi-threaded, in-memory, storage engine\n* Who is Databricks?\n  * The inventors of Apache Spark\n  * The #1 contributor to Apache Spark\n  * Sponsors of Spark+AI Summit\n  * Host to Spark Meetups world wide\n  * Providers of the fastest, hosted version of Apache Spark\n  * All of the above\n* What environment can Spark run on?\n  * OpenStack\n  * Yarn\n  * Mesos\n  * EC2\n  * Docker\n  * Kubernetes\n  * Databricks\n  * All of the above\n  * All of the above and more\n* Which environment does Spark run best on?\n  * OpenStack\n  * Yarn\n  * Mesos\n  * EC2\n  * Docker\n  * Kubernetes\n  * Databricks\n* What is the relationship between a shard, workspace and a cluster?\n  * open-ended\n* Name five cluster managers:\n  * open-ended (Spark Standalone, Mesos, Yarn, Kubernetes & Databricks)\n* Why did we have to create a new cluster (next day)?\n  * Clusters are short lived processes\n  * The cluster auto-terminated due to inactivity\n  * Someone shut down the cluster for you\n  * The cluster crashed\n* What is the relationship between a core, thread & slot\n  * They are all the same thing\n  * There are N threads per core, subdivided into slots\n  * Cores refer to the CPU cores, one thread are multi-plexed to the CPU cores and slots are a Spark conceptualization of a thread.\n  * Slots are the space on the motherboard in which the cores is installed with multiple threads per core\n* What is the relationship between a job & a stage\n  * A job is a request to do some work in the executor, stage refers to the staging area in which that job executes\n  * A job is typically triggered by an action and further subdivided in to stages/units of work\n  * Stages refer to the sequence of work to be performed in parallel, each sequence consisting of multiple jobs\n  * None of the above\n* RDDs are the ? of the DataFrames API\n  * assembly language\n  * deprecated predecessors\n  * successors\n  * none of the above"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3765666a-d84d-4d96-82a9-465f4d16b9a3"}}},{"cell_type":"markdown","source":["## The Databricks Environment\n* What is the DBFS?\n  * Stands for DistriButed File System\n  * A virtual drive over an object store such as Azure Blog or Amazon S3\n  * A virtualized hard drive backed by a NAS device\n  * An ephemeral storage device\n* What is the name of the utility class for interacting with the DBFS?\n  * db_utils\n  * utils\n  * dbutils\n  * dbfs\n  * dbfs_utils\n  * databricks\n* What does the `display()` command do?\n  * A utility method for graphically rendering ML models\n  * A Databricks specific function for presenting information in a notebook\n  * A utility method for displaying Numpy visualizations\n  * A utility method for presenting up to 1000 records of a dataframe\n  * All of the above\n* Name some of the magic commands covered on day one.\n  * open-ended (%run, %fs, %sh, %r, %python, %scala, %sql, %md, %md-sandbox"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d6a6f1ae-30df-4cb0-add1-544969ca4343"}}},{"cell_type":"markdown","source":["## Reading Data\n* What does it mean to partition data?\n  * To merge two chunks of data into one\n  * To divide a chunk of data in half\n  * To divide a single dataset into smaller manageable chunks\n  * To combine many small chunks of data into one large manageable chunk\n* How do the `DataFrameReader`s decide how to partition data?\n  * Always specified by the optional parameter `partitions`\n  * Implementation is left to the sole discretion of the reader\n  * Controlled by the config parameter `spark.sql.shuffle.partitions`\n  * Controlled by the config parameter `spark.sql.read.partitions`\n  * None of the above\n* Which technique for reading data provided control over the number of partitions being created & what was so special about it?\n  * CSV files\n  * Parquet files\n  * JDBC connections\n  * ORC files\n  * TCP/IP Connections \n* What would be the ramifications of creating 1,000, 10,000 or even 100,000 partitions from a JDBC data source?\n  * Achieve extremely high parallelization\n  * Saturate the network connection between Spark and the SQL server\n  * Exceed the SQL server's maximum number of connections\n  * Crash the SQL server\n  * All of the above\n  * None of the above\n* What is the side effect of inferring the schema when reading in CSV or JSON data?\n  * Performance is increased for every subsequent query\n  * Performance is increased for the initialization of the DataFrame only\n  * Performance is decreased on every subsequent query\n  * Performance is decreased only when the DataFrame is initialized\n* Why is the Parquet file format better than CSV, JSON, ORC, JDBC?\n  * It is splittable allowing for parallel reads\n  * It supports multiple compression algorithms\n  * Columns of data can be read without reading the entire row of data\n  * The schema is stored as part of the file allowing for fast schema reads\n  * The data can be partitioned on disk allowing for selective reads\n  * It's harder to spell\n  * All of the above"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"87fd7a0c-2e6f-4bc1-9c00-62657959ed62"}}},{"cell_type":"markdown","source":["## Introduction to DataFrames, Part #1\n* In Spark 2.x, what is the name of the class for the \"entry point\" for the DataFrames API.\n  * SparkContext\n  * SparkSession\n  * SQLContext\n  * DataFrame\n  * Dataset\n* In the Scala API, what class do I need to search for in order to load the DataFrames API docs?\n  * Dataframe\n  * DataFrame\n  * DataFrameReader\n  * Dataset\n  * DataSet\n  * DataSetReader\n* What is the difference between a `DataFrame` and a `Dataset[Row]`?\n  * DataFrames are in Python only and Datasets are in Scala only\n  * Dataset[Row] is a subclass of DataFrame specifically for row-based data\n  * DataFrames work with most data sources where Dataset[Row] is specifically for relational databases\n  * None of the above\n* What DataFame operations did we cover so far?\n  * open-ended (limit, select, drop, distinct, dropDuplicates, show, display, count)\n* DataFrame operations generally fall into one of two categories. \n  * What category do operations like `limit()`, `select()`, `drop()` and `distinct()` fall into?\n    * Transformations\n    * Readers\n    * Utilities\n    * Actions\n    \n  * What category do operations like `show()`, `display()` and `count()` fall into?\n    * Transformations\n    * Readers\n    * Utilities\n    * Actions\n\n* Which type of operation **always** triggers a job?\n  * Transformations\n  * Readers\n  * Utilities\n  * Actions\n\n* In what case **might** a transformation trigger a job?\n  * open-ended: When readers need to touch the data so as to determine the schema\n* What is the difference between `display()` and `show()`?\n  * display is pretty\n  * show is ugly\n  * display is Databricks specific function\n  * show is part of core spark\n  * show is an action\n  * display calls an action\n* What is the difference between `distinct()` and `dropDuplicates()`?\n  * There is no difference\n  * distinct() only works on DataFrames & dropDuplicates() only works on Datasets\n  * dropDuplicates(..) accepts additional parameters\n  * All of the above"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"deb6899a-901d-4999-a762-18fb8e12db56"}}},{"cell_type":"markdown","source":["## Transformations & Actions\n* What set of operations are considered \"lazy\"?\n  * Utility methods\n  * Actions\n  * Transformations\n  * Readers\n  \n* What set of operations are considered \"eager\"?\n  * Utility methods\n  * Actions\n  * Transformations\n  * Readers\n  \n* Why do transformations not trigger a job (aside from some of the initial read operations)?\n  * Because the transformation is executed on the driver\n  * So that the transformations can be optimized\n  * Because the spark config `spark.sql.transformations.eager` is set to false\n  * Because the server crashed\n* What benefits does lazy execution afford us?\n  * All the data is not loaded before processing it\n  * They can be optimized before the data is processed\n  * It makes parallelization easier\n  * It allows the reader to decide how much data to pull from disk\n  * None of the above\n  * All of the above\n* What is the name of the entity responsible for optimizing our DataFrames and SQL and converting them into RDDs?\n  * The Tungsten Optimizer\n  * The Catalyst Optimizer\n  * The Cost Based Optimizer\n  * RDDs\n* What is the difference between a wide and narrow transformation?\n  * Wide operations process more data than narrow transformations\n  * Narrow transformations work on partitions less than 200 MB\n  * Narrow transformations are pipelined together and wide operations create a stage boundary\n  * Wide transformations are pipelined together and narrow operations create a stage boundary\n  * Narrow transformations reduce the number of partitions in a dataset and wide transformations increase them\n* What triggers a shuffle operation?\n  * Any action\n  * Any transformation\n  * Wide transformations\n  * Narrow transformations\n  * Wide Actions\n  * Narrow Actions\n* When are stages created?\n  * After a job is completed but before control is returned to the driver\n  * Between any wide transformation\n  * Between any narrow transformaion\n  * When the partition size is > 200 MB\n* Can we begin executing tasks in Stage #2 if other tasks on other executors are still running in Stage #1?\n  * Yes\n  * No\n  * Yes, but only if all transformations in the previous stage were narrow\n  * Yes, but only if all transformations in the previous stage were wide\n* Why must we wait for all operations in a given stage to complete before moving on to the next stage?\n  * open-ended\n* What is the term used for the scenario when data is read into RAM and several transformations are sequentially executed against that data?\n  * Shuffling\n  * Tuning\n  * Optimizing\n  * Pipelining\n  * None of the above\n* How is Pipelining in Apache Spark different when processing transformations compared to MapReduce?\n  * Spark pipelines results in faster processing than MapReduce\n  * MapReduce doesn't pipeline multiple transformations together\n  * MapReduce writes each result back to disk\n  * All of the above\n  * None of the above\n* Describe how Apache Spark can determine that certain stages can be skipped when re-executing a query?\n  * Because the DataFrame was specifically cached\n  * Because the transformation results in a net-zero change\n  * Because the shuffle operation let temp files on the executor that can be reused\n  * Because the lineage of transformations is immutable\n* Where in the Spark UI can I see how long it took to execute a task?\n  * In the Job Details\n  * In the Stage Details\n  * In the Task Details\n  * In the Storage Tab\n  * In the Executor Tab\n* Where in the Spark UI can I see if my data is evenly distributed among each partition?\n  * In the Job Details\n  * In the Stage Details\n  * In the Task Details\n  * In the Storage Tab\n  * In the Executor Tab"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5febbabb-4a64-4078-9265-53bf8933a27e"}}},{"cell_type":"markdown","source":["## Introduction to DataFrames, Part #2\n* What are some of the transformations we reviewed yesterday?\n* What are some of the actions we reviewed yesterday?\n* What two new classes did we discuss yesterday?\n  * Hint: The first class is used in methods like `filter()` and `where()`\n  * Hint: The second method is the object backing every row of a `DataFrame`.\n* What potential problem might I run into by calling `collect()`?\n* What type of object does `collect()` and `take(n)` return?\n* What type of object does `head()` and `first()` return?\n* What are some of the operations we can perform on a `Column` object?\n* With the Scala API, how do you access the 3rd column of a `DataFrame`'s `Row` object which happens to be a `Boolean` value?\n* What happens if you do not use the correct accessor method on a `Row` object?\n  * Hint: What happens if you ask for a `Boolean` when the backing object is a `String`?\n* Why don't we have these problems with the Scala API?\n* What is different about the transformations `orderBy(String)` and `filter(String)`?\n* What methods do you call on a `Column` object to test for equality, in-equality and null-safe equality?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07b28e1b-ee79-44a3-87d7-c3c46a0ae784"}}},{"cell_type":"markdown","source":["## Partitioning\n* On a non-autoscaling cluster, what is the magic ration between slots and partitions?\n* If you have 125 partitions and 200 cores, what type of problems might you run into?\n* What about 101 partitions and 100 cores?\n* How do I find out how many cores my cluster has?\n* What is the recommended size in MB for each partition (in RAM, i.e. cached)?\n* If I need to increase the number of partitions to match my cores, should I use `repartition(n)` or `coalesce(n)`?\n* What side affect might I run into by using `coalesce(n)`?\n* Which of the two operations is a wide transformation, `repartition(n)` or `coalesce(n)`?\n* What is the significance of the configuration setting `spark.sql.shuffle.partitions`?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dca0aadf-aaa8-428c-9cdf-07ec106e7866"}}},{"cell_type":"markdown","source":["## Introduction to DataFrames, Part #3\n* What is the name of the class returned by the `groupBy()` transformation?\n  * For Python: `GroupedData`\n  * For Scala: `RelationalGroupedDataset`\n* What are some of the aggregate functions available for a `groupBy()` operation?\n* With the various aggregate functions for `groubBy()`, what happens if you do not specify the column to `sum(..)`, for example?\n* How many different ways can you think of to rename/add a column?\n  * `withColumn()`\n  * `select()`\n  * `as()`\n  * `alias()`\n  * `withColumnRenamed()`\n* What are some of the `...sql.functions` methods we saw yesterday?\n  * `unix_timestamp()`\n  * `cast()`\n  * `year()`\n  * `month()`\n  * `dayofyear()`\n  * `sum()`\n  * `count()`\n  * `avg()`\n  * `min()`\n  * `max()`\n* In Python, can I use the `as()` to rename a column?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5df296b7-c98e-487e-93df-05aee7c56a53"}}},{"cell_type":"markdown","source":["## Caching\n* Should I cache data?\n* Why might it be better to convert a CSV file to Parquet instead of caching it?\n* What is the default storage level for the `DataFrame` API?\n* What is the default storage level for the `RDD` API?\n* When would I use the storage level `DISK_ONLY`?\n* When would I use any of the `XXXX_2` storage levels?\n* When would I use any of the `XXXX_SER` storage levels?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30cb1c6d-b88c-470a-ad35-cc99293ce09e"}}},{"cell_type":"markdown","source":["## Introduction to DataFrames, Part #4\n* What is a UDF?\n* What are some the gotchas with UDFs?\n  * Cannot be optimized\n  * Have to be careful about serialization\n  * Slower than built-in functions\n  * There is almost always a built in function that already does what you want.\n* How do you avoid problems with UDFs?\n* How does the performance with UDFs compare with Python vs Scala?\n* How is a join operation in the DataFrames API different than in MySQL or Oracle?\n* Is the join operation wide or narrow?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"620170ed-87c8-4eb6-9e9c-ed31380ece5f"}}},{"cell_type":"markdown","source":["## Broadcasting\n* What is a broadcast join?\n* What is the threshold for automatically triggering a broadcast join (at least on Databricks)\n* How do we change the threshold?\n* spark.sql.autoBroadcastJoinThreshold"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8664a106-3504-4700-a2fa-472c952afa34"}}},{"cell_type":"markdown","source":["## Catalyst Optimizer\n* Does it matter which language I use?\n* Does it matter which API I use? SQL / DataFrame / Dataset?\n* What is the difference between the logical and physical plans?\n* Why does the Catalyst Optimizer produce multiple physical models?\n* What is the last step/stage for the Catalyst Optimizer?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2625a0ed-b4e9-45fa-8630-125cba9a6758"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53e47faa-5047-4b42-84d4-e8f49808eddc"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Review Questions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3794944577839766}},"nbformat":4,"nbformat_minor":0}
