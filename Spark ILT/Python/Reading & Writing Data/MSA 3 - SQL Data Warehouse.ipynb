{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc90c324-8950-4f97-a779-03b256357cd9"}}},{"cell_type":"markdown","source":["# Reading and Writing to Azure SQL Data Warehouse\n**Technical Accomplishments:**\n- Access an Azure SQL Data Warehouse using the SQL Data Warehouse connector\n\n**Requirements:**\n- A database master key for the Azure SQL Data Warehouse\n\nYou will create a data warehouse and database master key for it in the steps below."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4aacbe7-9ddc-47f7-b32c-49d21c5cf30f"}}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup<br>\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the start of each lesson (see the next cell) and the **`Classroom-Cleanup`** cell at the end of each lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e91471ca-d8cc-4aa7-95c9-8ef00c3a1faa"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1b4c5ba-530a-416c-9fc4-0f2c2556f050"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Azure SQL Data Warehouse\nAzure SQL Data Warehouse leverages massively parallel processing (MPP) to quickly run complex queries across petabytes of data.\n\nImport big data into SQL Data Warehouse with simple PolyBase T-SQL queries, and then use MPP to run high-performance analytics.\n\nAs you integrate and analyze, the data warehouse will become the single version of truth your business can count on for insights."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a02b9da8-4e71-4da8-bd53-ec86e950bb5c"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) SQL Data Warehouse Connector\n- Use Azure Blob Storage as an intermediary between Azure Databricks and SQL Data Warehouse\n- In Azure Databricks: triggers Spark jobs to read and write data to Blob Storage\n- In SQL Data Warehouse: triggers data loading and unloading operations, performed by **PolyBase**\n\n**Note:** The SQL DW connector is more suited to ETL than to interactive queries.  \nFor interactive and ad-hoc queries, data should be extracted into a Databricks Delta table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba775691-4bf9-46ad-91e9-357e36b17d44"}}},{"cell_type":"markdown","source":["![](https://files.training.databricks.com/images/adbcore/AAHRBWKzrNVMUpfjecWUpfRb9p8pVZl7fsMB.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bad7dfdb-50b8-4649-920f-4c79763e9a5b"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Types of Connections in SQL Data Warehouse\n\n### **Spark Driver to SQL DW**\nSpark driver connects to SQL Data Warehouse via JDBC using a username and password.\n\n### **Spark Driver and Executors to Azure Blob Storage**\nSpark uses the **Azure Blob Storage connector** bundled in Databricks Runtime to connect to the Blob Storage container.\n  - Requires **`wasbs`** URI scheme to specify connection\n  - Requires **storage account access key** to set up connection\n    - Set in a notebook's session configuration, which doesn't affect other notebooks attached to the same cluster\n    - **`spark`** is the SparkSession object provided in the notebook\n\n### **SQL DW to Azure Blob Storage**\nSQL DW connector forwards the access key from notebook session configuration to SQL Data Warehouse instance over JDBC.\n  - Requires **`forwardSparkAzureStorageCredentials`** set to **`true`**\n  - Represents access key with a temporary <a href=\"https://docs.microsoft.com/en-us/sql/t-sql/statements/create-database-scoped-credential-transact-sql?view=sql-server-2017\" target=\"_blank\">database scoped credential</a> in the SQL Data Warehouse instance\n  - Creates a database scoped credential before asking SQL DW to load or unload data, and deletes after loading/unloading is finished"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85f4d29e-47f5-4c73-b73d-40f034fb9ca3"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Configuration\n\n### Create Azure Blob Storage\nFollow these steps to <a href=\"https://docs.microsoft.com/en-us/azure/storage/common/storage-quickstart-create-account?tabs=azure-portal#regenerate-storage-access-keys\" target=\"_blank\">create an Azure Storage Account</a> and Container.  \nThe SQL DW connector will use a <a href=\"https://docs.microsoft.com/en-us/rest/api/storageservices/authorize-with-shared-key\" target=\"_blank\">Shared Key</a> for authorization.\n\nAs you work through the following steps, record the **Storage Account Name**, **Container Name**, and **Access Key** in the cell below:\n0. Access the Azure Portal > Create a new resource > Storage account\n0. Specify the correct *Resource Group* and *Region*, and use any unique string for the **Storage Account Name**\n0. Access the new Storage account > Access Blobs\n0. Create a New Container using any unique string for the **Container Name**\n0. Retrieve the primary **Access Key** for the new Storage Account"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"441a629a-e269-4d70-88b0-b002edbf52af"}}},{"cell_type":"code","source":["# TODO\nstorageAccount = \"\"\ncontainerName = \"\"\naccessKey = \"\"\n\nspark.conf.set(\"fs.azure.account.key.{}.blob.core.windows.net\".format(storageAccount), accessKey)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86b96167-5608-4395-aa17-2872c4437720"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Configuration\n\n### Create Azure SQL Data Warehouse\nFollow these steps to <a href=\"https://docs.microsoft.com/en-us/azure/sql-data-warehouse/create-data-warehouse-portal\" target=\"_blank\">create an Azure SQL Data Warehouse</a>.\n\n0. Access the Azure Portal > Create a new resource > SQL Data Warehouse\n0. Specify the following attributes for the SQL Data Warehouse:\n   - Use any string for the **Data warehouse name**\n   - Select an existing or create a new SQL Server\n   - Under the **Additional Settings** tab, select **Sample** for the **Use existing data** option\n0. Access the new SQL Data Warehouse\n0. Select **Query Editor (preview)** under **Common Tasks** in the sidebar and enter the proper credentials\n0. Run these two queries:\n   - Create a Master Key in the SQL DW. This facilitates the SQL DW connection\n   \n     **`CREATE MASTER KEY ENCRYPTION BY PASSWORD = 'CORRECT-horse-battery-staple';`**\n\n   - Use a CTAS to create a staging table for the Customer Table. This query will create an empty table with the same schema as the Customer Table.\n   \n     **`CREATE TABLE dbo.DimCustomerStaging`**  \n     **`WITH`**  \n     **`( DISTRIBUTION = ROUND_ROBIN, CLUSTERED COLUMNSTORE INDEX )`**  \n     **`AS`**  \n     **`SELECT  *`**  \n     **`FROM dbo.DimCustomer`**  \n     **`WHERE 1 = 2`**  \n     **`;`**\n\n0. Access Connection Strings\n0. Select JDBC and copy the **JDBC URI**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d61d363-5c4a-4379-8253-3ccbead3ad59"}}},{"cell_type":"code","source":["# TODO\ntableName = \"dbo.DimCustomer\"\njdbcURI = \"\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e1f37c4-5d0e-416f-b38c-666619d106b2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Read from the Customer Table\n\nUse the SQL DW Connector to read data from the Customer Table.\n\nUse the read to define a temporary table that can be queried.\n\nNote the following options in the DataFrameReader in the cell below:\n* **`url`** specifies the JDBC connection to the SQL Data Warehouse\n* **`tempDir`** specifies the **`wasbs`** URI of the caching directory on the Azure Blob Storage container\n* **`forwardSparkAzureStorageCredentials`** is set to **`true`** to ensure that the Azure storage account access keys are forwarded from the notebook's session configuration to the SQL Data Warehouse"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d0d39f7b-f0a2-4100-b453-38949a5fc47b"}}},{"cell_type":"code","source":["cacheDir = \"wasbs://{}@{}.blob.core.windows.net/cacheDir\".format(containerName, storageAccount)\n\ncustomerDF = (spark.read\n  .format(\"com.databricks.spark.sqldw\")\n  .option(\"url\", jdbcURI)\n  .option(\"tempDir\", cacheDir)\n  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n  .option(\"dbTable\", tableName)\n  .load())\n\ncustomerDF.createOrReplaceTempView(\"customer_data\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c05efb96-0d10-4ecc-b84b-08b2affb49f5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Use SQL queries to count the number of rows in the Customer table and to display table metadata."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b9a394a-a92b-439b-b6e5-d7452c0b71d6"}}},{"cell_type":"markdown","source":["Note that **`CustomerKey`** and **`CustomerAlternateKey`** use a very similar naming convention."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"654015bf-6456-483b-b8a0-14186ac05a36"}}},{"cell_type":"markdown","source":["When merging many new customers into this table, we may have issues with uniqueness in the **`CustomerKey`**. \n\nLet's redefine **`CustomerAlternateKey`** for stronger uniqueness using a <a href=\"https://en.wikipedia.org/wiki/Universally_unique_identifier\" target=\"_blank\">UUID</a>. To do this, we will define a UDF and use it to transform the **`CustomerAlternateKey`** column."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54d5ee90-29fc-4d1e-af5e-40a39dc252b5"}}},{"cell_type":"code","source":["from pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nimport uuid\n\nuuidUdf = udf(lambda : str(uuid.uuid4()), StringType())\ncustomerUpdatedDF = customerDF.withColumn(\"CustomerAlternateKey\", uuidUdf())\ndisplay(customerUpdatedDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a27eac56-b59b-4544-80c1-643cbfd21111"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Use the Polybase Connector to Write to the Staging Table\n\nUse the SQL DW Connector to write the updated customer table to a staging table.\n\nIt is best practice to update the SQL Data Warehouse via a staging table.\n\nNote the following options in the DataFrameWriter in the cell below:\n* **`url`** specifies the JDBC connection to the SQL Data Warehouse\n* **`tempDir`** specifies the **`wasbs`** URI of the caching directory on the Azure Blob Storage container\n* **`forwardSparkAzureStorageCredentials`** is set to **`true`** to ensure that the Azure storage account access keys are forwarded from the notebook's session configuration to the SQL Data Warehouse\n\nThese options are the same as those in the DataFrameReader above."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"39b939b7-8930-4bcc-b8f9-37b539dc3921"}}},{"cell_type":"code","source":["(customerUpdatedDF.write\n  .format(\"com.databricks.spark.sqldw\")\n  .mode(\"overwrite\")\n  .option(\"url\", jdbcURI)\n  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n  .option(\"dbtable\", tableName + \"Staging\")\n  .option(\"tempdir\", cacheDir)\n  .save())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c57d6837-36bd-470b-93e7-54b4e7ccd3c8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Read From the New Staging Table\nUse the SQL DW Connector to read the new table we just wrote.\n\nUse the read to define a temporary table that can be queried."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7659d8f4-e042-41b3-8829-d90a1526d0a7"}}},{"cell_type":"code","source":["customerTempDF = (spark.read\n  .format(\"com.databricks.spark.sqldw\")\n  .option(\"url\", jdbcURI)\n  .option(\"tempDir\", cacheDir)\n  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n  .option(\"dbTable\", tableName + \"Staging\")\n  .load())\n\ncustomerTempDF.createOrReplaceTempView(\"customer_temp_data\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3dc0924f-290c-4ee7-9ed1-dd0b9bcc96e5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"053f1893-3a4e-4831-93f8-2f059a56d033"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Cleanup\"\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ba734e6-5ee9-43c2-a04a-09e4b70cd2ea"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"23176dea-216d-44e9-ad16-915a2805c8d9"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"MSA 3 - SQL Data Warehouse","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3794944577840670}},"nbformat":4,"nbformat_minor":0}
