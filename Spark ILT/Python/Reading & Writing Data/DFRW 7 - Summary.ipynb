{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f34e6925-979f-47f6-8d56-a4fed1c1fd28"}}},{"cell_type":"markdown","source":["# Reading Data - Summary\n\nIn this notebook, we will quickly compare the various methods for reading in data.\n\n**Technical Accomplishments:**\n- Contrast the various techniques for reading data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"18318dc4-67f5-40b4-aa20-e4264e564d3f"}}},{"cell_type":"markdown","source":["## General\n- `SparkSession` is our entry point for working with the `DataFrames` API\n- `DataFrameReader` is the interface to the various read operations\n- Each reader behaves differently when it comes to the number of initial partitions and depends on both the file format (CSV vs Parquet vs ORC) and the source (Azure Blob vs Amazon S3 vs JDBC vs HDFS)\n- Ultimately, it is dependent on the implementation of the `DataFrameReader`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"750e544d-1ac0-4e8a-8b11-3a01d27044ea"}}},{"cell_type":"markdown","source":["-sandbox\n\n## Comparison\n| Type    | <span style=\"white-space:nowrap\">Inference Type</span> | <span style=\"white-space:nowrap\">Inference Speed</span> | Reason                                          | <span style=\"white-space:nowrap\">Should Supply Schema?</span> |\n|---------|--------------------------------------------------------|---------------------------------------------------------|----------------------------------------------------|:--------------:|\n| <b>CSV</b>     | <span style=\"white-space:nowrap\">Full-Data-Read</span> | <span style=\"white-space:nowrap\">Slow</span>            | <span style=\"white-space:nowrap\">File size</span>  | Yes            |\n| <b>Parquet</b> | <span style=\"white-space:nowrap\">Metadata-Read</span>  | <span style=\"white-space:nowrap\">Fast/Medium</span>     | <span style=\"white-space:nowrap\">Number of Partitions</span> | No (most cases)             |\n| <b>Tables</b>  | <span style=\"white-space:nowrap\">n/a</span>            | <span style=\"white-space:nowrap\">n/a</span>            | <span style=\"white-space:nowrap\">Predefined</span> | n/a            |\n| <b>JSON</b>    | <span style=\"white-space:nowrap\">Full-Read-Data</span> | <span style=\"white-space:nowrap\">Slow</span>            | <span style=\"white-space:nowrap\">File size</span>  | Yes            |\n| <b>Text</b>    | <span style=\"white-space:nowrap\">Dictated</span>       | <span style=\"white-space:nowrap\">Zero</span>            | <span style=\"white-space:nowrap\">Only 1 Column</span>   | Never          |\n| <b>JDBC</b>    | <span style=\"white-space:nowrap\">DB-Read</span>        | <span style=\"white-space:nowrap\">Fast</span>            | <span style=\"white-space:nowrap\">DB Schema</span>  | No             |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0306d767-bc87-4c0d-828d-3d6a27c6f1f2"}}},{"cell_type":"markdown","source":["##Reading CSV\n- `spark.read.csv(..)`\n- There are a large number of options when reading CSV files including headers, column separator, escaping, etc.\n- We can allow Spark to infer the schema at the cost of first reading in the entire file.\n- Large CSV files should always have a schema pre-defined."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d799bed5-6ad9-4060-906f-47c67a3d353f"}}},{"cell_type":"markdown","source":["## Reading Parquet\n- `spark.read.parquet(..)`\n- Parquet files are the preferred file format for big-data.\n- It is a columnar file format.\n- It is a splittable file format.\n- It offers a lot of performance benefits over other formats including predicate pushdown.\n- Unlike CSV, the schema is read in, not inferred.\n- Reading the schema from Parquet's metadata can be extremely efficient."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c54c906e-1c82-4055-b344-798fbf90a08a"}}},{"cell_type":"markdown","source":["## Reading Tables\n- `spark.read.table(..)`\n- The Databricks platform allows us to register a huge variety of data sources as tables via the Databricks UI.\n- Any `DataFrame` (from CSV, Parquet, whatever) can be registered as a temporary view.\n- Tables/Views can be loaded via the `DataFrameReader` to produce a `DataFrame`\n- Tables/Views can be used directly in SQL statements."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c54bec9c-e4ed-4faa-b577-673d86ead957"}}},{"cell_type":"markdown","source":["## Reading JSON\n- `spark.read.json(..)`\n- JSON represents complex data types unlike CSV's flat format.\n- Has many of the same limitations as CSV (needing to read the entire file to infer the schema)\n- Like CSV has a lot of options allowing control on date formats, escaping, single vs. multiline JSON, etc."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86d1001b-6800-48d5-be4e-4e27c6962d22"}}},{"cell_type":"markdown","source":["## Reading Text\n- `spark.read.text(..)`\n- Reads one line of text as a single column named `value`.\n- Is the basis for more complex file formats such as fixed-width text files."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dcdc268f-ff98-4af5-9ae5-106d4e2e4a97"}}},{"cell_type":"markdown","source":["## Reading JDBC\n- `spark.read.jdbc(..)`\n- Requires one database connection per partition.\n- Has the potential to overwhelm the database.\n- Requires specification of a stride to properly balance partitions."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9f2732d-dde3-4dbb-a999-f87bcee7887d"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/labs.png) Reading Data Lab\nIt's time to put what we learned to practice.\n\nGo ahead and open the notebook [Reading Data Lab]($./Reading Data 8 - Lab) and complete the exercises."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74d65025-0040-4826-8ae3-c4cf8edb58f2"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd08aee4-13ba-4755-9f60-40c68b6e7e1f"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DFRW 7 - Summary","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3794944577840488}},"nbformat":4,"nbformat_minor":0}
