{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"311b66ad-dc4d-4fef-a0e7-7ed9bdb5651c"}}},{"cell_type":"markdown","source":["# Reading Data - JDBC Connections\n\n**Technical Accomplishments:**\n- Read Data from Relational Database"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06ae4a14-2ebd-4fe3-acd8-6d73d1270699"}}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup<br>\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the start of each lesson (see the next cell) and the **`Classroom-Cleanup`** cell at the end of each lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b837d976-c79a-4b85-b706-103b698bf890"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"728c84ae-69a3-4ce3-85cb-a1d69be4015b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Reading from JDBC\n\nWorking with a JDBC data source is significantly different than any of the other data sources.\n* Configuration settings can be a lot more complex.\n* Often required to \"register\" the JDBC driver for the target database.\n* We have to juggle the number of DB connections.\n* We have to instruct Spark how to partition the data.\n\n**NOTE:** The database is read-only\n* For security reasons. \n* The notebook does not demonstrate writing to a JDBC database."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a881c853-24c9-4c3c-bf01-4e3b011fc4cb"}}},{"cell_type":"markdown","source":["* For examples of writing via JDBC, see \n  * <a href=\"https://docs.azuredatabricks.net/spark/latest/data-sources/sql-databases.html\" target=\"_blank\">Connecting to SQL Databases using JDBC</a>\n  * <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#jdbc-to-other-databases\" target=\"_blank\">JDBC To Other Databases</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ec486bb-6baf-49d8-aef1-d2d917be0986"}}},{"cell_type":"code","source":["%scala\n\n// Ensure that the driver class is loaded. \n// Seems to be necessary sometimes.\nClass.forName(\"org.postgresql.Driver\") "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f759de51-ca38-482f-b6fc-ff1e78c9cd77"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["tableName = \"training.people_1m\"\njdbcURL = \"jdbc:postgresql://54.213.33.240/training\"\n\n# Username and Password w/read-only rights\nconnProperties = {\n  \"user\" : \"readonly\",\n  \"password\" : \"readonly\"\n}\n\n# And for some consistency in our test to come\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15f09cb7-248e-4fb4-a694-d9480d7b33f5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["exampleOneDF = spark.read.jdbc(\n  url=jdbcURL,                # the JDBC URL\n  table=tableName,            # the name of the table\n  properties=connProperties)  # the connection properties\n\nexampleOneDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1716369e-100a-49e8-8ad6-20b5a99ebfa2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Question:** Compared to CSV and even Parquet, what is missing here?\n\n**Question:** Based on the answer to the previous question, what are the ramifications of the missing...?\n\n**Question:** Before you run the next cell, what's your best guess as to the number of partitions?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"250a181a-39f7-468b-baff-4a6550166f7e"}}},{"cell_type":"code","source":["print(\"Partitions: \" + str(exampleOneDF.rdd.getNumPartitions()) )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5c7be6d-1689-47ee-a6ae-09cdd7211d3d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) That's not Parallelized\n\nLet's try this again, and this time we are going to increase the number of connections to the database.\n\n** *Note:* ** *If any one of these properties is specified, they must all be specified:*\n* `partitionColumn` - the name of a column of an integral type that will be used for partitioning.\n* `lowerBound` - the minimum value of columnName used to decide partition stride.\n* `upperBound` - the maximum value of columnName used to decide partition stride\n* `numPartitions` - the number of partitions/connections\n\nTo quote the <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#jdbc-to-other-databases\" target=\"_blank\">Spark SQL, DataFrames and Datasets Guide</a>:\n> These options must all be specified if any of them is specified. They describe how to partition the table when reading in parallel from multiple workers. `partitionColumn` must be a numeric column from the table in question. Notice that `lowerBound` and `upperBound` are just used to decide the partition stride, not for filtering the rows in a table. So all rows in the table will be partitioned and returned. This option applies only to reading."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89fe6a4a-40a0-4b2f-b803-7af2cc40ae69"}}},{"cell_type":"code","source":["jdbcURL = \"jdbc:postgresql://54.213.33.240/training\"\n\nexampleTwoDF = spark.read.jdbc(\n  url=jdbcURL,                  # the JDBC URL\n  table=tableName,              # the name of the table\n  column=\"id\",                  # the name of a column of an integral type that will be used for partitioning.\n  lowerBound=1,                 # the minimum value of columnName used to decide partition stride.\n  upperBound=200000,            # the maximum value of columnName used to decide partition stride\n  numPartitions=8,              # the number of partitions/connections\n  properties=connProperties)    # the connection properties"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0032e5a6-de3d-46d0-82dd-7dbe40246a68"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's start with checking how many partitions we have (it should be 8)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"01fb464f-763b-4072-8d7a-e178036ace84"}}},{"cell_type":"code","source":["print(\"Partitions: \" + str(exampleTwoDF.rdd.getNumPartitions()) )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c4371b8-1da3-4724-9ccb-afb3859290f2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["But how many records were loaded per partition?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8507c278-4c62-4bb4-b2d7-d44e3f739844"}}},{"cell_type":"markdown","source":["Using the utility method we created above we can print the per-partition count."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"341488c4-3fa8-43e9-8ecc-f3f3e8ba5d10"}}},{"cell_type":"code","source":["printRecordsPerPartition(exampleTwoDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"297bfb6b-c0c3-4929-a45e-6de98b47b654"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["That might be a problem... notice how many records are in the last partition?\n\n**Question:** What are the performance ramifications of leaving our partitions like this?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"88d1caff-b961-4d87-85d7-4519961961e5"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) That's Not [Well] Distributed\n\nAnd this is one of the little gotchas when working with JDBC - to properly specify the stride, we need to know the minimum and maximum value of the IDs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48ce5a27-8b55-439b-b56d-e0b87a6827cf"}}},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\nminimumID = (exampleTwoDF\n  .select(min(\"id\"))   # Compute the minimum ID\n  .first()[\"min(id)\"]  # Extract as an integer\n)\nmaximumID = (exampleTwoDF\n  .select(max(\"id\"))   # Compute the maximum ID\n  .first()[\"max(id)\"]  # Extract as an integer\n)\nprint(\"Minimum ID: \" + str(minimumID))\nprint(\"Maximum ID: \" + str(maximumID))\nprint(\"-\"*80)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0b8c3ca-201d-4290-b531-c381ba2f7be2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now, let's try this one more time... this time with the proper stride:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9390c5d3-b76e-48e4-bb3f-817196bd0e51"}}},{"cell_type":"code","source":["exampleThree = spark.read.jdbc(\n  url=\"jdbc:postgresql://54.213.33.240/training\", # the JDBC URL\n  table=tableName,                                # the name of the table\n  column=\"id\",                                    # the name of a column of an integral type that will be used for partitioning.\n  lowerBound=minimumID,                           # the minimum value of columnName used to decide partition stride.\n  upperBound=maximumID,                           # the maximum value of columnName used to decide partition stride\n  numPartitions=8,                                # the number of partitions/connections\n  properties=connProperties)                      # the connection properties\n\nprintRecordsPerPartition(exampleThree)\nprint(\"-\"*80)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10e4b664-6cc2-4d6d-8ad6-d091a1ec5c5b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["And of course we can view that data here:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce2aa102-3972-42b6-8d88-fa273dda9457"}}},{"cell_type":"code","source":["display(exampleThree)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"639601e5-6295-4321-bffe-291eacb41c6f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d868f3ee-3701-453b-be2b-92ca9e49cb4d"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Cleanup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1080c515-b06b-4e9f-823c-a25367bbd0d7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Next Steps\n\n* [Reading Data #1 - CSV]($./Reading Data 1 - CSV)\n* [Reading Data #2 - Parquet]($./Reading Data 2 - Parquet)\n* [Reading Data #3 - Tables]($./Reading Data 3 - Tables)\n* [Reading Data #4 - JSON]($./Reading Data 4 - JSON)\n* [Reading Data #5 - Text]($./Reading Data 5 - Text)\n* Reading Data #6 - JDBC\n* [Reading Data #7 - Summary]($./Reading Data 7 - Summary)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f68d4b35-d20f-4564-bebe-776ef7afaf7b"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"431bf32c-3568-4de6-b942-53341adbb1b2"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DFRW 6 - JDBC","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3794944577840640}},"nbformat":4,"nbformat_minor":0}
