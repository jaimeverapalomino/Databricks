{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19e0ce33-4399-4dcf-a1b6-a355098b5c1c"}}},{"cell_type":"markdown","source":["# Reading Data - Tables\n\n**Technical Accomplishments:**\n* Demonstrate how to pre-register data sources in Databricks.\n* Introduce temporary views over files.\n* Read data from tables/views.\n* Regarding `printRecordsPerPartition(..)`, it \n  * converts the specified `DataFrame` to an RDD\n  * counts the number of records in each partition\n  * prints the results to the console."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1c73397-a26f-4ee5-9507-48f841b57dc6"}}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup<br>\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the start of each lesson (see the next cell) and the **`Classroom-Cleanup`** cell at the end of each lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"857e0142-2b5a-41a4-b592-87e354e4c8b8"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"950875fb-eac8-4177-9093-6bbc54fb59f1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Registering Tables in Databricks\n\nSo far we've seen purely programmatic methods for reading in data.\n\nDatabricks allows us to \"register\" the equivalent of \"tables\" so that they can be easily accessed by all users. \n\nIt also allows us to specify configuration settings such as secret keys, tokens, username & passwords, etc without exposing that information to all users."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9cf6da65-d184-4ef4-a2ed-a98c926a0a73"}}},{"cell_type":"markdown","source":["## Register a Table/View\n* Databricks UI has built in support for working with a number of different data sources\n* New ones are being added regularly\n* In our case we are going to upload the file <a href=\"http://files.training.databricks.com/static/data/pageviews_by_second_example.tsv\">pageviews_by_second_example.tsv</a>\n* .. and then use the UI to create a table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7507db34-ece1-4ac6-b7fc-032d1571e610"}}},{"cell_type":"markdown","source":["There are several benefits to this strategy:\n* Once setup, it never has to be done again\n* It is available for any user on the platform (permissions permitting)\n* Minimizes exposure of credentials\n* No real overhead to reading the schema (no infer-schema)\n* Easier to advertise available datasets to other users"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"282da89e-15d4-4e4c-87ed-294ba59e7b63"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Reading from a Table/View\n\nWe can now read in the \"table\" **pageviews_by_seconds_example** as a `DataFrame` with one simple command (and then print the schema):"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58936fe6-df05-4052-a158-71d2ec2fde98"}}},{"cell_type":"code","source":["pageviewsBySecondsExampleDF = spark.read.table(\"pageviews_by_second_example_tsv\")\n\npageviewsBySecondsExampleDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e63f60e1-6eaa-4cc3-9bce-739327c33370"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["And of course we can now view that data as well:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0bb85433-a93b-489b-8992-577cf07a06ea"}}},{"cell_type":"code","source":["display(pageviewsBySecondsExampleDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"904bb8cc-2b2a-4fa7-83c7-6d8c463f0258"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Review: Reading from Tables\n* No job is executed - the schema is stored in the table definition on Databricks.\n* The data types shown here are those we defined when we registered the table.\n* In our case, the file was uploaded to Databricks and is stored on the DBFS.\n  * If we used JDBC, it would open the connection to the database and read it in.\n  * If we used an object store (like what is backing the DBFS), it would read the data from source.\n* The \"registration\" of the table simply makes future access, or access by multiple users easier.\n* The users of the notebook cannot see username and passwords, secret keys, tokens, etc."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb0e7fb5-93d3-49a3-ba3a-bc8e4f90fae4"}}},{"cell_type":"markdown","source":["Let's take a look at some of the other details of the `DataFrame` we just created for comparison sake."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98fff870-a0f2-434b-afff-2475e4fa8b85"}}},{"cell_type":"code","source":["print(\"Partitions: \" + str(pageviewsBySecondsExampleDF.rdd.getNumPartitions()))\nprintRecordsPerPartition(pageviewsBySecondsExampleDF)\nprint(\"-\"*80)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"423da2c5-3f89-4721-be0e-ae7a8a124678"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Temporary Views\n\nTables that are loadable by the call `spark.read.table(..)` are also accessible through the SQL APIs.\n\nFor example, we already used Databricks to expose **pageviews_by_second_example_tsv** as a table/view."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d25b9fb-72f3-4008-9d77-8e36efddc67b"}}},{"cell_type":"code","source":["%sql\nselect * from pageviews_by_second_example_tsv limit(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"93a0d622-2d3d-4c1b-bf03-1b1f2055571f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["You can also take an existing `DataFrame` and register it as a view exposing it as a table to the SQL API.\n\nIf you recall from earlier, we have an instance called `parquetDF`.\n\nWe can create a [temporary] view with this call..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3a0b09f-f2d6-4327-b2aa-9a2fda125659"}}},{"cell_type":"code","source":["# create a DataFrame from a parquet file\nparquetFile = \"/mnt/training/wikipedia/pagecounts/staging_parquet_en_only_clean/\"\nparquetDF = spark.read.parquet(parquetFile)\n\n# create a temporary view from the resulting DataFrame\nparquetDF.createOrReplaceTempView(\"parquet_table\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"554b5e96-01d7-43c8-99fa-25a825b4298e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["And now we can use the SQL API to reference that same `DataFrame` as the table **parquet_table**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22a3ddc6-0d95-4486-8892-90db59bac974"}}},{"cell_type":"code","source":["%sql\nselect * from parquet_table order by requests desc limit(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0dc486b-410a-46c5-a7bd-6a00d7995276"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["** *Note #1:* ** *The method createOrReplaceTempView(..) is bound to the SparkSession meaning it will be discarded once the session ends.*\n\n** *Note #2:* ** On the other hand, the method createOrReplaceGlobalTempView(..) is bound to the spark application.*\n\n*Or to put that another way, I can use createOrReplaceTempView(..) in this notebook only. However, I can call createOrReplaceGlobalTempView(..) in this notebook and then access it from another.*"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96708d77-9e15-43cb-9378-5e4d8cfc515b"}}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be62cccf-c55c-42e4-92dc-db3954dfd926"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Cleanup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7ab9e5b-643d-4338-92b4-e598c8b44843"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Next Steps\n\n* [Reading Data #1 - CSV]($./Reading Data 1 - CSV)\n* [Reading Data #2 - Parquet]($./Reading Data 2 - Parquet)\n* Reading Data #3 - Tables\n* [Reading Data #4 - JSON]($./Reading Data 4 - JSON)\n* [Reading Data #5 - Text]($./Reading Data 5 - Text)\n* [Reading Data #6 - JDBC]($./Reading Data 6 - JDBC)\n* [Reading Data #7 - Summary]($./Reading Data 7 - Summary)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3746c086-fd00-47d5-8874-6c2022c2f716"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85e068c3-f1ed-4905-95d5-240f8d1508f7"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DFRW 3 - Tables","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3794944577840696}},"nbformat":4,"nbformat_minor":0}
