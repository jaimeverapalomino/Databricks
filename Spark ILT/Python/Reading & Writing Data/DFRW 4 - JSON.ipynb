{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6ea08b3-c872-49a4-b0a5-b0cea021d8e1"}}},{"cell_type":"markdown","source":["# Reading Data - JSON Files\n\n**Technical Accomplishments:**\n- Read data from:\n  * JSON without a Schema\n  * JSON with a Schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3c84e5e0-2751-49c2-a4c8-b91ecbabef54"}}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup<br>\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the start of each lesson (see the next cell) and the **`Classroom-Cleanup`** cell at the end of each lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99eda727-6e61-4de6-8434-81a4e7e60133"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5093a1f4-0cc8-4ccb-9130-48a5f707b398"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Reading from JSON w/ InferSchema\n\nReading in JSON isn't that much different than reading in CSV files.\n\nLet's start with taking a look at all the different options that go along with reading in JSON files."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6aa9b01-cefc-4f29-bfa7-562a528280a6"}}},{"cell_type":"markdown","source":["### JSON Lines\n\nMuch like the CSV reader, the JSON reader also assumes...\n* That there is one JSON object per line and...\n* That it's delineated by a new-line.\n\nThis format is referred to as **JSON Lines** or **newline-delimited JSON** \n\nMore information about this format can be found at <a href=\"http://jsonlines.org/\" target=\"_blank\">http://jsonlines.org</a>.\n\n** *Note:* ** *Spark 2.2 was released on July 11th 2016. With that comes File IO improvements for CSV & JSON, but more importantly, **Support for parsing multi-line JSON and CSV files**. You can read more about that (and other features in Spark 2.2) in the <a href=\"https://databricks.com/blog/2017/07/11/introducing-apache-spark-2-2.html\" target=\"_blank\">Databricks Blog</a>.*"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7f8eff3-8c25-4f49-aa6a-db18b98c8f64"}}},{"cell_type":"markdown","source":["### The Data Source\n* For this exercise, we will be using the file called **snapshot-2016-05-26.json** (<a href=\"https://wikitech.wikimedia.org/wiki/Stream.wikimedia.org/rc\" target=\"_blank\">4 MB</a> file from Wikipedia).\n* The data represents a set of edits to Wikipedia articles captured in May of 2016.\n* It's located on the DBFS at **dbfs:/mnt/training/wikipedia/edits/snapshot-2016-05-26.json**\n* Like we did with the CSV file, we can use **&percnt;fs ls ...** to view the file on the DBFS."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9d9086d6-e478-4904-9d89-2d346a70a947"}}},{"cell_type":"code","source":["%fs ls dbfs:/mnt/training/wikipedia/edits/snapshot-2016-05-26.json"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"29fac221-56ad-4318-b876-01485f9e18ae"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Like we did with the CSV file, we can use **&percnt;fs head ...** to peek at the first couple lines of the JSON file."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a93c360-017a-4c11-88a0-a0a0dd2b2ebb"}}},{"cell_type":"code","source":["%fs head dbfs:/mnt/training/wikipedia/edits/snapshot-2016-05-26.json"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49e0f626-a9a5-4360-8d92-8d3c93ab62d5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Read The JSON File\n\nThe command to read in JSON looks very similar to that of CSV.\n\nIn addition to reading the JSON file, we will also print the resulting schema."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03ad7cab-3dca-43eb-bf69-eda02a98edcb"}}},{"cell_type":"code","source":["jsonFile = \"dbfs:/mnt/training/wikipedia/edits/snapshot-2016-05-26.json\"\n\nwikiEditsDF = (spark.read           # The DataFrameReader\n    .option(\"inferSchema\", \"true\")  # Automatically infer data types & column names\n    .json(jsonFile)                 # Creates a DataFrame from JSON after reading in the file\n )\nwikiEditsDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16a07059-35ef-4dee-9981-baf73c3d805f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["With our DataFrame created, we can now take a peak at the data.\n\nBut to demonstrate a unique aspect of JSON data (or any data with embedded fields), we will first create a temporary view and then view the data via SQL:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47145b95-6f8f-427a-b489-470b116abaed"}}},{"cell_type":"code","source":["# create a view called wiki_edits\nwikiEditsDF.createOrReplaceTempView(\"wiki_edits\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff830110-c93a-4bbe-b219-aa153bf75de0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["And now we can take a peak at the data with simple SQL SELECT statement:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"479ece23-2fa7-4562-9878-faf0d5014dae"}}},{"cell_type":"code","source":["%sql\n\nSELECT * FROM wiki_edits "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bac6d166-1d61-4e20-b8c6-9562674f3c87"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Notice the **geocoding** column has embedded data.\n\nYou can expand the fields by clicking the right triangle in each row.\n\nBut we can also reference the sub-fields directly as we see in the following SQL statement:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54d5201e-b7a0-4269-ab8d-9c6a1f792a74"}}},{"cell_type":"code","source":["%sql\n\nSELECT channel, page, geocoding.city, geocoding.latitude, geocoding.longitude \nFROM wiki_edits \nWHERE geocoding.city IS NOT NULL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"daf0d67b-7b44-4fc1-819a-6af42e22aacd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Review: Reading from JSON w/ InferSchema\n\nWhile there are similarities between reading in CSV & JSON there are some key differences:\n* We only need one job even when inferring the schema.\n* There is no header which is why there isn't a second job in this case - the column names are extracted from the JSON object's attributes.\n* Unlike CSV which reads in 100% of the data, the JSON reader only samples the data.  \n**Note:** In Spark 2.2 the behavior was changed to read in the entire JSON file."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf778525-be10-4a45-842f-7f1ce90f7ec4"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Reading from JSON w/ User-Defined Schema\n\nTo avoid the extra job, we can (just like we did with CSV) specify the schema for the `DataFrame`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9bc56a12-7543-4282-a24d-6f252484ade9"}}},{"cell_type":"markdown","source":["### Step #1 - Create the Schema\n\nCompared to our CSV example, the structure of this data is a little more complex.\n\nNote that we can support complex data types as seen in the field `geocoding`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9241f9d5-3aa4-4a3c-8c7a-0811c03bc670"}}},{"cell_type":"code","source":["# Required for StructField, StringType, IntegerType, etc.\nfrom pyspark.sql.types import *\n\njsonSchema = StructType([\n  StructField(\"channel\", StringType(), True),\n  StructField(\"comment\", StringType(), True),\n  StructField(\"delta\", IntegerType(), True),\n  StructField(\"flag\", StringType(), True),\n  StructField(\"geocoding\", StructType([\n    StructField(\"city\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"countryCode2\", StringType(), True),\n    StructField(\"countryCode3\", StringType(), True),\n    StructField(\"stateProvince\", StringType(), True),\n    StructField(\"latitude\", DoubleType(), True),\n    StructField(\"longitude\", DoubleType(), True)\n  ]), True),\n  StructField(\"isAnonymous\", BooleanType(), True),\n  StructField(\"isNewPage\", BooleanType(), True),\n  StructField(\"isRobot\", BooleanType(), True),\n  StructField(\"isUnpatrolled\", BooleanType(), True),\n  StructField(\"namespace\", StringType(), True),\n  StructField(\"page\", StringType(), True),\n  StructField(\"pageURL\", StringType(), True),\n  StructField(\"timestamp\", StringType(), True),\n  StructField(\"url\", StringType(), True),\n  StructField(\"user\", StringType(), True),\n  StructField(\"userURL\", StringType(), True),\n  StructField(\"wikipediaURL\", StringType(), True),\n  StructField(\"wikipedia\", StringType(), True)\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3b38e36-b44a-4331-ad48-dc7a53405fb1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["That was a lot of typing to get our schema!\n\nFor a small file, manually creating the the schema may not be worth the effort.\n\nHowever, for a large file, the time to manually create the schema may be worth the trade off of a really long infer-schema process."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fae3a04f-b1e2-49f4-b443-17a1265ae1a5"}}},{"cell_type":"markdown","source":["### Step #2 - Read in the JSON\n\nNext, we will read in the JSON file and once again print its schema."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5fb1656-37f2-478b-b209-18375f06a77a"}}},{"cell_type":"code","source":["(spark.read            # The DataFrameReader\n  .schema(jsonSchema)  # Use the specified schema\n  .json(jsonFile)      # Creates a DataFrame from JSON after reading in the file\n  .printSchema()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff609303-1cb3-4bcb-b0eb-c4c4182cd758"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Review: Reading from JSON w/ User-Defined Schema\n* Just like CSV, providing the schema avoids the extra jobs.\n* The schema allows us to rename columns and specify alternate data types.\n* Can get arbitrarily complex in its structure."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6dc4add-33b8-4633-b6be-397ac00d6764"}}},{"cell_type":"markdown","source":["Let's take a look at some of the other details of the `DataFrame` we just created for comparison sake."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b2ebbb2-e0a4-491d-a047-f92b61f1ec7e"}}},{"cell_type":"code","source":["jsonDF = (spark.read\n  .schema(jsonSchema)\n  .json(jsonFile)    \n)\nprint(\"Partitions: \" + str(jsonDF.rdd.getNumPartitions()))\nprintRecordsPerPartition(jsonDF)\nprint(\"-\"*80)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"370721b6-c454-40e9-afe0-ee34333c21de"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["And of course we can view that data here:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ebbcad3-7d93-4e0d-8a23-60e01fc9a66c"}}},{"cell_type":"code","source":["display(jsonDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82c15f72-c90a-4a6f-8c2b-96eefd7d50b6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"748ea0a9-3216-471a-b61c-17970d9f98b6"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Cleanup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d0aa9682-51fc-4d77-8303-a6ecb8a635a2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Next Steps\n\n* [Reading Data #1 - CSV]($./Reading Data 1 - CSV)\n* [Reading Data #2 - Parquet]($./Reading Data 2 - Parquet)\n* [Reading Data #3 - Tables]($./Reading Data 3 - Tables)\n* Reading Data #4 - JSON\n* [Reading Data #5 - Text]($./Reading Data 5 - Text)\n* [Reading Data #6 - JDBC]($./Reading Data 6 - JDBC)\n* [Reading Data #7 - Summary]($./Reading Data 7 - Summary)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6dfbd80f-04eb-497c-bd92-251d91c0ad68"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a673ed3e-a6fe-480d-8d15-a0e7c0ec82bc"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DFRW 4 - JSON","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3794944577840501}},"nbformat":4,"nbformat_minor":0}
