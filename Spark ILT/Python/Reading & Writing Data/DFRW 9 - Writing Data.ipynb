{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ef5be9e-10f7-48ce-ad60-dc31d0c6dc43"}}},{"cell_type":"markdown","source":["# Writing Data\n\nJust as there are many ways to read data, we have just as many ways to write data.\n\nIn this notebook, we will take a quick peek at how to write data back out to Parquet files.\n\n**Technical Accomplishments:**\n- Writing data to Parquet files"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9aecc8a-3fb4-4d2e-ad20-c19559dd7a76"}}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup<br>\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the start of each lesson (see the next cell) and the **`Classroom-Cleanup`** cell at the end of each lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"715fd38f-14d2-4473-9f0a-dbb9a86d047c"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e1676fe-0b25-4202-8914-7a90bd0136c1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Writing Data\n\nLet's start with one of our original CSV data sources, **pageviews_by_second.tsv**:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc17df71-575b-4e7d-95a6-dfc43049a3f9"}}},{"cell_type":"code","source":["from pyspark.sql.types import *\n\ncsvSchema = StructType([\n  StructField(\"timestamp\", StringType(), False),\n  StructField(\"site\", StringType(), False),\n  StructField(\"requests\", IntegerType(), False)\n])\n\ncsvFile = \"/mnt/training/wikipedia/pageviews/pageviews_by_second.tsv\"\n\ncsvDF = (spark.read\n  .option('header', 'true')\n  .option('sep', \"\\t\")\n  .schema(csvSchema)\n  .csv(csvFile)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"87459c43-5c52-421d-b772-27d50a0f0e19"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now that we have a `DataFrame`, we can write it back out as Parquet files or other various formats."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd940fde-c2b5-48a3-933d-51e24483cf72"}}},{"cell_type":"code","source":["fileName = userhome + \"/pageviews_by_second.parquet\"\nprint(\"Output location: \" + fileName)\n\n(csvDF.write                       # Our DataFrameWriter\n  .option(\"compression\", \"snappy\") # One of none, snappy, gzip, and lzo\n  .mode(\"overwrite\")               # Replace existing files\n  .parquet(fileName)               # Write DataFrame to Parquet files\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a0b58b80-b18e-42ff-964c-06ad9194faa0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now that the file has been written out, we can see it in the DBFS:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c440b5f3-290d-4aef-af1e-feccb3dd44d6"}}},{"cell_type":"code","source":["display(\n  dbutils.fs.ls(fileName)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6380eba4-3a4d-452f-9c34-92b8901b2f5d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["And lastly we can read that same parquet file back in and display the results:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"950040f1-baf6-43bd-b79f-1517e9d041cd"}}},{"cell_type":"code","source":["display(\n  spark.read.parquet(fileName)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7beb159a-2dd9-4d95-9aee-d3d24eec8de5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9e00b0a-81a4-4516-8c64-57796a08e919"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Cleanup\"\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b84481e-5c31-4581-a581-e1e80629f250"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"464a5788-2df8-4151-a1b1-deedadb2167b"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DFRW 9 - Writing Data","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3794944577840549}},"nbformat":4,"nbformat_minor":0}
