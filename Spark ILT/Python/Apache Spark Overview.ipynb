{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56c17b61-807d-4aa6-8f43-3f8404f3dc1c"}}},{"cell_type":"markdown","source":["# Apache Spark Overview\n* Short History of Apache Spark\n* Who is Databricks?\n* What is Apache Spark?\n* A Unifying Engine\n* The RDD\n* DataFrames, Datasets & SQL\n* Scala, Python, Java, R & SQL\n* The Cluster: Drivers, Executors, Slots & Tasks\n* Quick Note on Jobs & Stages\n* Quick Note on Cluster Management\n* Local Mode & Databricks CE\n* Architectural & Administrative Topics"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"186e3a6a-febe-458f-b66f-2cbf62ca4f32"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Short History of Apache Spark\n* <a href=\"https://en.wikipedia.org/wiki/Apache_Spark\" target=\"_blank\">Apache Spark</a> started as a research project at the \nUniversity of California AMPLab, in 2009 by <a href=\"https://en.wikipedia.org/wiki/Matei_Zaharia\" target=\"_blank\">Matei Zaharia</a>.\n* In 2013, the project was\n  * donated to the Apache Software Foundation\n  * open sourced\n  * adopted the Apache 2.0 license\n* In February 2014, Spark became a Top-Level <a href=\"https://spark.apache.org/\" target=\"_blank\">Apache Project<a/>.\n* In November 2014, Spark founder <a href=\"https://en.wikipedia.org/wiki/Matei_Zaharia\" target=\"_blank\">Matei_Zaharia</a>'s \ncompany <a href=\"https://databricks.com\" target=\"_blank\">Databricks</a> set a new world record in large scale sorting using Spark.\n* Latest stable release: <a href=\"https://spark.apache.org/downloads.html\" target=\"_blank\">CLICK-HERE</a>\n* 600,000+ lines of code (75% Scala)\n* Built by 1,000+ developers from more than 250+ organizations"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ddc69702-b0ee-4908-befc-a84363b69f72"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Who is Databricks?\n* <a href=\"https://databricks.com\" target=\"_blank\">Databricks</a> was started by Spark founder <a href=\"https://en.wikipedia.org/wiki/Matei_Zaharia\" target=\"_blank\">Matei Zaharia</a>.\n* Today, Databricks remains the #1 contributor to Apache Spark.\n* Fully committed to maintaining Apache Spark as an Open Source project.\n* *\"Provides a Unified Analytics Platform that accelerates innovation by unifying data science, engineering, and business.\"*\n  * Databricks Workspace - Interactive Data Science & Collaboration.\n  * Databricks Workflows - Production Jobs & Workflow Automation.\n  * Databricks Runtime\n  * Databricks I/O (DBIO) - Optimized Data Access Layer\n  * Databricks Serverless - Fully Managed Auto-Tuning Platform\n  * Databricks Enterprise Security (DBES) - End-To-End Security & Compliance\n* Actively involved with the Apache Spark community:\n  * Private & Public Training\n  * Consulting Services\n  * Hosting Meetups\n  * Blogs, Articles, Videos\n  * And Much More!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99a1e583-68e1-4a32-aca9-6560d89ba244"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) What is Apache Spark?\n\nSpark is a unified processing engine that can analyze big data using SQL, machine learning, graph processing or real-time stream analysis:\n\n![Spark Engines](https://files.training.databricks.com/images/wiki-book/book_intro/spark_4engines.png)\n<br/>\n<br/>\n* At its core is the Spark Engine.\n* The DataFrames API provides an abstraction above RDDs while simultaneously improving performance 5-20x over traditional RDDs with its Catalyst Optimizer.\n* Spark ML provides high quality and finely tuned machine learning algorithms for processing big data.\n* The Graph processing API gives us an easily approachable API for modeling pairwise relationships between people, objects, or nodes in a network.\n* The Streaming APIs give us End-to-End Fault Tolerance, with Exactly-Once semantics, and the possibility for sub-millisecond latency.\n\nAnd it all works together seamlessly!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68f51fa6-754c-494c-84e5-54aaf261910c"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) A Unifying Engine\n\nAnd as a compute engine, Apache Spark is not tied to a specific environment or data warehouse strategy.\n\n![Unified Engine](https://files.training.databricks.com/images/105/unified-engine.png)\n<br/>\n<br/>\n* Built upon the Spark Core\n* Apache Spark is data and environment agnostic.\n* Languages: **Scala, Java, Python, R, SQL**\n* Environments: **Yarn, Docker, EC2, Mesos, OpenStack, Databricks (our favorite), Digital Ocean, and much more...**\n* Data Sources: **Hadoop HDFS, Casandra, Kafka, Apache Hive, HBase, JDBC (PostgreSQL, MySQL, etc.), CSV, JSON, Azure Blob, Amazon S3, ElasticSearch, Parquet, and much, much more...**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5caf4d66-8e41-452e-9dbd-482e032f9c71"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) RDDs\n* The primary data abstraction of Spark engine is the RDD: Resilient Distributed Dataset\n  * Resilient, i.e., fault-tolerant with the help of RDD lineage graph and so able to recompute missing or damaged partitions due to node failures.\n  * Distributed with data residing on multiple nodes in a cluster.\n  * Dataset is a collection of partitioned data with primitive values or values of values, e.g., tuples or other objects.\n* The original paper that gave birth to the concept of RDD is <a href=\"https://cs.stanford.edu/~matei/papers/2012/nsdi_spark.pdf\" target=\"_blank\">Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing</a> by Matei Zaharia et al.\n* Starting with Spark 2.0, we treat RDDs as the assembly language of the Spark ecosystem.\n* DataFrames, Datasets & SQL provide the higher level abstraction over RDDs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7540786f-e9b4-43dd-b22a-4df05c919e58"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Scala, Python, Java, R & SQL\n* Besides being able to run in many environments...\n* Apache Spark makes the platform even more approachable by supporting multiple languages:\n  * Scala - Apache Spark's primary language.\n  * Python - More commonly referred to as PySpark\n  * R - <a href=\"https://spark.apache.org/docs/latest/sparkr.html\" target=\"_blank\">SparkR</a> (R on Spark)\n  * Java\n  * SQL - Closer to ANSI SQL 2003 compliance\n    * Now running all 99 TPC-DS queries\n    * New standards-compliant parser (with good error messages!)\n    * Subqueries (correlated & uncorrelated)\n    * Approximate aggregate stats\n* With the older RDD API, there are significant differences with each language's implementation, namely in performance.\n* With the newer DataFrames API, the performance differences between languages are nearly nonexistence (especially for Scala, Java & Python).\n* With that, not all languages get the same amount of love - just the same, that API gap for each language is rapidly closing, especially between Spark 1.x and 2.x.\n\n![RDD vs DataFrames](https://files.training.databricks.com/images/105/rdd-vs-dataframes.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6a92862-fe15-4c07-b337-6cddbbf216b1"}}},{"cell_type":"markdown","source":["-sandbox\n\n##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) The Cluster: Drivers, Executors, Slots & Tasks\n![Spark Physical Cluster, slots](https://files.training.databricks.com/images/105/spark_cluster_slots.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3fadb74d-1b97-45e0-b9ee-290808713ea5"}}},{"cell_type":"markdown","source":["* The **Driver** is the JVM in which our application runs.\n* The secret to Spark's awesome performance is parallelism.\n  * Scaling vertically is limited to a finite amount of RAM, Threads and CPU speeds.\n  * Scaling horizontally means we can simply add new \"nodes\" to the cluster almost endlessly.\n* We parallelize at two levels:\n  * The first level of parallelization is the **Executor** - a Java virtual machine running on a node, typically, one instance per node.\n  * The second level of parallelization is the **Slot** - the number of which is determined by the number of cores and CPUs of each node.\n* Each **Executor** has a number of **Slots** to which parallelized **Tasks** can be assigned to it by the **Driver**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00143ca4-df4b-4972-84db-d51a22982a55"}}},{"cell_type":"markdown","source":["![Spark Physical Cluster, tasks](https://files.training.databricks.com/images/105/spark_cluster_tasks.png)\n<br/>\n<br/>\n* The JVM is naturally multithreaded, but a single JVM, such as our **Driver**, has a finite upper limit.\n* By creating **Tasks**, the **Driver** can assign units of work to **Slots** for parallel execution.\n* Additionally, the **Driver** must also decide how to partition the data so that it can be distributed for parallel processing (not shown here).\n* Consequently, the **Driver** is assigning a **Partition** of data to each task - in this way each **Task** knows which piece of data it is to process.\n* Once started, each **Task** will fetch from the original data source the **Partition** of data assigned to it."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa9681f2-3de5-4773-9fa5-ac7aa296dc85"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Quick Note on Jobs & Stages\n* Each parallelized action is referred to as a **Job**.\n* The results of each **Job** (parallelized/distributed action) is returned to the **Driver**.\n* Depending on the work required, multiple **Jobs** will be required.\n* Each **Job** is broken down into **Stages**. \n* This would be analogous to building a house (the job)\n  * The first stage would be to lay the foundation.\n  * The second stage would be to erect the walls.\n  * The third stage would be to add the room.\n  * Attempting to do any of these steps out of order just won't make sense, if not just impossible.\n  \n** *Note:* ** *We will be going much deeper into Jobs & Stages and the *<br/>\n*effect they have on our software as we progress through this class.*"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed67b09b-74ea-4134-9186-f2b999864e4e"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Quick Note on Cluster Management\n\n* At a much lower level, Spark Core employs a **Cluster Manager** that is responsible for provisioning nodes in our cluster.\n  * Databricks provides a robust, high-performing **Cluster Manager** as part of its overall offerings.\n  * Additional Cluster Managers are available for \n    <a href=\"https://spark.apache.org/docs/latest/running-on-mesos.html\" target=\"_blank\">Mesos</a>,\n    <a href=\"https://spark.apache.org/docs/latest/running-on-yarn.html\" target=\"_blank\">Yarn</a> and by other third parties.\n  * In addition to this, Spark has a <a href=\"https://spark.apache.org/docs/latest/spark-standalone.html\" target=\"_blank\">Standalone</a> mode in which you manually configure each node.\n* In each of these scenarios, the **Driver** is [presumably] running on one node, with each **Executors** running on N different nodes.\n* For the sake of this class, we don't need to concern ourselves with cluster management.\n  * Ya Databricks!\n* From a developer's and student's perspective my primary focus is on...\n  * The number of **Partitions** my data is divided into.\n  * The number of **Slots** I have for parallel execution.\n  * How many **Jobs** am I triggering?\n  * And lastly the **Stages** those jobs are divided into."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc8bed17-10ac-43b4-ad5c-ed0c9acb822f"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Architectural & Administrative Topics\n\nArchitectural & administrative topics go beyond the scope of this class. Our goal is to focus<br/>\non the core components of Spark that you need to know to get started developing applications.<br/>\n\n**Examples include:**\n* Which cluster manager should I use?\n* How should I configure the Executor's JVM for minimum performance?\n* What is the moral implication of setting the *spark.executor.logs.rolling.strategy* parameter to \"time\"?\n* Why does it make kittens cry in China when I run Apache Spark with the *spark.pet.kitten* flag set to true?\n\nWe will be discussing the internals of Apache Spark as it relates to a developer's role - it's not strictly about the API.\n\nAnd we don't want to leave you hanging!\n\nIf you do have an advanced, kitten-type question, we encourage you to post it to this class' Q&A. \n\nAn instructor or engineer will do their best to help answer your question if not at the very least point you towards a solution."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d962a3e0-751f-43bb-989c-18bf7968cddf"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"863bc2ee-88d6-4200-8aa6-3d53fd387826"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Apache Spark Overview","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3794944577840739}},"nbformat":4,"nbformat_minor":0}
