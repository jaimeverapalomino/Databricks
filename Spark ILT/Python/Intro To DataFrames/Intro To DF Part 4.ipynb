{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57c44a78-3904-4d58-b7ae-0ce2937843f9"}}},{"cell_type":"markdown","source":["# Intro To DataFrames, Part #4\n\n**Technical Accomplishments:**\n* Create a User Defined Function (UDF)\n* Execute a join operation between two `DataFrames`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4ced529-5369-47a2-b1c4-3b4dee206155"}}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup<br>\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the start of each lesson (see the next cell) and the **`Classroom-Cleanup`** cell at the end of each lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ce83019-dd47-421e-a529-5def6d8e9390"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92047b9d-a6f9-4e8e-bde2-01849fff9ddd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) The Data Source\n\nThis data uses the **Pageviews By Seconds** data set."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ae4b244-0ae4-4815-ba21-f725f066bb40"}}},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n# I've already gone through the exercise to determine\n# how many partitions I want and in this case it is...\npartitions = 8\n\n# Make sure wide operations don't repartition to 200\nspark.conf.set(\"spark.sql.shuffle.partitions\", str(partitions))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"629f58d4-0f0c-46d7-94af-393900c1e316"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["(source, sasEntity, sasToken) = getAzureDataSource()\nspark.conf.set(sasEntity, sasToken)\n\nparquetFile = source + \"/wikipedia/pageviews/pageviews_by_second.parquet/\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"feca1155-d8e4-4413-8272-15292016079a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Create our initial DataFrame. We can let it infer the \n# schema because the cost for parquet files is really low.\npageviewsDF = (spark.read\n  .option(\"inferSchema\", \"true\")                # The default, but not costly w/Parquet\n  .parquet(parquetFile)                         # Read the data in\n  .repartition(partitions)                      # From 7 >>> 8 partitions\n  .withColumnRenamed(\"timestamp\", \"capturedAt\") # rename and convert to timestamp datatype\n  .withColumn(\"capturedAt\", unix_timestamp( col(\"capturedAt\"), \"yyyy-MM-dd'T'HH:mm:ss\").cast(\"timestamp\") )\n  .orderBy( col(\"capturedAt\"), col(\"site\") )    # sort our records\n  .cache()                                      # Cache the expensive operation\n)\n# materialize the cache\npageviewsDF.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1da90b07-4fbb-42eb-aa5f-d4850d7ebf41"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(pageviewsDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d0ba4168-571a-4314-8c7d-1e05624109ae"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) date_format()\n\nToday, we want to aggregate all the data **by the day of week** (Monday, Tuesday, Wednesday)...\n\n...and then **sum all the requests**.\n\nOur goal is to see **which day of the week** has the most traffic.\n\nOn of the functions that can help us with this the operation `date_format(..)` from the `functions` package.\n\nIf you recall from our review of `unix_timestamp(..)` Spark uses the <a href=\"https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html\" target=\"_blank\">SimpleDateFormat</a> from the Java API for parsing (and formatting)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7825c42-86be-465f-99e8-2c55f6551999"}}},{"cell_type":"code","source":["# Create a new DataFrame\nbyDayOfWeekDF = (pageviewsDF\n  .groupBy( date_format( col(\"capturedAt\"), \"E\") )         # format as Mon, Tue and then aggregate\n  .sum()                                                   # produce the sum of all records\n  .select( col(\"date_format(capturedAt, E)\").alias(\"dow\"), # rename to \"dow\"\n           col(\"sum(requests)\").alias(\"total\"))            # rename to \"total\"\n  .orderBy( col(\"dow\") )                                   # sort by \"dow\" MTWTFSS\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61b315b3-3be1-48df-a633-7dc63aedfa9a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["With that's done, let's look at the result.\n\nBut not just as a list of tables...\n\nSometimes you miss important details when you are looking at just numbers.\n\nLet's try a bar graph - all you have to do is click on the graph icon below your results."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a213cf8a-171d-4d27-8755-296fd130da98"}}},{"cell_type":"code","source":["display(byDayOfWeekDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"441b8d57-bda9-431c-ac13-f3c0065dc670"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["What's wrong with this graph?\n\nWhat if we could change the labels from \"Mon\" to \"1-Mon\" and \"Tue\" to \"2-Tue\"?\n\nWould that fix the problem? Sure...\n\nWhat API call(s) would solve that problem...\n\nWell there isn't one. We'll just have to create our own..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b06f927-b9fe-4114-97fe-ffb325737a25"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) User Defined Functions (UDF)\n\nAs you've seen, `DataFrames` can do anything!\n\nActually they can't.\n\nThere will always be the case for a transformation that cannot be accomplished with the provided functions.\n\nTo address those cases, Apache Spark provides for **User Defined Functions** or **UDF** for short.\n\nHowever, they come with a cost...\n* **UDFs cannot be optimized** by the Catalyst Optimizer - someday, maybe, but for now it has no insight to your code.\n* The function **has to be serialized** and sent out to the executors - this is a one-time cost, but a cost just the same.\n* If you are not careful, you could **serialize the whole world**.\n* In the case of Python, there is even more over head - we have to **spin up a Python interpreter** on every Executor to run your Lambda.\n\nLet's start with our function..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"760cc032-c504-4c57-91be-fa0538685144"}}},{"cell_type":"code","source":["def mapDayOfWeek(day):\n  _dow = {\"Mon\": \"1\", \"Tue\": \"2\", \"Wed\": \"3\", \"Thu\": \"4\", \"Fri\": \"5\", \"Sat\": \"6\", \"Sun\": \"7\"}\n  \n  n = _dow.get(day)\n  if n:\n    return n + \"-\" + day\n  else:\n    return \"UNKNOWN\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c10b582-3419-4e31-b27a-a656e8d0c297"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["And now we can test our function..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be4c6b49-125a-473e-a679-0b5741aa4767"}}},{"cell_type":"code","source":["assert \"1-Mon\" == mapDayOfWeek(\"Mon\")\nassert \"2-Tue\" == mapDayOfWeek(\"Tue\")\nassert \"3-Wed\" == mapDayOfWeek(\"Wed\")\nassert \"4-Thu\" == mapDayOfWeek(\"Thu\")\nassert \"5-Fri\" == mapDayOfWeek(\"Fri\")\nassert \"6-Sat\" == mapDayOfWeek(\"Sat\")\nassert \"7-Sun\" == mapDayOfWeek(\"Sun\")\nassert \"UNKNOWN\" == mapDayOfWeek(\"Xxx\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f0934d3-d4e3-476d-a522-123ccff3a00d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Great, it works! Now define a UDF that wraps this function:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ee38658-47a2-4229-950c-4296cd0d1c31"}}},{"cell_type":"code","source":["prependNumberUDF = udf(mapDayOfWeek)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62dcf9e3-0809-487b-86c3-35338dfafd36"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["At this point, we have told Spark that we want that **function to be serialized**...\n\nand sent out **to each executor**.\n\n**We can test it** as a UDF with a simple query..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0cbd87f6-dec8-43b5-b4d6-1eec690df42e"}}},{"cell_type":"code","source":["(byDayOfWeekDF\n   .select( prependNumberUDF( col(\"dow\")) )\n   .show()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c37f6fe2-5f79-41ba-b8af-ad0eedaa3017"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Our UDF looks like **it's working**. \n\nNext, let's **apply the UDF** and also **order the x axis** from Mon -> Sun\n\nOnce it's done executing, we can **render our bar graph**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97e4beff-8bef-42aa-be3a-c9e6abae22f7"}}},{"cell_type":"code","source":["display(\n  byDayOfWeekDF\n    .select( prependNumberUDF(col(\"dow\")).alias(\"Day Of Week\"), col(\"total\") )\n    .orderBy(\"Day Of Week\")\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"345546e9-a578-41b0-8dc6-ce8d0664a96c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) I Lied\n\n> What API call(s) would solve that problem...<br/>\n> Well there isn't one. We'll just have to create our own...\n\nThe truth is that **there is already a function** to do exactly what we want.\n\nBefore you go and create your own UDF, check, double check, triple check to make sure it doesn't exist.\n\nRemember...\n* UDFs induce a performance hit.\n* Big ones in the case of Python.\n* The Catalyst Optimizer cannot optimize it.\n* And you are re-inventing the wheel.\n\nIn this case, the solution to our problem is the operation `date_format(..)` from the `...sql.functions`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f6a7e7b-5dee-43f4-b357-76265cbb6528"}}},{"cell_type":"code","source":["display(\n  pageviewsDF\n    .groupBy( date_format(col(\"capturedAt\"), \"u-E\").alias(\"Day Of Week\") )\n    .sum()\n    .orderBy(col(\"Day Of Week\"))\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"71eedbe5-08c2-4113-b91c-55fcc04b2d1b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Conclusions?\n  \nSo what can we infer from this data?\n  \nRemember... we draw our conclusions first and **then** back 'em up with the data.\n0. Why does Monday have more records than any day of the week?\n0. Why does the weekend have less records than the rest of the week?\n0. What can we conclude about usage from Friday to Saturday to Sunday?\n0. Is there a correlation between that and Sunday to Monday?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa852fc1-c573-44a9-8fe3-926bc3c32349"}}},{"cell_type":"markdown","source":["See also <a href=\"https://docs.azuredatabricks.net/spark/latest/spark-sql/udaf-scala.html\" target=\"_blank\">User Defined Aggregate Functions</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aef73a34-7d9a-41e6-827f-92e33eff2fd6"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) What About Mondays?\n\nSomething is up with Mondays.\n\nBut is the problem unique to Mobile or Desktop?\n\nTo answer this, we can fork the `DataFrame`.\n\nOne for Mobile, another for Desktop."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"338092f4-121b-4c03-a8b7-97167daf8131"}}},{"cell_type":"code","source":["mobileDF = (pageviewsDF\n    .filter(col(\"site\") == \"mobile\")\n    .groupBy( date_format(col(\"capturedAt\"), \"u-E\").alias(\"Day Of Week\") )\n    .sum()\n    .withColumnRenamed(\"sum(requests)\", \"Mobile Total\")\n    .orderBy(col(\"Day Of Week\"))\n)\ndesktopDF = (pageviewsDF\n    .filter(col(\"site\") == \"desktop\")\n    .groupBy( date_format(col(\"capturedAt\"), \"u-E\").alias(\"Day Of Week\") )\n    .sum()\n    .withColumnRenamed(\"sum(requests)\", \"Desktop Total\")\n    .orderBy(col(\"Day Of Week\"))\n)\n# Cache and materialize\nmobileDF.cache().count()\ndesktopDF.cache().count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c7afb0a-919d-4c38-a52b-d7733bf6b4a0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now we can create two graphs, one for Mobile and another for Desktop.\n\nBut more realistically, I can better see the scale between the two if I could view them in one graph.\n\nBut even more importantly, one graph is really just a semi-contrived reason to demonstrate a `join(..)` operation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b35afa6d-d792-4d3c-bed7-67e34dba89b9"}}},{"cell_type":"code","source":["display(mobileDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9487fec3-ee5e-484d-b7c6-8393cd9179f2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(desktopDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"661771d1-7639-4342-93a0-2c4152ec2cfe"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) join(..)\n\nIf you a familiar with SQL joins then `DataFrame.join(..)` should be pretty strait forward.\n\nWe start with the left side - in this example `desktopDF`.\n\nThen join the right side - in this example `mobileDF`.\n\nThe tricky part is the join condition.\n\nWe want all records to align by the **Day Of Week** column.\n\nThe problem is that if we used `$\"Day Of Week\"` or `col(\"Day Of Week\")` the `DataFrame` cannot tell which source we are referring to...\n* `desktopDF`'s version of **Day Of Week** or\n* `mobileDF`'s version of **Day Of Week**\n\nIf you recall from our discussions on the `Column` class, one option was to use the `DataFrame` to return an instance of a `Column`.\n\nFor example."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f9c9a8cb-e4e6-42dc-ac3d-bc6eea904cb0"}}},{"cell_type":"code","source":["columnA = desktopDF[\"Day Of Week\"]\ncolumnB = mobileDF[\"Day Of Week\"]\n\nprint(columnA) # Python hack to print the data type\nprint(columnB)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"228a6ad2-87fe-4e47-a7a2-f7fc08d1f7fe"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["And now we can put it all together..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32b1ae6d-d90d-47dd-848c-3830d232bc62"}}},{"cell_type":"code","source":["tempDF = desktopDF.join(mobileDF, desktopDF[\"Day Of Week\"] == mobileDF[\"Day Of Week\"])\n\ndisplay(tempDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9e4db14-3e2f-46e2-811b-ca0213704261"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["As we can see above, we now have the two `DataFrames` \"joined\" into a single one.\n\nHowever, if you notice, we have two **Day Of Week** columns.\n\nThis will just create headaches later.\n\nLet's drop `desktopDF`'s copy of **Day Of Week** using the same technique we used in the join.\n\nAnd while we are at it, use a `select(..)` transformation to rearrange the columns."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3c8e2d73-c8b2-4f24-abe6-7d023be19a25"}}},{"cell_type":"code","source":["joinedDF = (tempDF\n  .drop(desktopDF[\"Day Of Week\"])\n  .select(col(\"Day Of Week\"), col(\"Desktop Total\"), col(\"Mobile Total\"))\n)\ndisplay(joinedDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a48bb6d0-2227-4fa4-9ff7-91cb76ea9b44"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Of course, there is always more than one way to solve the same problem.\n\nIn this case we can use an equi-join:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc94d37e-5a4e-462b-bc95-48c3da6143b4"}}},{"cell_type":"code","source":["altDF = desktopDF.join(mobileDF, \"Day Of Week\")\n\ndisplay(altDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6dbcbdc7-5adc-4dbb-9743-f106aae9a131"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["And just to wrap it up, plot the graph.\n\nIt should only show one of the two numbers.\n\nOpen the **Plot Options..** and...\n* Set the **Keys** to **Day Of Week**.\n* Set the **Values** to both **Desktop Total** AND **Mobile Total**.\n* Hint: You can drag and drop the values."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"40f7c517-38b6-478f-bce7-6f708b0497da"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) More Conclusions?\n  \nNow, what can we infer from this data?\n  \n0. What is the difference between the weekend numbers (F/S/S) for Mobile vs Desktop?\n0. What would explain the difference in weekend numbers for the two sites?\n0. How does the weekend numbers compare to Mondays? For Desktop? For Mobile?\n0. What conclusion can we draw about Mondays?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67caa407-0a85-47c6-b5a0-aec83a81e861"}}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8c8ffe8-2e56-438a-bfab-eceb9bf091e5"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Cleanup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf060fce-c234-4e1a-a0b8-d7f5b074246f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/labs.png) Data Frames Lab #4\n\nUnlike the previous labs, this next lab does not have an emphasis on UDFs or Joins.\n\nHowever, you will have to draw on everything you have learned so far in order to answer the question...\n\nWhat's up with Monday?\n\nGo ahead and open the notebook [Introduction to DataFrames, Lab #4]($./Intro To DF Part 4 Lab) and complete the exercises."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c458405-e67f-48eb-9f1f-87ab2d97bb70"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66ae33f4-852c-48ec-84dc-6771599a933d"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Intro To DF Part 4","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3794944577839408}},"nbformat":4,"nbformat_minor":0}
