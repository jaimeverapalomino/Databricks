{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b041e962-fa3e-4607-94e7-c44283914f17"}}},{"cell_type":"markdown","source":["# Intro To DataFrames, Part #5\n\n**Technical Accomplishments:**\n* Introduce the concept of Broadcast Joins"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53891059-221b-4fb0-bc11-17613f999254"}}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup<br>\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the start of each lesson (see the next cell) and the **`Classroom-Cleanup`** cell at the end of each lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a92b18f9-cb76-4662-8af4-a5d38385012a"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5efb1068-1166-4e08-b9ef-5611411c2af3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) The Data Source\n\nThis data uses the **Pageviews By Seconds** data set."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d35fc48-37c5-4c20-9fa6-f4a859a851ba"}}},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n# I've already gone through the exercise to determine\n# how many partitions I want and in this case it is...\npartitions = 8\n\n# Make sure wide operations don't repartition to 200\nspark.conf.set(\"spark.sql.shuffle.partitions\", str(partitions))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a290691b-79e8-42cf-a6eb-aba241cb41dd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["(source, sasEntity, sasToken) = getAzureDataSource()\nspark.conf.set(sasEntity, sasToken)\n\n# The directory containing our parquet files.\nparquetFile = source + \"/wikipedia/pageviews/pageviews_by_second.parquet/\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d6cfffc-f9cd-4d95-aa3a-60ba3144ff80"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Create our initial DataFrame. We can let it infer the \n# schema because the cost for parquet files is really low.\npageviewsDF = (spark.read\n  .option(\"inferSchema\", \"true\")                # The default, but not costly w/Parquet\n  .parquet(parquetFile)                         # Read the data in\n  .repartition(partitions)                      # From 7 >>> 8 partitions\n  .withColumnRenamed(\"timestamp\", \"capturedAt\") # rename and convert to timestamp datatype\n  .withColumn(\"capturedAt\", unix_timestamp( col(\"capturedAt\"), \"yyyy-MM-dd'T'HH:mm:ss\").cast(\"timestamp\") )\n  .orderBy( col(\"capturedAt\"), col(\"site\") )    # sort our records\n  .cache()                                      # Cache the expensive operation\n)\n# materialize the cache\npageviewsDF.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef5c848c-fc80-424a-8281-1022ac7db122"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Broadcast Joins\n\nIf you saw the section on UDFs, you know that we can **aggregate by the Day-Of-Week**.\n\nWe **first used a UDF** only to discover that there was a built in function to do the exact same thing.\n\nWe saw that **Monday had more data** than any other day of the week.\n\nWe then forked the `DataFrame` so as to compare **Mobile Requests to Desktop Requests**.\n\nNext, we **joined those to `DataFrames`** into one so that we could easily compare the two sets of data.\n\nWe know that the problem with the data **has nothing to do with Mobile vs Desktop**.\n\nSo we don't need that type of join (two ~large `DataFrames`)\n\nHowever, what if we wanted to **reproduce our first exercise** (counts per day-of-week)...\n* without a UDF...\n* with a lookup table for the day-of-week...\n* with a join between the pageviews and the lookup table.\n\nWhat's different about this example is that we are **joining a big `DataFrame` to a small `DataFrame`**.\n\nIn this scenario, Spark can optimize the join and **avoid the expensive shuffle** with a **Broadcast Join**.\n\nLet's start with two `DataFrames`\n* The first we will derive from our original `DataFrame`. In this case, we will use a simple number for the day-of-week.\n* The second `DataFrame` will map that number (1-7) to the labels **Monday**, **Tue**, **W**, or whatever...\n\nLet's take a look at our first `DataFrame`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a909f6e-3f2c-4232-a826-06a7ceacce22"}}},{"cell_type":"code","source":["columnTrans = date_format(col(\"capturedAt\"), \"u\").alias(\"dow\")\n\npageviewsWithDowDF = (pageviewsDF\n    .withColumn(\"dow\", columnTrans)  # Add the column dow\n)\n(pageviewsWithDowDF\n  .cache()                           # mark the data as cached\n  .count()                           # materialize the cache\n)\ndisplay(pageviewsWithDowDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f719c6f-a795-4bcc-a796-fc2a5a7c75a7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["All we did is add one column **dow** that has the value **1** for **Monday**, **2** for **Tuesday**, etc.\n\nNext, we are going to load a mapping of 1, 2, 3, etc. to Mon, Tue, Wed, etc from a **REALLY** small `DataFrame`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"75379b4f-c20e-44cc-ae09-bf9cdf0c898f"}}},{"cell_type":"code","source":["labelsDF = spark.read.parquet(source + \"/day-of-week\")\n\ndisplay(labelsDF) # view our labels"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2fa84a21-b4a7-48d9-b29b-8e3a846c4c22"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now that we have two `DataFrames`...\n\nWe can execute a join between the two `DataFrames`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0662ee72-78ca-4d6f-a364-1e80c2757c5c"}}},{"cell_type":"code","source":["joinedDowDF = (pageviewsWithDowDF\n  .join(labelsDF, pageviewsWithDowDF[\"dow\"] == labelsDF[\"dow\"])\n  .drop( pageviewsWithDowDF[\"dow\"] )\n)\ndisplay(joinedDowDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74f9031d-55fe-4b3e-8d60-cec4c3558a89"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now that the data is joined, we can aggregate by any (or all) of the various labels representing the day-of-week.\n\nNotice that we are not losing the numerical **dow** column which we can use to sort.\n\nAnd when we graph this, you can graph by any one of the labels..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22e7b7f6-4123-4069-bc93-8cf04b4d0d76"}}},{"cell_type":"code","source":["aggregatedDowDF = (joinedDowDF\n  .groupBy(col(\"dow\"), col(\"longName\"), col(\"abbreviated\"), col(\"shortName\"))  \n  .sum(\"requests\")                                             \n  .withColumnRenamed(\"sum(requests)\", \"Requests\")\n  .orderBy(col(\"dow\"))\n)\n# Display and then graph...\ndisplay(aggregatedDowDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04d97494-6e36-4c0a-ab7c-4501bc2f78a4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Already Broadcasted\n\nBelieve it or not, that was a broadcast join.\n\nThe proof can be seen by looking at the physical plan.\n\nRun the `explain(..)` below and then look for **BroadcastHashJoin** and/or **BroadcastExchange**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"935756a2-9e78-4988-afbf-731bfdc5b45b"}}},{"cell_type":"code","source":["aggregatedDowDF.explain()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91bed476-80d2-42fc-afc1-8827a0492f4e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["From the code perspective, it looks just like other joins.\n\nSo what's the difference between a regular and a broadcast-join?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"311b9a96-d99b-4523-8260-db4bb3957372"}}},{"cell_type":"markdown","source":["-sandbox\n## Standard Join\n\n* In a standard join, **ALL** the data is shuffled\n* This can be really expensive\n<br/><br/>\n<div style=\"text-align:center\"><img src=\"https://files.training.databricks.com/images/join-standard.png\" style=\"max-height:400px\"/></div>\n<p>Here we see how all the records keyed by \"green\" are moved to the same partition.<br/>The process would be repeated for \"red\" and \"blue\" records.</p>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ccc52a52-355e-48fa-b178-56790bfe802e"}}},{"cell_type":"markdown","source":["-sandbox\n## Broadcast Join\n* In a Broadcast Join, only the \"small\" data is moved.\n* It duplicates the \"small\" data across all executors.\n* But the \"big\" data is left untouched.\n* If the \"small\" data is small enough, this can be **VERY** efficient.\n<br/><br/>\n<div style=\"text-align:center\"><img src=\"https://files.training.databricks.com/images/join-broadcasted.png\" style=\"max-height:400px\"/></div>\n\n<p>Here we see the records keyed by \"red\" being replicated into the first partition.<br/>\n   The process would be repeated for each executor.<br/>\n   The entire process would be repeated again for \"green\" and \"blue\" records.</p>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c1782517-69d3-4bd5-8a40-d3328620eacd"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Broadcasted, How?\n\nBehind the scenes, Spark is analyzing our two `DataFrames`.\n\nIt attempts to estimate if either or both are < 10MB.\n\nWe can see/change this threshold value with the config **spark.sql.autoBroadcastJoinThreshold**. \n\nThe documentation reads as follows:\n> Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 broadcasting can be disabled..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c600c440-44b1-4ed8-ba24-a627ebb89154"}}},{"cell_type":"code","source":["threshold = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\nprint(f\"Threshold: {threshold}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9418b521-5561-43b0-8601-6b7b1a65c3a6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In such a case it will take the small `DataFrame`, the `labelsDF` in our case\n* Send the entire `DataFrame` to every **Executor**\n* Then do a join on the local copy of `labelsDF`\n* Compared to taking our big `DataFrame` `pageviewsWithDowDF` and shuffling it across all executors."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"474db00c-1e2e-4755-9ff2-822d7c9eac68"}}},{"cell_type":"markdown","source":["We can see proof of this by dropping the threshold:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb870335-0ab0-4e39-a819-f6d814abec5e"}}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1a561cf-7e61-45c6-846a-e37fb9524786"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Run the `explain(..)` below and then look for the **ABSENCE OF** the **BroadcastHashJoin** and/or **BroadcastExchange**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"effdc2f0-f025-4454-91bf-707e30e3a56d"}}},{"cell_type":"code","source":["(joinedDowDF\n  .groupBy(col(\"dow\"), col(\"longName\"), col(\"abbreviated\"), col(\"shortName\"))  \n  .sum(\"requests\")                                             \n  .withColumnRenamed(\"sum(requests)\", \"Requests\")\n  .orderBy(col(\"dow\"))\n  .explain()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"09cc987e-52bc-4349-ada4-440d7c883386"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["And now that we are done, let's restore the original threshold:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ddc3b093-3eb5-4d8c-ad81-e1aae54ab518"}}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", threshold)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1972c9c7-2977-482d-ac5e-d79384a8cdbd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) broadcast(..)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48990bf8-ecca-445c-8497-eb826db239c1"}}},{"cell_type":"markdown","source":["What if I wanted to broadcast the data and it was over the 10MB [default] threshold?\n\nWe can specify that a `DataFrame` is to be broadcasted by using the `broadcast(..)` operation from the `...sql.functions` package.\n\nHowever, **it is only a hint**. Spark is allowed to ignore it."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26512aa9-f4aa-4f2b-819f-dd4a57ad5336"}}},{"cell_type":"code","source":["pageviewsWithDowDF.join(   broadcast(labelsDF)   , pageviewsWithDowDF[\"dow\"] == labelsDF[\"dow\"])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0541c11-e944-4a96-a0fd-8c2931de2f56"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34b665f4-67c1-4cb7-90cd-3d6f4eb395dc"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Cleanup\"\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5beee532-d61e-4537-9d78-5a6169d9b016"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12fbde20-5ed7-4da4-81b9-30df6a4c6e97"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Intro To DF Part 5","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3794944577839456}},"nbformat":4,"nbformat_minor":0}
