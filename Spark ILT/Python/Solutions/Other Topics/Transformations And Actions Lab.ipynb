{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"404d32fd-dff4-4ee0-9f2a-da11708e00f3"}}},{"cell_type":"markdown","source":["# Transformations & Actions Lab\n## Exploring T&As in the Spark UI"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32cd7f8d-7cb2-4ef1-9a87-93b4f97ba681"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c356773f-b50b-4f6d-bf3d-e3aa26cd56d4"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62c62664-b61d-4a6e-a6d5-c65f6f59355a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Instructions\n0. Run the cell below.<br/>** *Note:* ** *There is no real rhyme or reason to this code.*<br/>*It simply includes a couple of actions and a handful of narrow and wide transformations.*\n0. Answer each of the questions.\n  * All the answers can be found in the **Spark UI**.\n  * All aspects of the **Spark UI** may or may not have been reviewed with you - that's OK.\n  * The goal is to get familiar with diagnosing applications.\n0. Submit your answers for review.\n\n**WARNING:** Run the following cell only once. Running it multiple times will change some of the answers and make validation a little harder."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c3bc05e-7741-4e90-a798-4cb9fc84e4e5"}}},{"cell_type":"code","source":["initialDF = (spark                                                       \n  .read                                                                     \n  .parquet(\"/mnt/training/wikipedia/pagecounts/staging_parquet_en_only_clean/\")   \n  .cache()\n)\ninitialDF.foreach(lambda x: None) # materialize the cache\n\ndisplayHTML(\"All done<br/><br/>\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80f2a437-5644-46ac-a357-0c10f36eb3bd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Round #1 Questions\n0. How many jobs were triggered?\n0. Open the Spark UI and select the **Jobs** tab.\n  0. What action triggered the first job?\n  0. What action triggered the second job?\n0. Open the details for the second job, how many MB of data was read in? Hint: Look at the **Input** column.\n0. Open the details for the first stage of the second job, how many records were read in? Hint: Look at the **Input Size / Records** column."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"431d23a0-d53c-48ae-a745-0e212a7d3b75"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col, upper\n\nsomeDF = (initialDF\n  .withColumn(\"first\", upper( col(\"article\").substr(0,1)) )\n  .where( col(\"first\").isin(\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\") )\n  .groupBy(\"project\", \"first\").sum()\n  .drop(\"sum(bytes_served)\")\n  .orderBy(\"first\", \"project\")\n  .select( col(\"first\"), col(\"project\"), col(\"sum(requests)\").alias(\"total\"))\n  .filter( col(\"total\") > 10000)\n)\ntotal = someDF.count()\n\ndisplayHTML(\"All done<br/><br/>\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae1df2cb-e4de-42c4-8327-bcb580c07bc9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Round #2 Questions\n0. How many jobs were triggered?\n0. How many actions were executed?\n0. Open the **Spark UI** and select the **Jobs** tab.\n  0. What action triggered the first job?\n  0. What action triggered the second job?\n0. Open the **SQL** tab - what is the relationship between these two jobs?\n0. For the first job...\n  0. How many stages are there?\n  0. Open the **DAG Visualization**. What do you suppose the green dot refers to?\n0. For the second job...\n  0. How many stages are there?\n  0. Open the **DAG Visualization**. Why do you suppose the first stage is grey?\n  0. Can you figure out what transformation is triggering the shuffle at the end of \n    0. The first stage? Hint: If you can't figure it out, look at the SQL tab again.  Exchange means shuffle.  What happened after the shuffle?\n    0. The second stage?\n    0. The third stage? HINT: It's not a transformation but an action.\n0. For the second job, the second stage, how many records (total) \n  0. Were read in as a result of the previous shuffle operation?\n  0. Were written out as a result of this shuffle operation?  \n  Hint: look for the **Aggregated Metrics by Executor**\n0. Open the **Event Timeline** for the second stage of the second job.\n  * Make sure to turn on all metrics under **Show Additional Metrics**.\n  * Note that there were 200 tasks executed.\n  * Visually compare the **Scheduler Delay** to the **Executor Computing Time**\n  * Then in the **Summary Metrics**, compare the median **Scheduler Delay** to the median **Duration** (aka **Executor Computing Time**)\n  * What is taking longer? scheduling, execution, task deserialization, garbage collection, result serialization or getting the result?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2aea0a81-c030-4e80-9614-3754f0e84824"}}},{"cell_type":"code","source":["someDF.take(total)\n\ndisplayHTML(\"All done<br/><br/>\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e48da41-2834-4c36-939c-4c20bf405253"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Round #3 Questions\n0. Collectively, `someDF.count()` produced 2 jobs and 6 stages.  \nHowever, `someDF.take(total)` produced only 1 job and 2 stages.  \n  0. Why did it only produce 1 job?\n  0. Why did the last job only produce 2 stages?\n0. Look at the **Storage** tab. How many partitions were cached?\n0. True or False: The cached data is fairly evenly distributed.\n0. How many MB of data is being used by our cache?\n0. How many total MB of data is available for caching?\n0. Go to the **Executors** tab. How many executors do you have?\n0. Go to the **Executors** tab. How many total cores do you have available?\n0. Go to the **Executors** tab. What is the IP Address of your first executor?\n0. How many tasks is your cluster able to execute simultaneously?\n0. What is the path to your **Java Home**?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ffb0d161-8575-43ba-a8b1-a8f8c931566a"}}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2bb30e4e-c065-4d64-9313-359833e8eae2"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Cleanup\"\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4d556325-888d-42b4-b3bc-ad3b7f95169b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d413bc2-fa84-4690-bf79-883b8744a2aa"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Transformations And Actions Lab","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3794944577840047}},"nbformat":4,"nbformat_minor":0}
