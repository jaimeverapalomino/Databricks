{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2a114a43-f6ce-45f4-b3f8-57a5f6ecba5d"}}},{"cell_type":"markdown","source":["# Intro To DataFrames, Lab #4\n## What-The-Monday?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5fa560d0-f556-406f-a366-9cc718b0d06c"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Instructions\n\nAs we saw in the previous notebook...\n* There are a lot more requests for sites on Monday than on any other day of the week.\n* The variance is **NOT** unique to the mobile or desktop site.\n\nYour mission, should you choose to accept it, is to demonstrate conclusively why there are more requests on Monday than on any other day of the week.\n\nFeel free to copy & paste from the previous notebook."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e8b033d-2ef5-47ba-8fcd-219b7f494800"}}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup<br>\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the start of each lesson (see the next cell) and the **`Classroom-Cleanup`** cell at the end of each lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a7d3160-0c3a-42f7-b00c-ebff631911a5"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e2a3269-72d2-4d8b-ba5d-dc53f1756495"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["(source, sasEntity, sasToken) = getAzureDataSource()\nspark.conf.set(sasEntity, sasToken)\n\nfileName = source + \"/wikipedia/pageviews/pageviews_by_second.parquet\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f155292e-d819-4f41-b5b0-eecfab12f46a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# ANSWER\n\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n# I've already gone through the exercise to determine\n# how many partitions I want and in this case it is...\npartitions = 8\n\n# Make sure wide operations don't repartition to 200\nspark.conf.set(\"spark.sql.shuffle.partitions\", str(partitions))\n\n# The directory containing our parquet files.\nparquetFile = \"/mnt/training/wikipedia/pageviews/pageviews_by_second.parquet/\"\n\n# Create our initial DataFrame. We can let it infer the \n# schema because the cost for parquet files is really low.\npageviewsDF = (spark.read\n  .option(\"inferSchema\", \"true\")                # The default, but not costly w/Parquet\n  .parquet(parquetFile)                         # Read the data in\n  .repartition(partitions)                      # From 7 >>> 8 partitions\n  .withColumnRenamed(\"timestamp\", \"capturedAt\") # rename and convert to timestamp datatype\n  .withColumn(\"capturedAt\", unix_timestamp( col(\"capturedAt\"), \"yyyy-MM-dd'T'HH:mm:ss\").cast(\"timestamp\") )\n  .orderBy( col(\"capturedAt\"), col(\"site\") )    # sort our records\n  .cache()                                      # Cache the expensive operation\n)\n# materialize the cache\npageviewsDF.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37c25021-91aa-45d1-be65-e3c622fd91fb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# ANSWER\n\nrecordCount = pageviewsDF.count()\ndistinctCount = pageviewsDF.distinct().count()\n\nprint(f\"The DataFrame contains {recordCount} records. Number of distinct records: {distinctCount}\\n{recordCount - distinctCount} records are duplicated.\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8dc38ead-76c7-4c96-8202-438ba264b1dc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import col\n\n# Let's see the duplicates\ndisplay(pageviewsDF.groupBy(col(\"capturedAt\"), col(\"site\")).count().orderBy(col(\"count\").desc(), col(\"capturedAt\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74567095-2dec-4b28-9393-6270c181cf52"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# ANSWER\n\nanswerDF = (pageviewsDF\n  .withColumn(\"day_of_year\", dayofyear(col(\"capturedAt\")))\n  .filter(\"day_of_year = 110\")\n  .orderBy(\"capturedAt\", \"site\")\n)\ndisplay(answerDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"baebf0cb-5841-4d75-a7b7-823d8c13f8b4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfdc9de6-9533-415a-9336-fd8b256385ee"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Cleanup\"\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10ead033-8bf7-4c04-9f00-0f1e9e8ebbb3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6cc61142-0f07-4cc1-acf5-3e9262440769"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Intro To DF Part 4 Lab","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3794944577839883}},"nbformat":4,"nbformat_minor":0}
