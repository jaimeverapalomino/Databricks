{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be0b4b7f-5ebd-4abd-b482-6dc0e898c8de"}}},{"cell_type":"markdown","source":["# Catalyst Optimizer\n\n**Technical Accomplishments:**\n* Understand what the Catalyst Optimizer is\n* Understand the different stages of the Catalyst Optimizer\n* Example of Physical Plan Optimization (x2)\n* Example of Predicate Pushdown"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b98ca32e-64e8-47ef-a56c-0146c2cbecf8"}}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup<br>\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the start of each lesson (see the next cell) and the **`Classroom-Cleanup`** cell at the end of each lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"795d3e25-453c-4bff-8cd2-edaaabe7a14b"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f129f18b-864a-4234-9c00-6ce9bdd5a536"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Because we will need it later...\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a60fe2e-99b4-4dee-b328-65ce28ebc4be"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Catalyst Optimizer\n\n* Fundamental to the `SQL` and `DataFrames` API is the Catalyst Optimizer.\n* It's an **extensible query optimizer**.\n* Contains a **general library for representing trees and applying rules** to manipulate them.\n* Several public extension points, including external data sources and user-defined types.\n\nSee also: <a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\" target=\"_blank\">Deep Dive into Spark SQLâ€™s Catalyst Optimizer</a> (April 13, 2015)\n\nProcessing is broken down into several stages as we can see here:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f956ef3-b565-45fb-8e83-933d837280e1"}}},{"cell_type":"markdown","source":["![Catalyst](https://files.training.databricks.com/images/105/catalyst-diagram.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49f251b9-84d8-4516-bb5f-94340db13ae3"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Optimized Logical Plan\n\nOne of the many optimizations performed by the Catalyst Optimizer involves **rewriting our code**.\n  \nIn this case, we will see **two examples** involving the rewriting of our filters.\n\nThe first is an **innocent mistake** almost most every new Spark developer makes.\n\nThe second \"mistake\" is... well... **really bad** - but Spark can fix it."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48aa2c5b-cf30-4fd8-a742-2649d80c8749"}}},{"cell_type":"markdown","source":["### Example #1: Innocent Mistake\n\nI don't want any project that starts with **en.zero**.\n\nThere are **better ways of doing this**, as in it can be done with a single condition.\n\nBut we will make **8 passes** on the data **with 8 different filters**.\n\nAfter every individual pass, we will **go back over the remaining dataset** to filter out the next set of records."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec927061-8cc4-41fc-90f6-dfe80b37e2fa"}}},{"cell_type":"code","source":["allDF = spark.read.parquet(\"/mnt/training/wikipedia/pagecounts/staging_parquet_en_only_clean/\")\n\npass1 = allDF.filter( col(\"project\") != \"en.zero\")\npass2 = pass1.filter( col(\"project\") != \"en.zero.n\")\npass3 = pass2.filter( col(\"project\") != \"en.zero.s\")\npass4 = pass3.filter( col(\"project\") != \"en.zero.d\")\npass5 = pass4.filter( col(\"project\") != \"en.zero.voy\")\npass6 = pass5.filter( col(\"project\") != \"en.zero.b\")\npass7 = pass6.filter( col(\"project\") != \"en.zero.v\")\npass8 = pass7.filter( col(\"project\") != \"en.zero.q\")\n\nprint(\"Pass 1: {0:,}\".format( pass1.count() ))\nprint(\"Pass 2: {0:,}\".format( pass2.count() ))\nprint(\"Pass 3: {0:,}\".format( pass3.count() ))\nprint(\"Pass 4: {0:,}\".format( pass4.count() ))\nprint(\"Pass 5: {0:,}\".format( pass5.count() ))\nprint(\"Pass 6: {0:,}\".format( pass6.count() ))\nprint(\"Pass 7: {0:,}\".format( pass7.count() ))\nprint(\"Pass 8: {0:,}\".format( pass8.count() ))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4cf9f6ed-d29c-442f-ac06-8a12a6a6e601"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Logically**, the code above is the same as the code below.\n\nThe only real difference is that we are **not asking for a count** after every filter."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"185b746b-170c-462e-a7c7-a04bc657d881"}}},{"cell_type":"code","source":["innocentDF = (spark.read.parquet(\"/mnt/training/wikipedia/pagecounts/staging_parquet_en_only_clean/\")\n  .filter( col(\"project\") != \"en.zero\")\n  .filter( col(\"project\") != \"en.zero.n\")\n  .filter( col(\"project\") != \"en.zero.s\")\n  .filter( col(\"project\") != \"en.zero.d\")\n  .filter( col(\"project\") != \"en.zero.voy\")\n  .filter( col(\"project\") != \"en.zero.b\")\n  .filter( col(\"project\") != \"en.zero.v\")\n  .filter( col(\"project\") != \"en.zero.q\")\n)\nprint(\"Final Count: {0:,}\".format( innocentDF.count() ))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"970af6ee-ce86-46bd-9e01-14be47ddb8a0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We don't even have to execute the code to see what is **logically** or **physically** taking place under the hood.\n\nHere we can use the `explain(..)` command."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2dbab326-45ed-4397-ba51-8fd07d06e6b5"}}},{"cell_type":"code","source":["innocentDF.explain(True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a511839e-2357-4404-a91e-1e72a7af687a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Of course, if we were to write this the correct way, the first time, ignoring the fact that there are better methods, it would look something like this..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3c4bf66-4d61-439e-b47b-b4b7b947609d"}}},{"cell_type":"code","source":["betterDF = (spark.read.parquet(\"/mnt/training/wikipedia/pagecounts/staging_parquet_en_only_clean/\")\n  .filter( (col(\"project\").isNotNull()) &\n           (col(\"project\") != \"en.zero\") & \n           (col(\"project\") != \"en.zero.n\") & \n           (col(\"project\") != \"en.zero.s\") & \n           (col(\"project\") != \"en.zero.d\") & \n           (col(\"project\") != \"en.zero.voy\") & \n           (col(\"project\") != \"en.zero.b\") & \n           (col(\"project\") != \"en.zero.v\") & \n           (col(\"project\") != \"en.zero.q\")\n        )\n)\n\nprint(\"Final: {0:,}\".format( betterDF.count() ))\n\nbetterDF.explain(True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ac62af0-08af-4f0a-b4b8-9230f997b117"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Example #2: Bad Programmer\n\nThis time we are going to do something **REALLY** bad...\n\nEven if the compiler combines these filters into a single filter, **we still have five different tests** for any column that doesn't have the value \"whatever\"."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fcca1162-55c0-4954-921a-57be60a53304"}}},{"cell_type":"code","source":["stupidDF = (spark.read.parquet(\"/mnt/training/wikipedia/pagecounts/staging_parquet_en_only_clean/\")\n  .filter( col(\"project\") != \"whatever\")\n  .filter( col(\"project\") != \"whatever\")\n  .filter( col(\"project\") != \"whatever\")\n  .filter( col(\"project\") != \"whatever\")\n  .filter( col(\"project\") != \"whatever\")\n)\n\nstupidDF.explain(True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10f8457e-e879-41f3-8a97-ac3a04417878"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["** *Note:* ** *`explain(..)` is not the only way to get access to this level of detail...<br/>\nWe can also see it in the **Spark UI**. *"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a49e107-cc84-4976-8bae-88431a36efc5"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Columnar Predicate Pushdown\n\nThe Columnar Predicate Pushdown takes place when a filter can be pushed down to the original data source, such as a database server.\n\nIn this example, we are going to compare `DataFrames` from two different sources:\n* JDBC - where a predicate pushdown **WILL** take place.\n* CSV - where a predicate pushdown will **NOT** take place.\n\nIn each case, we can see evidence of the pushdown (or lack of it) in the **Physical Plan**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f30bee6d-f583-400f-8e4a-12faca4c5184"}}},{"cell_type":"markdown","source":["### Example #3: JDBC\n\nStart by initializing the JDBC driver.\n\nThis needs to be done regardless of language."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55f12512-6d48-4f26-829e-8da21e266cc1"}}},{"cell_type":"code","source":["%scala\n\n// Ensure that the driver class is loaded. \n// Seems to be necessary sometimes.\nClass.forName(\"org.postgresql.Driver\") "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce296116-0913-4954-8c38-848e4f42a955"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Next, we can create a `DataFrame` via JDBC and then filter by **gender**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e39d7920-809a-4209-aeb8-a60f8c3525a5"}}},{"cell_type":"code","source":["jdbcURL = \"jdbc:postgresql://54.213.33.240/training\"\n\n# Username and Password w/read-only rights\nconnProperties = {\n  \"user\" : \"training\",\n  \"password\" : \"training\"\n}\n\nppExampleThreeDF = (spark.read.jdbc(\n    url=jdbcURL,                  # the JDBC URL\n    table=\"training.people_1m\",   # the name of the table\n    column=\"id\",                  # the name of a column of an integral type that will be used for partitioning\n    lowerBound=1,                 # the minimum value of columnName used to decide partition stride\n    upperBound=1000000,           # the maximum value of columnName used to decide partition stride\n    numPartitions=8,              # the number of partitions/connections\n    properties=connProperties     # the connection properties\n  )\n  .filter(col(\"gender\") == \"M\")   # Filter the data by gender\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca49acf7-57fb-4049-a559-5c3d685d1452"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["With the `DataFrame` created, we can ask Spark to `explain(..)` the **Physical Plan**.\n\nWhat we are looking for...\n* is the lack of a **Filter** and\n* the presence of a **PushedFilters** in the **Scan**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"843aeebb-a71e-4043-a781-7d56b3ac7e74"}}},{"cell_type":"code","source":["ppExampleThreeDF.explain()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0754366d-df50-4d2a-9218-df2cfff305d4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["This will make a little more sense if we **compare it to some examples** that don't push down the filter."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f3902c2-2bb9-4911-a4a3-0f85e7ae0516"}}},{"cell_type":"markdown","source":["### Example #4: Cached JDBC\n\nIn this example, we are going to cache our data before filtering and thus eliminating the possibility for the predicate push down:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f958f099-ad4a-45a5-83be-51aab3384d6c"}}},{"cell_type":"code","source":["ppExampleFourCachedDF = (spark.read.jdbc(\n    url=jdbcURL,                  # the JDBC URL\n    table=\"training.people_1m\",   # the name of the table\n    column=\"id\",                  # the name of a column of an integral type that will be used for partitioning\n    lowerBound=1,                 # the minimum value of columnName used to decide partition stride\n    upperBound=1000000,           # the maximum value of columnName used to decide partition stride\n    numPartitions=8,              # the number of partitions/connections\n    properties=connProperties     # the connection properties\n  ))\n\n(ppExampleFourCachedDF\n  .cache()                        # cache the data\n  .count())                       # materialize the cache\n\nppExampleFourFilteredDF = (ppExampleFourCachedDF\n  .filter(col(\"gender\") == \"M\"))  # Filter the data by gender"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9666a8f-b039-4e10-8bc2-3e619d9fa257"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now that we have cached the data and THEN filtered it, we have eliminated the possibility to benefit from the predicate push down.\n\nAnd so that it's easier to compare the two examples, we can re-print the physical plan for the previous example too."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a0062bf5-aa53-41f4-972b-9997aa9954e1"}}},{"cell_type":"code","source":["print(\"****Example Three****\\n\")\nppExampleThreeDF.explain()\n\nprint(\"\\n****Example Four****\\n\")\nppExampleFourFilteredDF.explain()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f14b148-ea8a-4140-945c-9521562584bf"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["It should be clearer now...\n\nIn the first example we see only the **Scan** which is the JDBC read.\n\nIn the second example, you can see the **Scan** but you also see the **InMemoryTableScan** followed by a **Filter** which means Spark had to filter ALL the data from RAM instead of in the Database."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6527001-2cbe-4407-8d47-e88388eff6bd"}}},{"cell_type":"markdown","source":["### Example #5: CSV File\n\nThis example is identical to the previous one except...\n* this is a CSV file instead of JDBC source\n* we are filtering on **site**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cbf8d5d1-1bb1-4420-a654-fc7732de3943"}}},{"cell_type":"code","source":["schema = StructType(\n  [\n    StructField(\"timestamp\", StringType(), False),\n    StructField(\"site\", StringType(), False),\n    StructField(\"requests\", IntegerType(), False)\n  ]\n)\n\nppExampleThreeDF = (spark.read\n   .option(\"header\", \"true\")\n   .option(\"sep\", \"\\t\")\n   .schema(schema)\n   .csv(\"/mnt/training/wikipedia/pageviews/pageviews_by_second.tsv\")\n   .filter(col(\"site\") == \"desktop\")\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c91227b4-ae27-4791-ab4c-a3403fb1bc21"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["With the `DataFrame` created, we can ask Spark to `explain(..)` the **Physical Plan**.\n\nWhat we are looking for...\n* is the presence of a **Filter** and\n* the presence of a **PushedFilters** in the **FileScan csv**\n\nAnd again, we see **PushedFilters** because Spark is *trying* to push down to the CSV file.\n\nBut that doesn't work here and so we see that just like in the last example, we have a **Filter** after the **FileScan**, actually an **InMemoryFileIndex**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92266134-b5b8-4e68-b703-df813c16f7ed"}}},{"cell_type":"code","source":["ppExampleThreeDF.explain()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4c6f9d9-1a91-44c2-b09b-c2e715f9a02a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89f54d53-b59c-4f87-8f40-03eaf4ef4cb1"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Cleanup\"\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe1acb06-c369-41df-b25e-9b179d3d8e3b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7ae36ee-f8c3-4f73-80c1-79bda427fcd5"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Catalyst Optimizer","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3794944577840210}},"nbformat":4,"nbformat_minor":0}
