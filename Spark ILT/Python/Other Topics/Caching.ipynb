{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0489923-9647-47d8-9903-7b951df3821f"}}},{"cell_type":"markdown","source":["# Caching\n\n**Technical Accomplishments:**\n* Understand how caching works\n* Explore the different caching mechanisms\n* Discuss tips for the best use of the cache"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"418dd627-8b4b-4395-8a97-0bc1413c9a11"}}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup<br>\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the start of each lesson (see the next cell) and the **`Classroom-Cleanup`** cell at the end of each lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3b8ca6c-d2b0-4b46-8978-fca621eb4fd2"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cc6f011-60ca-4f83-bb8f-572e407a19a5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) A Fresh Start\nFor this section, we need to clear the existing cache.\n\nThere are several ways to accomplish this:\n  * Remove each cache one-by-one, fairly problematic\n  * Restart the cluster - takes a fair while to come back online\n  * Just blow the entire cache away - this will affect every user on the cluster!!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f739528-f799-44d7-a1c4-80df817c168b"}}},{"cell_type":"code","source":["#!!! DO NOT RUN THIS ON A SHARED CLUSTER !!!\n#YOU WILL CLEAR YOUR CACHE AND YOUR COWORKER'S\n\n# spark.catalog.clearCache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e32f641-3946-47f0-ab3e-ba06cf3aa92e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["This will ensure that any caches produced by other labs/notebooks will be removed.\n\nNext, open the **Spark UI** and go to the **Storage** tab - it should be empty."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1a5c590-807a-44ab-8af1-28a8d1a3f16e"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) The Data Source\n\nThis data uses the **Pageviews By Seconds** data set.\n\nThe files are located on DBFS at **dbfs:/mnt/training/wikipedia/pageviews/pageviews_by_second.tsv**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a759ff79-98fb-444b-821c-32eaef6d05a9"}}},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\nschema = StructType(\n  [\n    StructField(\"timestamp\", StringType(), False),\n    StructField(\"site\", StringType(), False),\n    StructField(\"requests\", IntegerType(), False)\n  ]\n)\n\nfileName = \"dbfs:/mnt/training/wikipedia/pageviews/pageviews_by_second.tsv\"\n\npageviewsDF = (spark.read\n  .option(\"header\", \"true\")\n  .option(\"sep\", \"\\t\")\n  .schema(schema)\n  .csv(fileName)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e335980b-b5c9-40ba-bb58-b345b7825bd4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The 255 MB pageviews file is currently in our object store, which means each time you scan through it, your Spark cluster has to read the 255 MB of data remotely over the network."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54e18015-2c6e-4d59-b0e1-f9496dcbb1a0"}}},{"cell_type":"markdown","source":["Once again, use the **`count()`** action to scan the entire 255 MB file from disk and count how many total records (rows) there are:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6c07ee4-b7ca-4520-8204-3ed369713f68"}}},{"cell_type":"code","source":["total = pageviewsDF.count()\n\nprint(\"Record Count: {0:,}\".format( total ))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12a5902b-dba2-4d54-b817-966523ccbe84"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The pageviews DataFrame contains 7.2 million rows.\n\nMake a note of how long the previous operation takes.\n\nRe-run it several times trying to establish an average.\n\nLet's try a slightly more complicated operation, such as sorting, which induces an \"expensive\" shuffle."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a0fc573-83ca-4614-85f9-82c8d399a4f1"}}},{"cell_type":"code","source":["(pageviewsDF\n  .orderBy(\"requests\")\n  .count()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b12037f-2732-4cc3-9e83-e903f9bce858"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Again, make note of how long the operation takes.\n\nRerun it several times to get an average."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cfb13fe2-99e1-4fd8-b0a7-74cae6a81458"}}},{"cell_type":"markdown","source":["Every time we re-run these operations, it goes all the way back to the original data store.\n\nThis requires pulling all the data across the network for every execution.\n\nIn many/most cases, this network IO is the most expensive part of a job."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f5c4b44-b1aa-4171-87b6-22cbe907f2dc"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) cache()\n\nWe can avoid all of this overhead by caching the data on the executors.\n\nGo ahead and run the following command.\n\nMake note of how long it takes to execute."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a00807a8-10da-4277-94d4-7bab42912214"}}},{"cell_type":"code","source":["pageviewsDF.cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a087a53-9a15-47ce-91e5-c044799a0d9b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The **`cache(..)`** operation doesn't do anything other than mark a **`DataFrame`** as cacheable.\n\nAnd while it does return an instance of **`DataFrame`** it is not technically a transformation or action\n\nIn order to actually cache the data, Spark has to process over every single record.\n\nAs Spark processes every record, the cache will be materialized.\n\nA very common method for materializing the cache is to execute a **`count()`**.\n\n**BUT BEFORE YOU DO** Check the **Spark UI** to make sure it's still empty even after calling **`cache()`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a806daf-6a67-4a8a-af82-edd4e598af99"}}},{"cell_type":"code","source":["pageviewsDF.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49ecf2a8-e8a6-46c8-ad90-7274ec56077b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The last **`count()`** will take a little longer than normal.\n\nIt has to perform the cache and do the work of materializing the cache.\n\nNow the **`pageviewsDF`** is cached **AND** the cache has been materialized.\n\nBefore we rerun our queries, check the **Spark UI** and the **Storage** tab.\n\nNow, run the two queries and compare their execution time to the ones above."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d527b317-c99a-4d70-8e6a-1d084d26c055"}}},{"cell_type":"code","source":["pageviewsDF.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3a6745f-2fbb-41aa-996a-00bff14aeade"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["(pageviewsDF\n  .orderBy(\"requests\")\n  .count()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"929422d5-1002-4632-a3c4-724b2f517d29"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Faster, right?\n\nAll of our data is being stored in RAM on the executors.\n\nWe are no longer making network calls.\n\nOur plain **`count()`** should be sub-second.\n\nOur **`orderBy(..)`** & **`count()`** should be around 3 seconds."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2298239f-a0d8-4aa0-b7a5-debbfe29aa4f"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Spark UI - Storage"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a576b27c-4a5c-4f15-8c76-8d4f2493ddf1"}}},{"cell_type":"markdown","source":["Now that the pageviews **`DataFrame`** is cached in memory let's go review the **Spark UI** in more detail.\n\nIn the **RDDs** table, you should see only one record - multiple if you reran the **`cache()`** operation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5097b849-5de0-466f-9977-d8fb5b7ecb0a"}}},{"cell_type":"markdown","source":["Let's review the **Spark UI**'s **Storage** details\n* RDD Name\n* Storage Level\n* Cached Partitions\n* Fraction Cached\n* Size in Memory\n* Size on Disk"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae8c6df0-8a34-4629-a23c-169871a9ae48"}}},{"cell_type":"markdown","source":["Next, let's dig deeper into the storage details...\n\nClick on the link in the **RDD Name** column to open the **RDD Storage Info**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff7d2156-a687-4fc7-a1c8-353ae036497c"}}},{"cell_type":"markdown","source":["Let's review the **RDD Storage Info**\n* Size in Memory\n* Size on Disk\n* Executors\n\nIf you recall...\n* We should have 8 partitions.\n* With 255MB of data divided into 8 partitions...\n* The first seven partitions should be 32MB each.\n* The last partition will be significantly smaller than the others.\n\n**Question:** Why is the **Size in Memory** nowhere near 32MB?\n\n**Question:** What is the difference between **Size in Memory** and **Size on Disk**?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1217567b-a140-4806-9edb-af8c7beb2260"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) persist()\n\n`cache()` is just an alias for **`persist()`**\n\nLet's take a look at the API docs for\n* **`Dataset.persist(..)`** if using Scala\n* **`DataFrame.persist(..)`** if using Python\n\n`persist()` allows one to specify an additional parameter (storage level) indicating how the data is cached:\n* DISK_ONLY\n* DISK_ONLY_2\n* MEMORY_AND_DISK\n* MEMORY_AND_DISK_2\n* MEMORY_AND_DISK_SER\n* MEMORY_AND_DISK_SER_2\n* MEMORY_ONLY\n* MEMORY_ONLY_2\n* MEMORY_ONLY_SER\n* MEMORY_ONLY_SER_2\n* OFF_HEAP\n\n** *Note:* ** *The default storage level for...*\n* *RDDs are **MEMORY_ONLY**.*\n* *DataFrames are **MEMORY_AND_DISK**.* \n* *Streaming is **MEMORY_AND_DISK_2**.*"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62df9e11-7e12-4e09-8c2e-f0ca7c660813"}}},{"cell_type":"markdown","source":["Before we can use the various storage levels, it's necessary to import the enumerations..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58352745-1e0f-46a8-84c6-cc4d2ad8c702"}}},{"cell_type":"code","source":["from pyspark import StorageLevel"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bcfe1854-e95b-4348-883e-4747830b95a5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Question:** How do we purge data from the cache?\n\n**`unpersist(..)`** or **`uncache()`**?\n\nTry it..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c3d31cf3-c61c-49d7-ba99-ce6eca1300c9"}}},{"cell_type":"code","source":["# pageviewsDF.uncache()\n# pageviewsDF.unpersist()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"23673d22-1c2a-4e74-933d-a5721737d681"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Real quick, go check the **Storage** tab in the **Spark UI** and confirm that the cache has been expunged."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef8f34db-66c6-4ac9-85e9-c91a9e3536f1"}}},{"cell_type":"markdown","source":["**Question:** What will happen if you take 75% of the cache and then I come along and try to use %50 (of the total)...\n* with **MEMORY_ONLY**?\n* with **MEMORY_AND_DISK**?\n* with **DISK_ONLY**?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca884e2b-b64f-4c56-8d23-71849b49f3e6"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) RDD Name\n\nIf you haven't noticed yet, the **RDD Name** on the **Storage** tab in the **Spark UI** is a big ugly name.\n\nIt's a bit hacky, but there is a workaround for assigning a name.\n0. Create your **`DataFrame`**.\n0. From that **`DataFrame`**, create a temporary view with your desired name.\n0. Specifically, cache the table via the **`SparkSession`** and its **`Catalog`**.\n0. Materialize the cache."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a462b358-3b55-44e3-a391-5b450ac326bb"}}},{"cell_type":"code","source":["pageviewsDF.unpersist()\n\npageviewsDF.createOrReplaceTempView(\"Pageviews_DF_Python\")\nspark.catalog.cacheTable(\"Pageviews_DF_Python\")\n\npageviewsDF.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6c23511-a714-413f-adee-32e577ebdca4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["And now to clean up after ourselves..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b3921a4-d959-46e4-87b1-7225cc155b3e"}}},{"cell_type":"code","source":["pageviewsDF.unpersist()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ea04fc20-1a58-4346-b82f-4674c7039d88"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1e97a0b-09bd-48a8-86b5-055c499833cd"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Cleanup\"\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9c58799-543c-400b-8edb-d0e3d01f2e73"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"118a5ab4-905f-42f6-a884-d5340c9c84cd"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Caching","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3794944577840131}},"nbformat":4,"nbformat_minor":0}
