{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd6be42b-d1b7-4fc5-9f2e-708e17eca573"}}},{"cell_type":"markdown","source":["#Partitioning\n\n** Data Source **\n* English Wikipedia pageviews by second\n* Size on Disk: ~255 MB\n* Type: Tab Separated Text File & Parquet files\n* More Info: <a href=\"https://old.datahub.io/dataset/english-wikipedia-pageviews-by-second\" target=\"_blank\">https&#58;old.datahub.io/dataset/english-wikipedia-pageviews-by-second</a>\n\n**Technical Accomplishments:**\n* Understand the relationship between partitions and slots/cores\n* Review `repartition(n)` and `coalesce(n)`\n* Review one key side effect of shuffle partitions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90501f5d-e20e-4997-b5ba-1b5ad1aef81c"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3713453b-2262-4cff-bb2d-80b930294f04"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9dd6adfe-39a4-44e6-a03e-0d306c44e288"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) **The Data Source**\n\nThis data uses the **Pageviews By Seconds** data set.\n\nThe file is located on the DBFS at **dbfs:/mnt/training/wikipedia/pageviews/pageviews_by_second.tsv**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"09605f4d-f978-4fbf-92e9-2c28b4dc2876"}}},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\n# Use a schema to avoid the overhead of inferring the schema\n# In the case of CSV/TSV it requires a full scan of the file.\nschema = StructType(\n  [\n    StructField(\"timestamp\", StringType(), False),\n    StructField(\"site\", StringType(), False),\n    StructField(\"requests\", IntegerType(), False)\n  ]\n)\n\nfileName = \"/mnt/training/wikipedia/pageviews/pageviews_by_second.tsv\"\n\n# Create our initial DataFrame\ninitialDF = (spark.read\n  .option(\"header\", \"true\")\n  .option(\"sep\", \"\\t\")\n  .schema(schema)\n  .csv(fileName)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94d92a26-85c9-48b5-aae8-fc4d61b74f8b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We can see below that our data consists of...\n* when the record was created\n* the site (mobile or desktop) \n* and the number of requests\n\nThis amounts to one record per site, per second, and captures the number of requests made in that one second. \n\nThat means for every second of the day, there are two records."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34e4758c-56b8-476a-85ad-2ecec7bb6ccf"}}},{"cell_type":"code","source":["display(initialDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6842f6fb-e0b6-4fc1-9454-783175bc06b9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) My First Steps\n\nBefore processing any data, there are normally several steps to simply prepare the data for analysis such as\n0. <div style=\"text-decoration:line-through\">Read the data in</div>\n0. Balance the number of partitions to the number of slots\n0. Cache the data\n0. Adjust the `spark.sql.shuffle.partitions`\n0. Perform some basic ETL (ie convert strings to timestamp)\n0. Possibly re-cache the data if the ETL was costly"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ab82dd8-513f-46f9-9288-f481524af699"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Partitions vs Slots\n\n* We have our `initialDF` (**Step #1**) which amounts to nothing more than reading in the data.\n* For **Step #2** we have to ask the question, what is the relationship between partitions and slots.\n\n\n** *Note:* ** *The Spark API uses the term **core** meaning a thread available for parallel execution.*<br/>*Here we refer to it as **slot** to avoid confusion with the number of cores in the underlying CPU(s)*<br/>*to which there isn't necessarily an equal number.*"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3484cc5a-f699-4004-89ad-13d60e419137"}}},{"cell_type":"markdown","source":["### Slots/Cores\n\nIn most cases, if you created your cluster, you should know how many cores you have.\n\nHowever, to check programmatically, you can use `SparkContext.defaultParallelism`\n\nFor more information, see the doc <a href=\"https://spark.apache.org/docs/latest/configuration.html#execution-behavior\" target=\"_blank\">Spark Configuration, Execution Behavior</a>\n> For operations like parallelize with no parent RDDs, it depends on the cluster manager:\n> * Local mode: number of cores on the local machine\n> * Mesos fine grained mode: 8\n> * **Others: total number of cores on all executor nodes or 2, whichever is larger**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9f4bf45-234d-4a1d-8f90-f8ad92e0e272"}}},{"cell_type":"code","source":["cores = sc.defaultParallelism\n\nprint(\"You have {} cores, or slots.\".format(cores))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8988ce49-168a-4986-8e0f-8ce89c3b5c60"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Partitions\n\n* The second 1/2 of this question is how many partitions of data do I have?\n* With that we have two subsequent questions:\n  0. Why do I have that many?\n  0. What is a partition?\n\nFor the last question, a **partition** is a small piece of the total data set.\n\nGoogle defines it like this:\n> the action or state of dividing or being divided into parts.\n\nIf our goal is to process all our data (say 1M records) in parallel, we need to divide that data up.\n\nIf I have 8 **slots** for parallel execution, it would stand to reason that I want 1M / 8 or 125,000 records per partition."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ea1cf27-dd20-4966-a49c-0236d94fbc8e"}}},{"cell_type":"markdown","source":["Back to the first question, we can answer it by running the following command which\n* takes the `initialDF`\n* converts it to an `RDD`\n* and then asks the `RDD` for the number of partitions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67fae3b8-e119-475b-88d3-f55775a44f1e"}}},{"cell_type":"code","source":["partitions = initialDF.rdd.getNumPartitions()\nprint(\"Partitions: {0:,}\".format( partitions ))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0b9f41d-eaf6-4ace-9cad-a57a5e95f5f3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["* It is **NOT** coincidental that we have **8 slots** and **8 partitions**\n* In Spark 2.0 a lot of optimizations have been added to the readers.\n* Namely the readers looks at **the number of slots**, the **size of the data**, and makes a best guess at how many partitions **should be created**.\n* You can actually double the size of the data several times over and Spark will still read in **only 8 partitions**.\n* Eventually it will get so big that Spark will forgo optimization and read it in as 10 partitions, in that case.\n\nBut 8 partitions and 8 slots is just too easy.\n  * Let's read in another copy of this same data.\n  * A parquet file that was saved in 5 partitions.\n  * This gives us an excuse to reason about the **relationship between slots and partitions**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cacc4145-07e2-4d02-ba56-cca4961b5437"}}},{"cell_type":"code","source":["# Create our initial DataFrame. We can let it infer the \n# schema because the cost for parquet files is really low.\nalternateDF = (spark.read\n  .parquet(\"/mnt/training/wikipedia/pageviews/pageviews_by_second.parquet\")\n)\n\nprint(\"Partitions: {0:,}\".format( alternateDF.rdd.getNumPartitions() ))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e3790bd-6985-464e-a515-5c259edc1afb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now that we have only 5 partitions we have to ask...\n\nWhat is going to happen when I perform and action like `count()` **with 8 slots and only 5 partitions?**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19acac58-9eb9-4b9c-9a4d-9c42cfbcd672"}}},{"cell_type":"code","source":["alternateDF.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38c26fbd-959a-4433-b267-2161a0c75d29"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Question #1:** Is it OK to let my code continue to run this way?\n\n**Question #2:** What if it was a **REALLY** big file that read in as **200 partitions** and we had **256 slots**?\n\n**Question #3:** What if it was a **REALLY** big file that read in as **200 partitions** and we had only **8 slots**, how long would it take compared to a dataset that has only 8 partitions?\n\n**Question #4:** Given the previous example (**200 partitions** vs **8 slots**) what are our options (given that we cannot increase the number of partitions)?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"985baeea-2386-43d4-b4e2-7d535018278b"}}},{"cell_type":"markdown","source":["### Use Every Slot/Core\n\nWith some very few exceptions, you always want the number of partitions to be **a factor of the number of slots**.\n\nThat way **every slot is used**.\n\nThat is, every slots is being assigned a task.\n\nWith 5 partitions & 8 slots we are **under-utilizing three of the eight slots**.\n\nWith 9 partitions & 8 slots we just guaranteed our **job will take 2x** as long as it may need to.\n* 10 seconds, for example, to process the first 8.\n* Then as soon as one of the first 8 is done, another 10 seconds to process the last partition."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b41a42a8-8a96-4056-a463-2e6e9a59b184"}}},{"cell_type":"markdown","source":["### More or Less Partitions?\n\nAs a **general guideline** it is advised that each partition (when cached) is roughly around 200MB.\n* Size on disk is not a good gauge. For example...\n* CSV files are large on disk but small in RAM - consider the string \"12345\" which is 10 bytes compared to the integer 12345 which is only 4 bytes.\n* Parquet files are highly compressed but uncompressed in RAM.\n* In a relational database... well... who knows?\n\nThe **200 comes from** the real-world-experience of Databricks' engineers and is **based largely on efficiency** and not so much resource limitations. \n\nOn an executor with a reduced amount of RAM you might need to lower that.\n\nFor example, at 8 partitions (corresponding to our max number of slots) & 200MB per partition\n* That will use roughly **1.5GB**\n* We **might** get away with that on CE.\n* If you have transformations that balloon the data size (such as Natural Language Processing) you are sure to run into problems.\n\n**Question:** If I read in my data and it comes in as 10 partitions should I...\n* reduce my partitions down to 8 (1x number of slots)\n* or increase my partitions up to 16 (2x number of slots)\n\n**Answer:** It depends on the size of each partition\n* Read the data in. \n* Cache it. \n* Look at the size per partition.\n* If you are near or over 200MB consider increasing the number of partitions.\n* If you are under 200MB consider decreasing the number of partitions.\n\nThe goal will **ALWAYS** be to use as few partitions as possible while maintaining at least 1 x number-of-slots."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b69fe9f9-b071-4aa3-9e46-abc87f796247"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) repartition(n) or coalesce(n)\n\nWe have two operations that can help address this problem: `repartition(n)` and `coalesce(n)`.\n\nIf you look at the API docs, `coalesce(n)` is described like this:\n> Returns a new Dataset that has exactly numPartitions partitions, when fewer partitions are requested.<br/>\n> If a larger number of partitions is requested, it will stay at the current number of partitions.\n\nIf you look at the API docs, `repartition(n)` is described like this:\n> Returns a new Dataset that has exactly numPartitions partitions.\n\nThe key differences between the two are\n* `coalesce(n)` is a **narrow** transformation and can only be used to reduce the number of partitions.\n* `repartition(n)` is a **wide** transformation and can be used to reduce or increase the number of partitions.\n\nSo, if I'm increasing the number of partitions I have only one choice: `repartition(n)`\n\nIf I'm reducing the number of partitions I can use either one, so how do I decide?\n* First off, `coalesce(n)` is a **narrow** transformation and performs better because it avoids a shuffle.\n* However, `coalesce(n)` cannot guarantee even **distribution of records** across all partitions.\n* For example, with `coalesce(n)` you might end up with **a few partitions containing 80%** of all the data.\n* On the other hand, `repartition(n)` will give us a relatively **uniform distribution**.\n* And `repartition(n)` is a **wide** transformation meaning we have the added cost of a **shuffle operation**.\n\nIn our case, we \"need\" to go form 5 partitions up to 8 partitions - our only option here is `repartition(n)`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb428e5e-1c0e-4e33-9697-46a0ec0e203f"}}},{"cell_type":"code","source":["repartitionedDF = alternateDF.repartition(8)\n\nprint(\"Partitions: {0:,}\".format( repartitionedDF.rdd.getNumPartitions() ))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb35dea8-d93e-44f3-bc5d-3d43859d4f3b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Cache, Again?\n\nBack to list...\n0. <div style=\"text-decoration:line-through\">Read the data in</div>\n0. <div style=\"text-decoration:line-through\">Balance the number of partitions to the number of slots</div>\n0. Cache the data\n0. Adjust the `spark.sql.shuffle.partitions`\n0. Perform some basic ETL (i.e., convert strings to timestamp)\n0. Possibly re-cache the data if the ETL was costly\n\nWe just balanced the number of partitions to the number of slots.\n\nDepending on the size of the data and the number of partitions, the shuffle operation can be fairly expensive (though necessary).\n\nLet's cache the result of the `repartition(n)` call..\n* Or more specifically, let's mark it for caching.\n* The actual cache will occur later once an action is performed\n* Or you could just execute a count to force materialization of the cache."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89031e48-a43d-4bb5-8c19-ac7ff1f3dc9c"}}},{"cell_type":"code","source":["repartitionedDF.cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2faf071-80ae-4346-908c-72c9ec905590"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) spark.sql.shuffle.partitions\n\n0. <div style=\"text-decoration:line-through\">Read the data in</div>\n0. <div style=\"text-decoration:line-through\">Balance the number of partitions to the number of slots</div>\n0. <div style=\"text-decoration:line-through\">Cache the data</div>\n0. Adjust the `spark.sql.shuffle.partitions`\n0. Perform some basic ETL (i.e., convert strings to timestamp)\n0. Possibly re-cache the data if the ETL was costly\n\nThe next problem has to do with a side effect of certain **wide** transformations.\n\nSo far, we haven't hit any **wide** transformations other than `repartition(n)`\n* But eventually we will... \n* Let's illustrate the problem that we will **eventually** hit\n* We can do this by simply sorting our data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d010d62a-49b1-4f54-b656-509f0aab70ad"}}},{"cell_type":"code","source":["(repartitionedDF\n  .orderBy(col(\"timestamp\"), col(\"site\")) # sort the data\n   .foreach(lambda x: None)               # literally does nothing except trigger a job\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"adaa63e4-73a2-43a4-9fe6-d9048b501826"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Quick Detour\nSomething isn't right here...\n* We only executed one action.\n* But two jobs were triggered.\n* If we look at the physical plan we can see the reason for the extra job.\n* The answer lies in the step **Exchange rangepartitioning**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ebe0830-eb83-4f2c-a680-b5ad4ae086f6"}}},{"cell_type":"code","source":["(repartitionedDF\n  .orderBy(col(\"timestamp\"), col(\"site\"))\n  .explain()\n)\nprint(\"-\"*80)\n\n(repartitionedDF\n  .orderBy(col(\"timestamp\"), col(\"site\"))\n  .limit(3000000)\n  .explain()\n)\nprint(\"-\"*80)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"737cf0f1-4633-4077-ac70-2360250bd5ec"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["And just to prove that the extra job is due to the number of records in our DataFrame, re-run it with only 3M records:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e352dc53-77df-4573-aa28-323ee2db1ead"}}},{"cell_type":"code","source":["(repartitionedDF\n  .orderBy(col(\"timestamp\"), col(\"site\")) # sort the data\n  .limit(3000000)                         # only 3 million please    \n  .foreach(lambda x: None)                # literally does nothing except trigger a job\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"861e8b90-6432-4dc6-9a72-0eef3e6c6572"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Only 1 job.\n\nSpark's Catalyst Optimizer is optimizing our jobs for us!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"777e6328-2c36-467b-9976-84e810d687a5"}}},{"cell_type":"markdown","source":["### The Real Problem\n\nBack to the original issue...\n* Rerun the original job (below).\n* Take a look at the second job.\n* Look at the 3rd Stage.\n* Notice that it has 200 partitions!\n* And this is our problem."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ba2b4a3-bc4a-405d-84ab-71e2985063ae"}}},{"cell_type":"code","source":["funkyDF = (repartitionedDF\n  .orderBy(col(\"timestamp\"), col(\"site\")) # sorts the data\n)                                         #\nfunkyDF.foreach(lambda x: None)           # literally does nothing except trigger a job"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee094d78-0cd6-436e-9cd0-f5ce3fc72c85"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The problem is the number of partitions we ended up with.\n\nBesides looking at the number of tasks in the final stage, we can simply print out the number of partitions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b79fabd-2580-4be4-868d-5df975e0c124"}}},{"cell_type":"code","source":["print(\"Partitions: {0:,}\".format( funkyDF.rdd.getNumPartitions() ))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b4753c17-343e-4cf4-bed4-5e2927e97d50"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The engineers building Apache Spark chose a default value, 200, for the new partition size.\n\nAfter all our work to determine the right number of partitions they go and undo it on us.\n\nThe value 200 is actually based on practical experience, attempting to account for the most common scenarios to date.\n\nWork is being done to intelligently determine this new value but that is still in progress.\n\nFor now, we can tweak it with the configuration value `spark.sql.shuffle.partitions`\n\nWe can see below that it is actually configured for 200 partitions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3097162b-aa85-45e9-ac91-f5a188f7746e"}}},{"cell_type":"code","source":["spark.conf.get(\"spark.sql.shuffle.partitions\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f1e6e54e-f8ad-468c-888b-7b92ed5c276a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We can change the config setting with the following command"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"546783b7-7774-449f-9304-b0379b0d3dc6"}}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"efe9cb7e-5f81-45b8-b4f3-9f5276bd647b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now, if we re-run our query, we will see that we end up with the 8 partitions we want post-shuffle."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4244d919-6016-43d3-a751-a581e4f86c68"}}},{"cell_type":"code","source":["betterDF = (repartitionedDF\n  .orderBy(col(\"timestamp\"), col(\"site\")) # sort the data\n)                                         #\nbetterDF.foreach(lambda x: None)          # literally does nothing except trigger a job\n\nprint(\"Partitions: {0:,}\".format( betterDF.rdd.getNumPartitions() ))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8bc5178c-e01c-481f-9ad1-455c9a1632fe"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Initial ETL\n\n0. <div style=\"text-decoration:line-through\">Read the data in</div>\n0. <div style=\"text-decoration:line-through\">Balance the number of partitions to the number of slots</div>\n0. <div style=\"text-decoration:line-through\">Cache the data</div>\n0. <div style=\"text-decoration:line-through\">Adjust the `spark.sql.shuffle.partitions`</div>\n0. Perform some basic ETL (i.e., convert strings to timestamp)\n0. Possibly re-cache the data if the ETL was costly\n\nWe may have some standard ETL.\n\nIn this case we will want to do something like convert the `timestamp` column from a **string** to a data type more appropriate for **date & time**.\n\nWe are not going to do that here, instead will will cover that specific case in a future notebook when we look at all the date & time functions.\n\nBut so as to not leave you in suspense..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7cc2455c-4128-4102-833b-16b4673e04f9"}}},{"cell_type":"code","source":["pageviewsDF = (repartitionedDF\n  .select(\n    unix_timestamp( col(\"timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss\").cast(\"timestamp\").alias(\"createdAt\"), \n    col(\"site\"), \n    col(\"requests\") \n  )\n)\n\nprint(\"****BEFORE****\")\nrepartitionedDF.printSchema()\n\nprint(\"****AFTER****\")\npageviewsDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f41773d-cdc1-427c-bdd6-1b745b309f58"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["And assuming that initial ETL was expensive... we would want to finish up by caching our final `DataFrame`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ff3acaf-0c60-404c-bd43-0865beb3b79c"}}},{"cell_type":"code","source":["# mark it as cached.\npageviewsDF.cache() \n\n# materialize the cache.\npageviewsDF.count() "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dacb38d0-067e-43ae-ad8d-e9221db118f5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) All Done\n\n0. <div style=\"text-decoration:line-through\">Read the data in</div>\n0. <div style=\"text-decoration:line-through\">Balance the number of partitions to the number of slots</div>\n0. <div style=\"text-decoration:line-through\">Cache the data</div>\n0. <div style=\"text-decoration:line-through\">Adjust the `spark.sql.shuffle.partitions`</div>\n0. <div style=\"text-decoration:line-through\">Perform some basic ETL (i.e., convert strings to timestamp)</div>\n0. <div style=\"text-decoration:line-through\">Possibly re-cache the data if the ETL was costly</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d471ead-38d7-4037-87e4-8f15b22fbca2"}}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"40ea5007-19ba-413f-aa2a-67c1395be50f"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Cleanup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b58c3e8-a714-44f2-84e0-86234d39dffe"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/labs.png) Partitioning Lab\nIt's time to put what we learned to practice.\n\nGo ahead and open the notebook [Partitioning Lab]($./Partitioning Lab) and complete the exercises."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"40961d0d-25c6-4c9c-bdad-5267c7bbab1f"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5bcf4bd-8c41-4d32-8770-f9c3ba8dd411"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Partitioning","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3794944577840318}},"nbformat":4,"nbformat_minor":0}
