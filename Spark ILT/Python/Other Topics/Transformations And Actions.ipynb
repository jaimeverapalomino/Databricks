{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f10c98c-53af-4fd4-b3f9-15d0f10e9336"}}},{"cell_type":"markdown","source":["# Transformations & Actions\n\n**Technical Accomplishments:**\n* Review the Lazy vs. Eager design\n* Quick review of Transformations\n* Quick review of Actions\n* Introduce the Catalyst Optimizer\n* Wide vs. Narrow Transformations"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5e18447-397d-43ec-9c60-0bf09dc75070"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a8d61e5-0e5c-4acb-8234-d98f655ac3d2"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1d95e80-4056-4529-970b-a098c97a598b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Laziness By Design\n\nFundamental to Apache Spark are the notions that\n* Transformations are **LAZY**\n* Actions are **EAGER**\n\nWe see this play out when we run multiple transformations back-to-back, and no job is triggered:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6efcc302-e29e-4076-a6c1-4d8081894473"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n\nschema = StructType(\n  [\n    StructField(\"project\", StringType(), False),\n    StructField(\"article\", StringType(), False),\n    StructField(\"requests\", IntegerType(), False),\n    StructField(\"bytes_served\", LongType(), False)\n  ]\n)\n\nparquetFile = \"dbfs:/mnt/training/wikipedia/pagecounts/staging_parquet_en_only_clean/\"\n\ntopTenDF = (spark                                          # Our SparkSession & Entry Point\n  .read                                                    # DataFrameReader\n  .schema(schema)                                          # DataFrameReader (config)\n  .parquet(parquetFile)                                    # Transformation (initial)\n  .where( \"project = 'en'\" )                               # Transformation\n  .drop(\"bytes_served\")                                    # Transformation\n  .filter( col(\"article\") != \"Main_Page\")                  # Transformation\n  .filter( col(\"article\") != \"-\")                          # Transformation\n  .filter( col(\"article\").startswith(\"Special:\") == False) # Transformation\n)\nprint(topTenDF) # Python hack to print the data type"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47b276d0-8ab4-497e-b79e-f4ee8199fba9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(topTenDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cc20140-93a8-46e1-8dda-0c20b1f1fa46"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["There is one exception to this.\n\nWhen you are reading data, Apache Spark needs to know the schema of the `DataFrame`.\n\nIn this case, the schema was specified.\n\nHowever, if the `DataFrameReader` doesn't specify the schema, it will trigger a one-time job to examine the files on disk and determine the schema:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9bc1e791-a276-4152-8104-59b916f32764"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n\n# schema = StructType(\n#   [\n#     StructField(\"project\", StringType(), False),\n#     StructField(\"article\", StringType(), False),\n#     StructField(\"requests\", IntegerType(), False),\n#     StructField(\"bytes_served\", LongType(), False)\n#   ]\n# )\n\nparquetFile = \"dbfs:/mnt/training/wikipedia/pagecounts/staging_parquet_en_only_clean/\"\n\n(spark                                                      # Our SparkSession & Entry Point\n  .read                                                     # DataFrameReader\n  #.schema(schema)                                          # DataFrameReader (config)\n  .parquet(parquetFile)                                     # Transformation (initial)\n  .where( \"project = 'en'\" )                                # Transformation\n  .drop(\"bytes_served\")                                     # Transformation\n  .filter( col(\"article\") != \"Main_Page\")                   # Transformation\n  .filter( col(\"article\") != \"-\")                           # Transformation\n  .filter( col(\"article\").startswith(\"Special:\") == False)  # Transformation\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"359a694c-92a8-471b-916e-4809d421f3b5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Back to our instance of `topTenDF`...\n\nWith it, we can trigger a job by calling an action such as `count()`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6622dfc-f390-433d-8ab2-76a5db99d9e0"}}},{"cell_type":"code","source":["topTenDF.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"137450d3-c1f4-4f52-9cd5-5c0a8c49ef7e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Why is Laziness So Important?\n\nThis is a common pattern in functional programming as well as with Big Data specific languages.\n* We see it in Scala as part of its core design.\n* Java 8 introduced the concept with its Streams API.\n* And many functional programming languages have similar APIs.\n\nIt has a number of benefits\n* Not forced to load all data at step #1 \n  * Technically impossible with **REALLY** large datasets.\n* Easier to parallelize operations \n  * N different transformations can be processed on a single data element, on a single thread, on a single machine. \n* Most importantly, it allows the framework to automatically apply various optimizations"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04a8e66e-c01f-4959-a915-3d7769d607cf"}}},{"cell_type":"markdown","source":["### Catalyst Optimizer\n\nBecause our API is declarative a large number of optimizations are available to us.\n\nSome of the examples include:\n  * Optimizing data type for storage\n  * Rewriting queries for performance\n  * Predicate push downs\n\n![Catalyst](https://files.training.databricks.com/images/105/catalyst-diagram.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1b161b5-fbbc-448f-87c8-9fc3996b5470"}}},{"cell_type":"markdown","source":["Additional Articles:\n* <a href=\"https://databricks.com/session/deep-dive-into-catalyst-apache-spark-2-0s-optimizer\" target=\"_blank\">Deep Dive into Catalyst: Apache Spark 2.0's Optimizer</a>, Yin Huai's Spark Summit 2016 presentation.\n* <a href=\"https://www.youtube.com/watch?v=6bCpISym_0w\" target=\"_blank\">Catalyst: A Functional Query Optimizer for Spark and Shark</a>, Michael Armbrust's presentation at ScalaDays 2016.\n* <a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\" target=\"_blank\">Deep Dive into Spark SQL's Catalyst Optimizer</a>, Databricks Blog, April 13, 2015.\n* <a href=\"http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf\" target=\"_blank\">Spark SQL: Relational Data Processing in Spark</a>, Michael Armbrust, Reynold S. Xin, Cheng Lian, Yin Huai, Davies Liu, Joseph K. Bradley, Xiangrui Meng, Tomer Kaftan, Michael J. Franklin, Ali Ghodsi, Matei Zaharia,<br/>_Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data_."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c3df5ab-8456-4517-85af-44af3c626b0a"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Actions\n\nTransformations always return a `DataFrame` (or in some cases, such as Scala & Java, `Dataset[Row]`).\n\nIn contrast, Actions either return a result or write to disc. For example:\n* The number of records in the case of `count()` \n* An array of objects in the case of `collect()` or `take(n)`\n\nWe've seen a good number of the actions - most of them are listed below. \n\nFor the complete list, one needs to review the API docs.\n\n| Method | Return | Description |\n|--------|--------|-------------|\n| `collect()` | Collection | Returns an array that contains all of Rows in this Dataset. |\n| `count()` | Long | Returns the number of rows in the Dataset. |\n| `first()` | Row | Returns the first row. |\n| `foreach(f)` | - | Applies a function f to all rows. |\n| `foreachPartition(f)` | - | Applies a function f to each partition of this Dataset. |\n| `head()` | Row | Returns the first row. |\n| `reduce(f)` | Row | Reduces the elements of this Dataset using the specified binary function. |\n| `show(..)` | - | Displays the top 20 rows of Dataset in a tabular form. |\n| `take(n)` | Collection | Returns the first n rows in the Dataset. |\n| `toLocalIterator()` | Iterator | Return an iterator that contains all of Rows in this Dataset. |\n\n** *Note #1:* ** *There are some variations in the methods from language to language *\n\n** *Note #2:* ** *The command `display(..)` is not included here because it's not part of the Spark API, even though it ultimately calls an action. *"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eac2bd13-5af2-4ba5-87ba-f9bf33b59f23"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Transformations\n\nTransformations have the following key characteristics:\n* They eventually return another `DataFrame` (or in the case of Scala and Java, `Dataset[T]`).\n* They are immutable - that is each instance of a `DataFrame` cannot be altered once it's instantiated.\n  * This means other optimizations are possible - such as the use of shuffle files (to be discussed in detail later)\n* Are classified as either a Wide or Narrow operation\n* In Scala & Java come in two flavors: Typed & Untyped\n\nLet's take a look at the API docs - in this case, looking at the Scala doc is a little easier to decipher.\n\n** *Note:* ** The list of transformations varies significantly between each language.<br/>\nMostly because Java & Scala are statically typed languages compared Python & R which are dynamically typed."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"716b7f90-48c0-438d-945d-535ddbd5d762"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Typed vs. Untyped Transformations\n\nThe difference between **Typed** and **Untyped** transformations is specific to Scala and Java where Datasets can contain statically typed objects (e.g. `Dataset[Person]`).\n\nA typed transformation does not change the columns and thus leaves the data types unchanged (e.g. `Dataset[T]`).  An untyped transformations can change the column names and therefore return a generic `DataFrame` (i.e. `Dataset[Row]`).\n\n| Typed Transformations <br/> returns *Dataset[T]*  | Untyped Transformations <br/> returns *DataFrame* | \n|:----------------------------:|:-----------------------------:|\n| `alias(..)`                  | `agg(..)`                     |\n| `as(..)`                     | `apply(..)`                   |\n| `coalesce(..)`               | `col(..)`                     |\n| `distinct(..)`               | `crossJoin(..)`               |\n| `dropDuplicates(..)`         | `cube(..)`                    |\n| `except(..)`                 | `drop(..)`                    |\n| `filter(..)`                 | `groupBy(..)`                 |\n| `flatMap(..)`                | `join(..)`                    |\n| `groupByKey(..)`             | `rollup(..)`                  | \n| `intersect(..)`              | `select(..)`                  |\n| `limit(..)`                  | `selectExpr(..)`              |                    \n| `map(..)`                    | `stat(..)`                    |                    \n| `mapPartitions(..)`          | `withColumn(..)`              |                    \n| `orderBy(..)`                | `withColumnRenamed(..)`       |                     \n| `randomSplit(..)`            |                               |                     \n| `repartition(..)`            |                               |                     \n| `sample(..)`                 |                               |                     \n| `sort(..)`                   |                               |                     \n| `sortWithinPartitions(..)`   |                               |                     \n| `union(..)`                  |                               |                     \n| `where(..)`                  |                               |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ee03d47-68b7-4c5a-8a53-07c2d143825b"}}},{"cell_type":"markdown","source":["-sandbox\n##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Wide vs. Narrow Transformations\n\nRegardless of language, transformations break down into two broad categories: wide and narrow.\n\n**Narrow Transformations**: The data required to compute the records in a single partition reside in at most one partition of the parent RDD.\n\nExamples include:\n* `select(..)`  \n* `filter(..)`  \n* `coalesce()`  \n\n<img src=\"https://files.training.databricks.com/images/105/transformations-narrow.png\" alt=\"Narrow Transformations\" style=\"height:300px\"/>\n\n<br/>\n\n**Wide Transformations**: The data required to compute the records in a single partition may reside in many partitions of the parent RDD. \n\nExamples include:\n* `distinct()`         \n* `groupBy(..).sum()`  \n* `sort(..)`           \n* `repartition(n)`     \n\n<img src=\"https://files.training.databricks.com/images/105/transformations-wide.png\" alt=\"Wide Transformations\" style=\"height:300px\"/>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"628d8b77-246a-43cb-b91e-1bf32a48f1c1"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Shuffles\n\nA shuffle operation is triggered when data needs to move between executors.\n\nFor example, to group by color, it will serve us best if...\n  * All the reds are in one partitions\n  * All the blues are in a second partition\n  * All the greens are in a third\n\nFrom there we can easily sum/count/average all of the reds, blues, and greens.\n\nTo carry out the shuffle operation Spark needs to\n* Convert the data to the UnsafeRow (if it isn't already), commonly referred to as Tungsten Binary Format.\n* Write that data to disk on the local node - at this point the slot is free for the next task.\n* Send that data across the wire to another executor\n  * Technically the Driver decides which executor gets which piece of data.\n  * Then the executor pulls the data it needs from the other executor's shuffle files.\n* Copy the data back into RAM on the new executor\n  * The concept, if not the action, is just like the initial read \"every\" `DataFrame` starts with.\n  * The main difference being it's the 2nd+ stage.\n\nAs we will see in a moment, this amounts to a free cache from what is effectively temp files.\n\n** *Note:* ** *Some actions induce in a shuffle.*<br/>\n*Good examples would include the operations `count()` and `reduce(..)`.*"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"18d997a5-8a57-4917-98c0-69ccbb9803f8"}}},{"cell_type":"markdown","source":["### UnsafeRow (aka Tungsten Binary Format)\n\nAs a quick side note, the data that is \"shuffled\" is in a format known as `UnsafeRow`, or more commonly, the Tungsten Binary Format.\n\n`UnsafeRow` is the in-memory storage format for Spark SQL, DataFrames & Datasets. \n\nAdvantages include:\n\n* Compactness: \n  * Column values are encoded using custom encoders, not as JVM objects (as with RDDs). \n  * The benefit of using Spark 2.x's custom encoders is that you get almost the same compactness as Java serialization, but significantly faster encoding/decoding speeds. \n  * Also, for custom data types, it is possible to write custom encoders from scratch.\n\n* Efficiency: Spark can operate _directly out of Tungsten_, without first deserializing Tungsten data into JVM objects."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12bc68d0-4eb2-47c5-acd1-c5a6355993de"}}},{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center\"><img src=\"https://files.training.databricks.com/images/unsafe-row-format.png\"></div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb6f99ce-b7fd-4559-a17f-f3a33a41427c"}}},{"cell_type":"markdown","source":["### How UnsafeRow works\n* The first field, \"123\", is stored in place as its primitive.\n* The next 2 fields, \"data\" and \"bricks\", are strings and are of variable length. \n* An offset for these two strings is stored in place (32L and 48L respectively shown in the picture below).\n* The data stored in these two offset’s are of format “length + data”. \n* At offset 32L, we store 4 + \"data\" and likewise at offset 48L we store 6 + \"bricks\"."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8f7223c1-b729-42f9-b9fe-e4108c359554"}}},{"cell_type":"markdown","source":["-sandbox\n### Lightning-fast Serialization with Encoders\n<div style=\"text-align:center\"><img src=\"https://files.training.databricks.com/images/encoders-vs-serialization-benchmark.png\"></div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61a5ebe7-112e-4451-b234-0609d4c4005c"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Stages\n* When we shuffle data, it creates what is known as a stage boundary.\n* Stage boundaries represent a process bottleneck.\n\nTake for example the following transformations:\n\n|Step |Transformation|\n|----:|--------------|\n| 1   | Read    |\n| 2   | Select  |\n| 3   | Filter  |\n| 4   | GroupBy |\n| 5   | Select  |\n| 6   | Filter  |\n| 7   | Write   |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7739db3b-91a3-4b5b-9136-20ca31a49dcf"}}},{"cell_type":"markdown","source":["Spark will break this one job into two stages (steps 1-4b and steps 4c-8):\n\n**Stage #1**\n\n|Step |Transformation|\n|----:|--------------|\n| 1   | Read |\n| 2   | Select |\n| 3   | Filter |\n| 4a | GroupBy 1/2 |\n| 4b | shuffle write |\n\n**Stage #2**\n\n|Step |Transformation|\n|----:|--------------|\n| 4c | shuffle read |\n| 4d | GroupBy  2/2 |\n| 5   | Select |\n| 6   | Filter |\n| 7   | Write |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b17493f-1a28-42e6-84fc-29294fe2e9e7"}}},{"cell_type":"markdown","source":["In **Stage #1**, Spark will create a pipeline of transformations in which the data is read into RAM (Step #1), and then perform steps #2, #3, #4a & #4b\n\nAll partitions must complete **Stage #1** before continuing to **Stage #2**\n* It's not possible to group all records across all partitions until every task is completed.\n* This is the point at which all the tasks must synchronize.\n* This creates our bottleneck.\n* Besides the bottleneck, this is also a significant performance hit: disk IO, network IO and more disk IO.\n\nOnce the data is shuffled, we can resume execution...\n\nFor **Stage #2**, Spark will again create a pipeline of transformations in which the shuffle data is read into RAM (Step #4c) and then perform transformations #4d, #5, #6 and finally the write action, step #7."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c67b1e31-b085-4f9f-97c9-620e0f7cc57b"}}},{"cell_type":"markdown","source":["-sandbox\n##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Pipelining\n<img src=\"https://files.training.databricks.com/images/pipelining-2.png\" style=\"float: right\"/>\n\n<ul>\n  <li>Pipelining is the idea of executing as many operations as possible on a single partition of data.</li>\n  <li>Once a single partition of data is read into RAM, Spark will combine as many narrow operations as it can into a single **Task**</li>\n  <li>Wide operations force a shuffle, conclude, a stage and end a pipeline.</li>\n  <li>Compare to MapReduce where: </li>\n  <ol>\n    <li>Data is read from disk</li>\n    <li>A single transformation takes place</li>\n    <li>Data is written to disk</li>\n    <li>Repeat steps 1-3 until all transformations are completed</li>\n  </ol>\n  <li>By avoiding all the extra network and disk IO, Spark can easily out perform traditional MapReduce applications.</li>\n</ul>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"efe3cf4d-4bdd-46cd-bb66-2ba139dd4eef"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Lineage\nFrom the developer's perspective, we start with a read and conclude (in this case) with a write\n\n|Step |Transformation|\n|----:|--------------|\n| 1   | Read    |\n| 2   | Select  |\n| 3   | Filter  |\n| 4   | GroupBy |\n| 5   | Select  |\n| 6   | Filter  |\n| 7   | Write   |\n  \nHowever, Spark starts with the action (`write(..)` in this case).\n\nNext, it asks the question, what do I need to do first?\n\nIt then proceeds to determine which transformation precedes this step until it identifies the first transformation.\n\n|Step |Transformation| |\n|----:|--------------|-|\n| 7   | Write   | Depends on #6 |\n| 6   | Filter  | Depends on #5 |\n| 5   | Select  | Depends on #4 |\n| 4   | GroupBy | Depends on #3 |\n| 3   | Filter  | Depends on #2 |\n| 2   | Select  | Depends on #1 |\n| 1   | Read    | First |\n\nThis would be equivalent to understanding your own lineage. \n* You don't ask if you are related to Genghis Khan and then work through the ancestry of all his children (5% of all people in Asia).\n* You start with your mother.\n* Then your grandmother\n* Then your great-grandmother\n* ... and so on\n* Until you discover you are actually related to Catherine Parr, the last queen of Henry the VIII."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab8d494c-966b-43e7-a01f-b55bb3c0bca0"}}},{"cell_type":"markdown","source":["### Why Work Backwards?\n**Question:** So what is the benefit of working backward through your action's lineage?<br/>\n**Answer:** It allows Spark to determine if it is necessary to execute every transformation.\n\nTake another look at our example:\n* Say we've executed this once already\n* On the first execution, step #4 resulted in a shuffle\n* Those shuffle files are on the various executors (src & dst)\n* Because the transformations are immutable, no aspect of our lineage can change.\n* That means the results of our last shuffle (if still available) can be reused.\n\n|Step |Transformation| |\n|----:|--------------|-|\n| 7   | Write   | Depends on #6 |\n| 6   | Filter  | Depends on #5 |\n| 5   | Select  | Depends on #4 |\n| 4   | GroupBy | <<< shuffle |\n| 3   | Filter  | *don't care* |\n| 2   | Select  | *don't care* |\n| 1   | Read    | *don't care* |\n\nIn this case, what we end up executing is only the operations from **Stage #2**.\n\nThis saves us the initial network read and all the transformations in **Stage #1**\n\n|Step |Transformation|   |\n|----:|---------------|:-:|\n| 1   | Read          | *skipped* |\n| 2   | Select        | *skipped* |\n| 3   | Filter        | *skipped* |\n| 4a  | GroupBy 1/2   | *skipped* |\n| 4b  | shuffle write | *skipped* |\n| 4c  | shuffle read  | - |\n| 4d  | GroupBy  2/2  | - |\n| 5   | Select        | - |\n| 6   | Filter        | - |\n| 7   | Write         | - |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27a59542-2791-4b20-bd88-c50ceb9f4239"}}},{"cell_type":"markdown","source":["### And Caching...\n\nThe reuse of shuffle files (aka our temp files) is just one example of Spark optimizing queries anywhere it can.\n\nWe cannot assume this will be available to us. \n\nShuffle files are by definition temporary files and will eventually be removed.\n\nHowever, we cache data to explicitly accomplish the same thing that happens inadvertently with shuffle files.\n\nIn this case, the lineage plays the same role. Take for example:\n\n|Step |Transformation| |\n|----:|--------------|-|\n| 7   | Write   | Depends on #6 |\n| 6   | Filter  | Depends on #5 |\n| 5   | Select  | <<< cache |\n| 4   | GroupBy | <<< shuffle files |\n| 3   | Filter  | ? |\n| 2   | Select  | ? |\n| 1   | Read    | ? |\n\nIn this case we cached the result of the `select(..)`. \n\nWe never even get to the part of the lineage that involves the shuffle, let alone **Stage #1**.\n\nInstead, we pick up with the cache and resume execution from there:\n\n|Step |Transformation|   |\n|----:|---------------|:-:|\n| 1   | Read          | *skipped* |\n| 2   | Select        | *skipped* |\n| 3   | Filter        | *skipped* |\n| 4a  | GroupBy 1/2   | *skipped* |\n| 4b  | shuffle write | *skipped* |\n| 4c  | shuffle read  | *skipped* |\n| 4d  | GroupBy  2/2  | *skipped* |\n| 5a  | cache read    | - |\n| 5b  | Select        | - |\n| 6   | Filter        | - |\n| 7   | Write         | - |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ace037d0-dd29-4dd8-9340-55aab2d7ae31"}}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"612cf29c-8e73-44b7-864a-cba6d60e419c"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Cleanup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fabbf2cb-9935-435b-bdb4-5050735306a5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/labs.png) Transformations & Actions Lab\nIt's time to put what we learned to practice.\n\nGo ahead and open the notebook [Transformations And Actions Lab]($./Transformations And Actions Lab) and complete the exercises."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24cbf0cf-8817-4de3-8af7-3dd31cd6fd64"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f01aba52-470f-4dc0-814f-641cb097a2a1"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Transformations And Actions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3794944577840175}},"nbformat":4,"nbformat_minor":0}
